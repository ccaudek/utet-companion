<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>44&nbsp; Valutare i modelli bayesiani – Metodi bayesiani in psicologia</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/entropy/conclusions_sec.html" rel="next">
<link href="../../chapters/entropy/02_kl.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0a72236910a44089af39cd28873f322e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9908c7b05874059c2106d454ac00f1d0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><style>html{ scroll-behavior: smooth; }</style>
<script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
</script><script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
// Suggerimento CSS: vedi sezione 3 per gli spazi attorno a display math
</script><script>
window.MathJax = {
  tex: {
    packages: {'[+]': ['boldsymbol']},
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: { fontCache: 'global' }
};
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../style/_typography-extras.css">
<link rel="stylesheet" href="../../style/_code-extras.css">
<link rel="stylesheet" href="../../style/_math-extras.css">
<link rel="stylesheet" href="../../style/styles.css">
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/introduction_sec.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Metodi bayesiani in psicologia</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/utet-companion/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Struttura del companion</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Prefazione</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_uncertainty_quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_conjugate_families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/03_stan_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Introduzione pratica a Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/04_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/05_mcmc_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Controlli predittivi bayesiani (a priori e a posteriori) con <code>cmdstanr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/06_stan_odds_ratio_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_stan_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Confrontare due medie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/08_stan_poisson_model_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/09_stan_gaussian_mixture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Modelli Mistura Gaussiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/10_stan_nuisance_parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Modelli con più di un parametro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/11_stan_hier_beta_binom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Modello gerarchico beta-binomiale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/12_stan_parametrization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Parametrizzazioni centered e non-centered</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/13_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive sulla sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar_sea_ice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_stan_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Regressione lineare in Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Errore di specificazione e bias da variabile omessa</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">GLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/01_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/02_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/03_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni con la regressione logistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/04_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/05_logistic_process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Dal GLM a un modello processuale per dati binari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/06_missing_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Dati mancanti in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive sulla sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Il modello di Rescorla–Wagner</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/04_study_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">Decisione ottimale e utilità attesa: l’approccio bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">56</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a35_other_conjugate_families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Altre famiglie coniugate</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a36_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a40_metropolis_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Metropolis: esempio Normale–Normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a41_cmdstanr_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Implementazione di modelli Bayesiani con Stan tramite <code>cmdstanr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Sommario</h2>
   
  <ul class="collapse">
<li><a href="#il-punto-di-partenza-dalle-previsioni-deterministiche-a-quelle-probabilistiche" id="toc-il-punto-di-partenza-dalle-previsioni-deterministiche-a-quelle-probabilistiche" class="nav-link active" data-scroll-target="#il-punto-di-partenza-dalle-previsioni-deterministiche-a-quelle-probabilistiche"><span class="header-section-number">44.1</span> Il punto di partenza: dalle previsioni deterministiche a quelle probabilistiche</a></li>
  <li><a href="#distribuzione-predittiva-posteriore-il-cuore-delle-previsioni-bayesiane" id="toc-distribuzione-predittiva-posteriore-il-cuore-delle-previsioni-bayesiane" class="nav-link" data-scroll-target="#distribuzione-predittiva-posteriore-il-cuore-delle-previsioni-bayesiane"><span class="header-section-number">44.2</span> Distribuzione predittiva posteriore: il cuore delle previsioni bayesiane</a></li>
  <li><a href="#il-problema-fondamentale-della-valutazione-predittiva" id="toc-il-problema-fondamentale-della-valutazione-predittiva" class="nav-link" data-scroll-target="#il-problema-fondamentale-della-valutazione-predittiva"><span class="header-section-number">44.3</span> Il problema fondamentale della valutazione predittiva</a></li>
  <li><a href="#sec-logscore" id="toc-sec-logscore" class="nav-link" data-scroll-target="#sec-logscore"><span class="header-section-number">44.4</span> Il log-score: misurare l’accuratezza predittiva punto per punto</a></li>
  <li><a href="#la-svolta-dalladattamento-alla-generalizzazione" id="toc-la-svolta-dalladattamento-alla-generalizzazione" class="nav-link" data-scroll-target="#la-svolta-dalladattamento-alla-generalizzazione"><span class="header-section-number">44.5</span> La svolta: dall’adattamento alla generalizzazione</a></li>
  <li><a href="#il-collegamento-con-la-divergenza-kl" id="toc-il-collegamento-con-la-divergenza-kl" class="nav-link" data-scroll-target="#il-collegamento-con-la-divergenza-kl"><span class="header-section-number">44.6</span> Il collegamento con la divergenza KL</a></li>
  <li><a href="#stimare-lelpd-nella-pratica-la-leave-one-out-cross-validation" id="toc-stimare-lelpd-nella-pratica-la-leave-one-out-cross-validation" class="nav-link" data-scroll-target="#stimare-lelpd-nella-pratica-la-leave-one-out-cross-validation"><span class="header-section-number">44.7</span> Stimare l’ELPD nella pratica: la Leave-One-Out Cross-Validation</a></li>
  <li><a href="#criteri-di-informazione" id="toc-criteri-di-informazione" class="nav-link" data-scroll-target="#criteri-di-informazione"><span class="header-section-number">44.8</span> Criteri di informazione</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/utet-companion/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/utet-companion/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/introduction_sec.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/03_model_comparison.html"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-div-kl-lppd-elpd" class="quarto-section-identifier"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><section id="introduzione" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Nei capitoli precedenti abbiamo visto due concetti fondamentali: l’<em>entropia</em>, che misura l’incertezza insita in una distribuzione, e la <em>divergenza di Kullback–Leibler</em> (<span class="math inline">\(D_{\text{KL}}\)</span>), che quantifica la distanza tra due distribuzioni di probabilità. Ora possiamo fare un passo ulteriore: usare queste idee per <em>valutare e confrontare modelli statistici</em> nel contesto bayesiano.</p>
<p>Il punto di partenza è una domanda cruciale: <em>quanto bene il modello riesce a prevedere nuovi dati?</em> Un buon modello non deve solo adattarsi ai dati osservati, ma deve anche saper <em>generalizzare</em> a situazioni future. Questa distinzione – adattamento vs.&nbsp;generalizzazione – è il cuore della valutazione predittiva.</p>
<p>Immaginiamo, ad esempio, di sviluppare un test psicologico per stimare l’ansia degli studenti prima di un esame. Non basta sapere che il modello descrive bene il campione usato per costruirlo: vogliamo essere ragionevolmente sicuri che le stesse previsioni funzionino anche per studenti che non hanno partecipato allo studio. In psicologia, scegliere tra due modelli è simile a decidere quale test usare: in entrambi i casi si cerca lo strumento che fornisce previsioni più affidabili.</p>
<section id="panoramica-del-capitolo" class="level3 unnumbered unlisted"><h3 class="unnumbered unlisted anchored" data-anchor-id="panoramica-del-capitolo">Panoramica del capitolo</h3>
<p>Per rispondere alla domanda fondamentale sulla qualità predittiva, seguiremo un percorso logico che ci condurrà dagli strumenti teorici ai metodi pratici:</p>
<ul>
<li>
<em>Prima costruiremo la base teorica</em>: vedremo come la <em>distribuzione predittiva posteriore</em> incorpora l’incertezza sui parametri nelle nostre previsioni</li>
<li>
<em>Poi definiremo le misure di accuratezza</em>: il <em>log-score</em> per valutare la bontà delle previsioni punto per punto</li>
<li>
<em>Distingueremo tra valutazione in-sample e out-of-sample</em>: LPPD (sui dati osservati) vs.&nbsp;ELPD (capacità di generalizzazione)</li>
<li>
<em>Collegheremo tutto alla divergenza KL</em>: per capire perché massimizzare l’ELPD equivale a trovare il modello più vicino alla realtà</li>
<li>
<em>Implementeremo metodi pratici</em>: LOO-CV per stimare l’ELPD senza conoscere la vera distribuzione dei dati</li>
<li>
<em>Confronteremo con altri criteri</em>: AIC, BIC, WAIC e i loro ambiti di applicazione</li>
</ul>
<p>L’obiettivo è fornire strumenti pratici e un quadro concettuale chiaro per guidare la scelta del modello più adatto al problema in esame.</p>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Per comprendere appieno questo capitolo è utile leggere il capitolo 7 <em>Ulysses’ Compass</em> di <em>Statistical Rethinking</em> (<span class="citation" data-cites="McElreath_rethinking">McElreath (<a href="#ref-McElreath_rethinking" role="doc-biblioref">2020</a>)</span>).</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="Preparazione del Notebook">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://gt.rstudio.com">gt</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://conflicted.r-lib.org/">conflicted</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/paul-buerkner/brms">brms</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://mc-stan.org/loo/">loo</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://conflicted.r-lib.org/reference/conflicts_prefer.html">conflicts_prefer</a></span><span class="op">(</span><span class="fu">rstan</span><span class="fu">::</span><span class="va"><a href="https://mc-stan.org/loo/reference/loo.html">loo</a></span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section></section><section id="il-punto-di-partenza-dalle-previsioni-deterministiche-a-quelle-probabilistiche" class="level2" data-number="44.1"><h2 data-number="44.1" class="anchored" data-anchor-id="il-punto-di-partenza-dalle-previsioni-deterministiche-a-quelle-probabilistiche">
<span class="header-section-number">44.1</span> Il punto di partenza: dalle previsioni deterministiche a quelle probabilistiche</h2>
<p>Prima di addentrarci nei dettagli tecnici, facciamo un passo indietro per capire perché la valutazione predittiva bayesiana è diversa da quella frequentista classica.</p>
<p><em>Nell’approccio classico</em>, una volta stimati i parametri (ad esempio con la massima verosimiglianza), li trattiamo come “veri” e fissi. Se <span class="math inline">\(\hat{\theta}\)</span> è la nostra stima, la previsione per un nuovo dato <span class="math inline">\(\tilde{y}\)</span> è semplicemente:</p>
<p><span class="math display">\[
p(\tilde{y} \mid \hat{\theta}).
\]</span> Questo approccio ignora completamente l’incertezza sulla stima dei parametri.</p>
<p><em>Nell’approccio bayesiano</em>, invece, riconosciamo che i parametri sono incerti. Anche dopo aver osservato i dati, la nostra conoscenza di <span class="math inline">\(\theta\)</span> è descritta da un’intera distribuzione posteriore <span class="math inline">\(p(\theta \mid y)\)</span>, non da un singolo valore. Le previsioni devono quindi riflettere questa incertezza.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Analogia didattica">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Analogia didattica
</div>
</div>
<div class="callout-body-container callout-body">
<p>Immaginiamo di dover prevedere il tempo domani. L’approccio “classico” è come consultare un solo meteorologo e fidarsi completamente della sua previsione. L’approccio bayesiano è come consultare tutti i meteorologi disponibili, pesare le loro opinioni in base alla loro affidabilità passata, e costruire una previsione che tenga conto di tutti i punti di vista plausibili.</p>
</div>
</div>
</section><section id="distribuzione-predittiva-posteriore-il-cuore-delle-previsioni-bayesiane" class="level2" data-number="44.2"><h2 data-number="44.2" class="anchored" data-anchor-id="distribuzione-predittiva-posteriore-il-cuore-delle-previsioni-bayesiane">
<span class="header-section-number">44.2</span> Distribuzione predittiva posteriore: il cuore delle previsioni bayesiane</h2>
<p>Nel capitolo sul modello <em>beta–binomiale</em> l’abbiamo già incontrata: è lo strumento che, nell’approccio bayesiano, consente di prevedere nuovi dati incorporando sia la struttura del modello sia l’incertezza sui parametri. La logica è elegante nella sua semplicità: dopo aver osservato i dati <span class="math inline">\(y\)</span>, non otteniamo un singolo “miglior” valore dei parametri, ma una <em>distribuzione posteriore</em> <span class="math inline">\(p(\theta \mid y)\)</span> che quantifica i valori plausibili di <span class="math inline">\(\theta\)</span> e la nostra incertezza.</p>
<blockquote class="blockquote">
<p><strong>Esempio concreto:</strong> Uno psicologo che stima il livello medio di ansia in una popolazione, invece di affermare “la media è 4.7”, dirà: “il valore più plausibile è 4.7, <em>ma</em> è ragionevole che sia tra 4.2 e 5.1”, riflettendo la variabilità della distribuzione a posteriori.</p>
</blockquote>
<section id="la-formula-fondamentale" class="level3" data-number="44.2.1"><h3 data-number="44.2.1" class="anchored" data-anchor-id="la-formula-fondamentale">
<span class="header-section-number">44.2.1</span> La formula fondamentale</h3>
<p>Per prevedere un nuovo dato <span class="math inline">\(\tilde{y}\)</span>, non fissiamo <span class="math inline">\(\theta\)</span>. <em>Mediamo</em> invece tutte le previsioni condizionate <span class="math inline">\(p(\tilde{y} \mid \theta)\)</span> pesandole con la densità posteriore <span class="math inline">\(p(\theta\mid y)\)</span>:</p>
<p><span id="eq-predictive-posterior"><span class="math display">\[
p(\tilde{y} \mid y) = \int p(\tilde{y} \mid \theta)\, p(\theta \mid y)\, d\theta
\tag{44.1}\]</span></span> Questa è la <em>distribuzione predittiva posteriore</em>, e rappresenta la nostra migliore previsione per dati futuri dato quello che abbiamo osservato.</p>
</section><section id="decomposizione-dellintegrale-cosa-significa-realmente" class="level3" data-number="44.2.2"><h3 data-number="44.2.2" class="anchored" data-anchor-id="decomposizione-dellintegrale-cosa-significa-realmente">
<span class="header-section-number">44.2.2</span> Decomposizione dell’integrale: cosa significa realmente</h3>
<p>L’<a href="#eq-predictive-posterior" class="quarto-xref">Equazione&nbsp;<span>44.1</span></a> può sembrare astratta, ma ha un’interpretazione intuitiva molto chiara:</p>
<ol type="1">
<li>
<span class="math inline">\(p(\tilde{y} \mid \theta)\)</span>: se conoscessimo il vero valore di <span class="math inline">\(\theta\)</span>, questa sarebbe la nostra previsione per <span class="math inline">\(\tilde{y}\)</span>;</li>
<li>
<span class="math inline">\(p(\theta \mid y)\)</span>: questa è la nostra incertezza su quale sia il vero valore di <span class="math inline">\(\theta\)</span>;</li>
<li>
<em>L’integrale</em>: combina le previsioni per tutti i possibili valori di <span class="math inline">\(\theta\)</span>, pesandole secondo quanto crediamo che ciascun valore sia plausibile.</li>
</ol>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Intuizione dettagliata">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intuizione dettagliata
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Se conoscessimo il valore vero di <span class="math inline">\(\theta\)</span>, potremmo prevedere i dati futuri usando la distribuzione predittiva condizionata:</p>
<p><span class="math display">\[
p(\tilde y \mid \theta).
\]</span> Il problema è che <span class="math inline">\(\theta\)</span> non lo conosciamo: abbiamo soltanto la distribuzione a posteriori <span class="math inline">\(p(\theta\mid y)\)</span>. Perciò, la distribuzione predittiva posteriore si costruisce combinando le previsioni condizionate per ogni valore possibile di <span class="math inline">\(\theta\)</span>, pesandole con quanto ciascun valore è plausibile a posteriori:</p>
<p><span class="math display">\[
p(\tilde y\mid y) = \int p(\tilde y\mid \theta)\,p(\theta\mid y)\,d\theta.
\]</span> Per fare un esempio concreto, consideriamo il caso binomiale. Supponiamo che i dati futuri siano generati da una Binomiale con <span class="math inline">\(m\)</span> prove e parametro <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
p(\tilde y = x \mid \theta) = \binom{m}{x}\,\theta^x(1-\theta)^{m-x}.
\]</span> La distribuzione predittiva posteriore diventa:</p>
<p><span class="math display">\[
p(\tilde y = x \mid y) = \int \binom{m}{x}\,\theta^x(1-\theta)^{m-x}\,p(\theta\mid y)\,d\theta.
\]</span> In alcuni casi particolari (per esempio con prior Beta e dati binomiali) questo integrale si può risolvere analiticamente, ottenendo la <em>Beta–Binomiale</em>. Ma in generale non c’è una formula chiusa e serve un’approssimazione numerica.</p>
<p><strong>Approssimazione numerica con il metodo su griglia:</strong> L’idea è semplice: sostituire l’integrale con una somma pesata su una griglia di valori possibili di <span class="math inline">\(\theta\)</span>. I passaggi algoritmici sono i seguenti.</p>
<ol type="1">
<li>
<p><strong>Definire una griglia di valori di <span class="math inline">\(\theta\)</span></strong>, ad esempio 1000 punti equispaziati tra 0 e 1:</p>
<p><span class="math display">\[
\theta_1, \theta_2, \dots, \theta_J.
\]</span></p>
</li>
<li>
<p><strong>Calcolare la posteriore su ciascun punto della griglia.</strong> Nel caso Beta–Binomiale:</p>
<p><span class="math display">\[
p(\theta_j \mid y) \propto \theta_j^{\,k+a-1}(1-\theta_j)^{n-k+b-1}.
\]</span></p>
<p>Poi normalizzare per avere somme che valgono 1:</p>
<p><span class="math display">\[
w_j = \frac{p(\theta_j \mid y)}{\sum_{\ell=1}^J p(\theta_\ell \mid y)}.
\]</span></p>
</li>
<li>
<p><strong>Combinare le predizioni condizionate</strong>. Per ogni valore futuro <span class="math inline">\(x=0,\dots,m\)</span>, si calcola:</p>
<p><span class="math display">\[
p(\tilde y = x \mid y) \approx \sum_{j=1}^J w_j \, \binom{m}{x}\theta_j^x(1-\theta_j)^{m-x}.
\]</span></p>
</li>
<li>
<p><strong>Interpretazione</strong>: la pmf ottenuta è la nostra approssimazione numerica della distribuzione predittiva posteriore. Da essa possiamo:</p>
<ul>
<li>calcolare probabilità,</li>
<li>generare campioni di <span class="math inline">\(\tilde y\)</span>,</li>
<li>confrontare la predizione con i dati osservati.</li>
</ul>
</li>
</ol>
<p><strong>Da ricordare:</strong></p>
<ul>
<li>La predittiva non si ottiene facendo la media dei valori di <span class="math inline">\(\tilde y\)</span>, ma costruendo <strong>un’intera distribuzione di probabilità</strong>.</li>
<li>Il metodo su griglia è il più semplice: discretizza <span class="math inline">\(\theta\)</span>, pesa ogni valore con la sua plausibilità a posteriori, e combina le predizioni condizionate.</li>
<li>In problemi più complessi, la stessa logica viene implementata tramite <strong>MCMC</strong>: invece di usare una griglia fissa, si usano campioni <span class="math inline">\(\theta^{(s)}\)</span> dalla posteriore.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio numerico completo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio numerico completo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Esaminiamo ora uno script in R che implementa passo per passo l’approssimazione della distribuzione predittiva posteriore binomiale con il metodo su griglia.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># ESEMPIO DIDATTICO: predittiva posteriore per Binomiale con metodo su griglia</span></span>
<span><span class="co"># Dati e prior</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">10</span>     <span class="co"># successi osservati</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">50</span>     <span class="co"># prove osservate</span></span>
<span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fl">1</span>      <span class="co"># prior Beta(a, b)</span></span>
<span><span class="va">b</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span><span class="va">m</span> <span class="op">&lt;-</span> <span class="fl">10</span>     <span class="co"># numero di prove future per la predizione (scelta didattica)</span></span>
<span><span class="va">J</span> <span class="op">&lt;-</span> <span class="fl">2000</span>   <span class="co"># numero di punti griglia su theta in [0,1]</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 1: Griglia su theta</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span></span>
<span><span class="va">theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="va">J</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 2: Densità posteriore non normalizzata su ogni punto di griglia</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span></span>
<span><span class="co"># Posteriore ~ Beta(a + k, b + n - k)  -&gt; densità proporzionale a theta^(a+k-1) (1-theta)^(b+n-k-1)</span></span>
<span><span class="va">post_unnorm</span> <span class="op">&lt;-</span> <span class="va">theta</span><span class="op">^</span><span class="op">(</span><span class="va">a</span> <span class="op">+</span> <span class="va">k</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">theta</span><span class="op">)</span><span class="op">^</span><span class="op">(</span><span class="va">b</span> <span class="op">+</span> <span class="va">n</span> <span class="op">-</span> <span class="va">k</span> <span class="op">-</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Normalizzazione per ottenere pesi che sommano a 1</span></span>
<span><span class="va">w</span> <span class="op">&lt;-</span> <span class="va">post_unnorm</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">post_unnorm</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 3: combinare le predittive condizionate p(tilde y | theta)</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># Obiettivo: costruire la pmf predittiva p(tilde y = x | y) </span></span>
<span><span class="co"># per ogni x = 0,...,m come media pesata (sulla griglia di θ) </span></span>
<span><span class="co"># delle pmf condizionate binomiali.</span></span>
<span></span>
<span><span class="co"># 1) Definiamo i valori futuri possibili di tilde y</span></span>
<span><span class="va">x_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">m</span></span>
<span></span>
<span><span class="co"># 2) Inizializziamo una matrice vuota: </span></span>
<span><span class="co">#    - J righe (una per ciascun θ_j della griglia)</span></span>
<span><span class="co">#    - (m+1) colonne (una per ogni valore possibile di x)</span></span>
<span><span class="va">px_given_theta</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="cn">NA_real_</span>, nrow <span class="op">=</span> <span class="va">J</span>, ncol <span class="op">=</span> <span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3) Riempiamo la matrice: per ogni θ_j (riga j) e per ogni x (colonna i)</span></span>
<span><span class="co">#    calcoliamo P(tilde y = x | θ_j) = Binomiale(x | m, θ_j)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">J</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">x</span> <span class="op">&lt;-</span> <span class="va">x_vals</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>    <span class="va">px_given_theta</span><span class="op">[</span><span class="va">j</span>, <span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">x</span>, size <span class="op">=</span> <span class="va">m</span>, prob <span class="op">=</span> <span class="va">theta</span><span class="op">[</span><span class="va">j</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># 4) Combinazione pesata:</span></span>
<span><span class="co">#    p(tilde y = x | y) ≈ somma_j w_j * P(tilde y = x | θ_j).</span></span>
<span><span class="co">#    Per ciascun valore di x (colonna i), facciamo la somma pesata.</span></span>
<span><span class="va">pred_pmf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">numeric</a></span><span class="op">(</span><span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">m</span> <span class="op">+</span> <span class="fl">1</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">pred_pmf</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">w</span> <span class="op">*</span> <span class="va">px_given_theta</span><span class="op">[</span>, <span class="va">i</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Nota didattica:</span></span>
<span><span class="co"># - Ogni colonna della matrice px_given_theta contiene le probabilità condizionate </span></span>
<span><span class="co">#   P(tilde y = x_i | θ_j) per tutti i valori di griglia θ_j.</span></span>
<span><span class="co"># - Moltiplicando riga per riga queste probabilità per i pesi posteriori w_j </span></span>
<span><span class="co">#   e sommando, otteniamo la probabilità predittiva p(tilde y = x_i | y).</span></span>
<span><span class="co"># - In questo modo l'integrale viene approssimato da una somma pesata.</span></span>
<span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span><span class="co"># PASSAGGIO 4: Risultato: una pmf su {0,1,...,m}</span></span>
<span><span class="co"># -------------------------------------------------------------</span></span>
<span></span>
<span><span class="va">pred_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x_vals</span>, p <span class="op">=</span> <span class="va">pred_pmf</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">)</span></span>
<span><span class="co">#&gt;     x          p</span></span>
<span><span class="co">#&gt; 1   0 0.11391218</span></span>
<span><span class="co">#&gt; 2   1 0.25060680</span></span>
<span><span class="co">#&gt; 3   2 0.27617892</span></span>
<span><span class="co">#&gt; 4   3 0.19946255</span></span>
<span><span class="co">#&gt; 5   4 0.10397516</span></span>
<span><span class="co">#&gt; 6   5 0.04068593</span></span>
<span><span class="co">#&gt; 7   6 0.01205509</span></span>
<span><span class="co">#&gt; 8   7 0.00266151</span></span>
<span><span class="co">#&gt; 9   8 0.00041780</span></span>
<span><span class="co">#&gt; 10  9 0.00004200</span></span>
<span><span class="co">#&gt; 11 10 0.00000205</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">p</span><span class="op">)</span>  <span class="co"># dovrebbe essere ~1</span></span>
<span><span class="co">#&gt; [1] 1</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># (Opzionale) campionamento dalla predittiva posteriore approssimata</span></span>
<span><span class="co"># Estrae N valori da {0,...,m} con le probabilità 'p'</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">5000</span></span>
<span><span class="va">tilde_y_samples</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">x</span>, size <span class="op">=</span> <span class="va">N</span>, replace <span class="op">=</span> <span class="cn">TRUE</span>, prob <span class="op">=</span> <span class="va">pred_df</span><span class="op">$</span><span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Controllo: istogramma delle simulazioni vs pmf teorica approssimata</span></span>
<span><span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_histogram</span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">tilde_y_samples</span><span class="op">)</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="fu">after_stat</span><span class="op">(</span><span class="va">density</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    binwidth <span class="op">=</span> <span class="fl">1</span>, breaks <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">0.5</span>, <span class="va">m</span> <span class="op">+</span> <span class="fl">0.5</span>, by <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>, fill <span class="op">=</span> <span class="st">"skyblue"</span>, </span>
<span>    color <span class="op">=</span> <span class="st">"black"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">geom_point</span><span class="op">(</span>data <span class="op">=</span> <span class="va">pred_df</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">p</span><span class="op">)</span>, pch <span class="op">=</span> <span class="fl">19</span>, cex <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span>data <span class="op">=</span> <span class="va">pred_df</span>, <span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">p</span><span class="op">)</span>, lwd <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">ylim</span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">p</span><span class="op">)</span> <span class="op">*</span> <span class="fl">1.1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu">labs</span><span class="op">(</span></span>
<span>    title <span class="op">=</span> <span class="st">"Posterior Predictive (grid) – m=10"</span>,</span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu">tilde</span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Density"</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="03_model_comparison_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<blockquote class="blockquote">
<p><strong>Nota:</strong> il vettore <code>pred_df$p</code> è la <em>pmf</em> della predittiva posteriore approssimata; da qui si leggono probabilità, si calcolano quantità riassuntive e si può estrarre <span class="math inline">\(\tilde y\)</span>.</p>
</blockquote>
<p><strong>Verifica quando esiste la formula chiusa:</strong> Quando prior e likelihood sono coniugate (Beta + Binomiale), la predittiva è <em>Beta–Binomiale</em>. Possiamo usarla solo come verifica didattica:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Confronto con Beta-Binomiale (se applicabile)</span></span>
<span><span class="va">a_post</span> <span class="op">&lt;-</span> <span class="va">a</span> <span class="op">+</span> <span class="va">k</span></span>
<span><span class="va">b_post</span> <span class="op">&lt;-</span> <span class="va">b</span> <span class="op">+</span> <span class="va">n</span> <span class="op">-</span> <span class="va">k</span></span>
<span></span>
<span><span class="co"># pmf beta-binomial (con funzione base: dbetabinom in VGAM, altrimenti la implementiamo)</span></span>
<span><span class="va">dbetabinom</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">m</span>, <span class="va">a</span>, <span class="va">b</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Beta-Binomiale: choose(m, x) * Beta(x+a, m-x+b) / Beta(a, b)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Special.html">choose</a></span><span class="op">(</span><span class="va">m</span>, <span class="va">x</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">beta</a></span><span class="op">(</span><span class="va">x</span> <span class="op">+</span> <span class="va">a</span>, <span class="va">m</span> <span class="op">-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">b</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/Special.html">beta</a></span><span class="op">(</span><span class="va">a</span>, <span class="va">b</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="va">bb_pmf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fl">0</span><span class="op">:</span><span class="va">m</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="fu">dbetabinom</span><span class="op">(</span><span class="va">x</span>, <span class="va">m</span>, <span class="va">a_post</span>, <span class="va">b_post</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cbind.html">cbind</a></span><span class="op">(</span>grid <span class="op">=</span> <span class="va">pred_df</span><span class="op">$</span><span class="va">p</span>, beta_binom <span class="op">=</span> <span class="va">bb_pmf</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">6</span>, <span class="op">]</span>  <span class="co"># prime 6 righe a confronto</span></span>
<span><span class="co">#&gt;        grid beta_binom</span></span>
<span><span class="co">#&gt; [1,] 0.1139     0.1139</span></span>
<span><span class="co">#&gt; [2,] 0.2506     0.2506</span></span>
<span><span class="co">#&gt; [3,] 0.2762     0.2762</span></span>
<span><span class="co">#&gt; [4,] 0.1995     0.1995</span></span>
<span><span class="co">#&gt; [5,] 0.1040     0.1040</span></span>
<span><span class="co">#&gt; [6,] 0.0407     0.0407</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">pred_df</span><span class="op">$</span><span class="va">p</span> <span class="op">-</span> <span class="va">bb_pmf</span><span class="op">)</span><span class="op">)</span>   <span class="co"># lo scarto massimo (dovrebbe essere ~0 con J grande)</span></span>
<span><span class="co">#&gt; [1] 1.68e-15</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="notazione-e-terminologia" class="level3" data-number="44.2.3"><h3 data-number="44.2.3" class="anchored" data-anchor-id="notazione-e-terminologia">
<span class="header-section-number">44.2.3</span> Notazione e terminologia</h3>
<p><strong>Notazione:</strong> Useremo talvolta la forma compatta <span class="math inline">\(p(\cdot \mid y)\)</span> per indicare la predittiva posteriore del modello. Quando ci servirà evidenziare la previsione <em>marginale</em> per una singola osservazione <span class="math inline">\(y_i\)</span>, scriveremo:</p>
<p><span class="math display">\[
p(y_i \mid y) = \int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta,
\]</span> cioè la verosimiglianza <span class="math inline">\(p(y_i\mid\theta)\)</span> integrata rispetto alla posteriore <span class="math inline">\(p(\theta\mid y)\)</span>.</p>
<p><strong>Idea chiave:</strong> La predittiva posteriore propaga l’incertezza sui parametri alle previsioni. È questo passaggio a rendere le valutazioni predittive coerenti con il principio bayesiano, e quindi utilizzabili nel confronto tra modelli e nella stima di quantità legate alla “distanza” dal generatore dei dati.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Mappa concettuale">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mappa concettuale
</div>
</div>
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 39%">
<col style="width: 34%">
</colgroup>
<thead><tr class="header">
<th>Quantità</th>
<th>Significato</th>
<th>Uso principale</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid \theta)\)</span></td>
<td>Verosimiglianza</td>
<td>Calcolo predittivo</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(\theta \mid y)\)</span></td>
<td>Distribuzione posteriore</td>
<td>Ponderazione</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p(y_i \mid y)\)</span></td>
<td>Predizione bayesiana media</td>
<td>Log-score, LPPD</td>
</tr>
<tr class="even">
<td><span class="math inline">\(p(y_i \mid y_{-i})\)</span></td>
<td>Predizione LOO (<em>leave-one-out</em>)</td>
<td>ELPD</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(p(\tilde{y} \mid y)\)</span></td>
<td>Distribuzione predittiva complessiva</td>
<td>Divergenza KL, confronto modelli</td>
</tr>
</tbody>
</table>
</div>
</div>
</section></section><section id="il-problema-fondamentale-della-valutazione-predittiva" class="level2" data-number="44.3"><h2 data-number="44.3" class="anchored" data-anchor-id="il-problema-fondamentale-della-valutazione-predittiva">
<span class="header-section-number">44.3</span> Il problema fondamentale della valutazione predittiva</h2>
<p>Ora che sappiamo come costruire previsioni bayesiane, affrontiamo la domanda centrale: <em>come valutiamo la qualità di queste previsioni?</em></p>
<section id="il-dilemma-teorico" class="level3" data-number="44.3.1"><h3 data-number="44.3.1" class="anchored" data-anchor-id="il-dilemma-teorico">
<span class="header-section-number">44.3.1</span> Il dilemma teorico</h3>
<p>Quando costruiamo un modello, vogliamo sapere <em>quanto bene riesce a predire dati futuri</em>. In altre parole: se ripetessimo l’esperimento o raccogliessimo nuovi dati, quanto sarebbero vicine le predizioni del modello a quei dati?</p>
<p>Per formalizzare questa idea, distinguiamo tra due distribuzioni:</p>
<ul>
<li>
<em>la distribuzione vera dei dati futuri</em> <span class="math inline">\(p(\tilde{y})\)</span>, che purtroppo non conosciamo,</li>
<li>
<em>la distribuzione predittiva del modello</em> <span class="math inline">\(p(\tilde{y} \mid y)\)</span>, cioè le predizioni basate sui dati osservati <span class="math inline">\(y\)</span>.</li>
</ul>
<p>L’obiettivo è misurare <em>quanto <span class="math inline">\(p(\tilde{y} \mid y)\)</span> si avvicina a <span class="math inline">\(p(\tilde{y})\)</span></em>.</p>
</section><section id="una-misura-di-distanza-la-divergenza-di-kullbackleibler" class="level3" data-number="44.3.2"><h3 data-number="44.3.2" class="anchored" data-anchor-id="una-misura-di-distanza-la-divergenza-di-kullbackleibler">
<span class="header-section-number">44.3.2</span> Una misura di distanza: la divergenza di Kullback–Leibler</h3>
<p>La <em>divergenza di Kullback–Leibler</em> (KL) fornisce una misura di questa distanza:</p>
<p><span id="eq-kl-divergence-def"><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_{p} \left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right].
\tag{44.2}\]</span></span> Interpretazione intuitiva:</p>
<ul>
<li>se <span class="math inline">\(q\)</span> (il nostro modello) assegna probabilità alte agli stessi eventi che sono probabili in <span class="math inline">\(p\)</span> (la realtà), la distanza sarà piccola,</li>
<li>se invece <span class="math inline">\(q\)</span> “sbaglia bersaglio”, assegnando probabilità alte a eventi che in realtà sono rari, la distanza sarà grande.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Idea chiave.</strong> La KL divergence misura <em>quanta informazione perdiamo</em> se usiamo le predizioni del modello <span class="math inline">\(q\)</span> al posto della distribuzione vera <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</div>
</section><section id="un-ostacolo-pratico-insormontabile" class="level3" data-number="44.3.3"><h3 data-number="44.3.3" class="anchored" data-anchor-id="un-ostacolo-pratico-insormontabile">
<span class="header-section-number">44.3.3</span> Un ostacolo pratico insormontabile</h3>
<p>Il problema è che <span class="math inline">\(p(\tilde{y})\)</span> non lo conosciamo mai: non abbiamo accesso alla “vera” distribuzione dei dati futuri. Questa è la sfida fondamentale della validazione predittiva: come possiamo valutare la qualità delle nostre previsioni senza conoscere la verità?</p>
<p>Per questo dobbiamo ricorrere a strategie di approssimazione, come la <em>validazione incrociata</em> e criteri predittivi come <em>ELPD</em>, che vedremo nelle prossime sezioni.</p>
<section id="mini-esempio-intuitivo" class="level4" data-number="44.3.3.1"><h4 data-number="44.3.3.1" class="anchored" data-anchor-id="mini-esempio-intuitivo">
<span class="header-section-number">44.3.3.1</span> Mini-esempio intuitivo</h4>
<p>Immagina una moneta truccata che dà <em>testa</em> il 70% delle volte.</p>
<p>Vogliamo confrontare due modelli:</p>
<ul>
<li>
<em>Modello A</em>: ipotizza una moneta equa (<span class="math inline">\(p = 0.5\)</span>),</li>
<li>
<em>Modello B</em>: ipotizza una probabilità leggermente sbilanciata (<span class="math inline">\(p = 0.65\)</span>).</li>
</ul>
<p>Se sapessimo che la probabilità “vera” è <span class="math inline">\(p = 0.7\)</span>, sarebbe chiaro che il Modello B è più vicino alla realtà. La divergenza di Kullback–Leibler serve proprio a quantificare <em>quanta informazione perdiamo</em> quando ci affidiamo a un modello meno accurato (come A) invece che a uno più vicino alla verità (come B).</p>
<p>Il punto cruciale è che <em>nella pratica</em> non conosciamo mai la probabilità vera della moneta. Abbiamo soltanto i dati osservati, cioè gli esiti dei lanci. Per decidere quale modello predice meglio dobbiamo quindi basarci sui dati disponibili: è qui che entrano in gioco i metodi di confronto predittivo che studieremo.</p>
</section></section></section><section id="sec-logscore" class="level2" data-number="44.4"><h2 data-number="44.4" class="anchored" data-anchor-id="sec-logscore">
<span class="header-section-number">44.4</span> Il log-score: misurare l’accuratezza predittiva punto per punto</h2>
<p>Abbiamo definito la distribuzione predittiva posteriore e il problema teorico della valutazione. Ora introduciamo il primo strumento pratico: il <em>log-score</em>.</p>
<section id="la-domanda-di-base" class="level3" data-number="44.4.1"><h3 data-number="44.4.1" class="anchored" data-anchor-id="la-domanda-di-base">
<span class="header-section-number">44.4.1</span> La domanda di base</h3>
<p>Per ogni osservazione nei nostri dati, vogliamo sapere: <em>quanto il nostro modello considerava plausibile questo specifico risultato?</em> Il log-score risponde proprio a questa domanda.</p>
</section><section id="definizione-formale" class="level3" data-number="44.4.2"><h3 data-number="44.4.2" class="anchored" data-anchor-id="definizione-formale">
<span class="header-section-number">44.4.2</span> Definizione formale</h3>
<p>Il <em>log-score</em> per un’osservazione <span class="math inline">\(y_i\)</span> è semplicemente il logaritmo della probabilità che il modello assegnava a quell’osservazione:</p>
<p><span id="eq-log-score-def"><span class="math display">\[
\text{Log-score}(y_i) = \log p(y_i \mid y) ,
\tag{44.3}\]</span></span> dove <span class="math inline">\(p(y_i \mid y)\)</span> è la distribuzione predittiva posteriore che abbiamo appena imparato a calcolare:</p>
<p><span class="math display">\[
p(y_i \mid y) = \int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta .
\]</span></p>
</section><section id="interpretazione-scommettere-sui-dati" class="level3" data-number="44.4.3"><h3 data-number="44.4.3" class="anchored" data-anchor-id="interpretazione-scommettere-sui-dati">
<span class="header-section-number">44.4.3</span> Interpretazione: “scommettere” sui dati</h3>
<p>Il log-score può essere interpretato come quanto il modello avrebbe “scommesso” su quel particolare risultato:</p>
<ul>
<li>
<em>se il modello assegna </em>alta probabilità* a <span class="math inline">\(y_i\)</span>*: <span class="math inline">\(p(y_i \mid y) \approx 1\)</span>, quindi <span class="math inline">\(\log p(y_i \mid y) \approx 0\)</span> (buono);</li>
<li>
<em>se il modello assegna </em>bassa probabilità* a <span class="math inline">\(y_i\)</span>*: <span class="math inline">\(p(y_i \mid y) \approx 0\)</span>, quindi <span class="math inline">\(\log p(y_i \mid y) \ll 0\)</span> (molto negativo, scarso).</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Perché il logaritmo?</strong><br>
Il logaritmo trasforma prodotti di probabilità in somme. Così possiamo sommare contributi <em>punto per punto</em> dei dati invece di moltiplicarli; inoltre stabilizza i numeri molto piccoli tipici delle verosimiglianze.</p>
</div>
</div>
</div>
</section><section id="dal-singolo-dato-al-punteggio-totale" class="level3" data-number="44.4.4"><h3 data-number="44.4.4" class="anchored" data-anchor-id="dal-singolo-dato-al-punteggio-totale">
<span class="header-section-number">44.4.4</span> Dal singolo dato al punteggio totale</h3>
<p>Per avere una visione complessiva della performance del modello, sommiamo i log-score su tutte le osservazioni:</p>
<p><span id="eq-log-score-sum-def"><span class="math display">\[
S = \sum_{i=1}^n \log p(y_i \mid y) .
\tag{44.4}\]</span></span> Più <span class="math inline">\(S\)</span> è alto (meno negativo), più il modello “scommette” bene sui dati osservati (<em>in-sample</em>).</p>
</section><section id="due-filosofie-a-confronto-parametri-fissi-vs.-incerti" class="level3" data-number="44.4.5"><h3 data-number="44.4.5" class="anchored" data-anchor-id="due-filosofie-a-confronto-parametri-fissi-vs.-incerti">
<span class="header-section-number">44.4.5</span> Due filosofie a confronto: parametri fissi vs.&nbsp;incerti</h3>
<p>Il calcolo del log-score può seguire due approcci concettualmente diversi, che riflettono due filosofie statistiche diverse.</p>
<section id="approccio-classico-parametri-fissi" class="level4" data-number="44.4.5.1"><h4 data-number="44.4.5.1" class="anchored" data-anchor-id="approccio-classico-parametri-fissi">
<span class="header-section-number">44.4.5.1</span> Approccio classico: parametri fissi</h4>
<p>Nell’impostazione frequentista classica, usiamo una stima puntuale dei parametri (ad es. Massima Verosimiglianza o MAP) e ignoriamo l’incertezza:</p>
<p><span class="math display">\[
\text{Log-score classico} = \log p(y_i \mid \hat{\theta}).
\]</span></p>
</section><section id="approccio-bayesiano-gestione-dellincertezza-sui-parametri" class="level4" data-number="44.4.5.2"><h4 data-number="44.4.5.2" class="anchored" data-anchor-id="approccio-bayesiano-gestione-dellincertezza-sui-parametri">
<span class="header-section-number">44.4.5.2</span> Approccio bayesiano: gestione dell’incertezza sui parametri</h4>
<p>Nell’approccio bayesiano non fissiamo i parametri a un singolo valore stimato, ma li trattiamo come <em>incerti</em>. Questo significa che, invece di calcolare la verosimiglianza con un $$, “mescoliamo” tutte le verosimiglianze possibili, pesandole in base alla loro plausibilità a posteriori:</p>
<p><span id="eq-integral-p-h-mid-yi"><span class="math display">\[
\begin{align}
\text{Log-score bayesiano} &amp;= \log p(y_i \mid y) \\
&amp;= \log \int p(y_i \mid \theta)\, p(\theta \mid y)\, d\theta .
\end{align}
\tag{44.5}\]</span></span></p>
<p>In altre parole, la probabilità predittiva di <span class="math inline">\(y_i\)</span> non dipende da un solo <span class="math inline">\(\theta\)</span>, ma dalla <em>media delle predizioni condizionate su tutti i valori plausibili</em> dei parametri.</p>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Differenza chiave</strong><br>
- L’approccio frequentista chiede: <em>“Quanto sono plausibili i dati se i parametri valgono esattamente <span class="math inline">\(\hat{\theta}\)</span>?”</em><br>
- L’approccio bayesiano chiede: <em>“Quanto sono plausibili i dati, in media, considerando tutti i valori di <span class="math inline">\(\theta\)</span> compatibili con i dati osservati?”</em></p>
<p>La seconda prospettiva è più onesta perché riconosce l’incertezza sui parametri.</p>
</div>
</div>
</div>
</section></section><section id="implementazione-pratica-con-il-metodo-mcmc" class="level3" data-number="44.4.6"><h3 data-number="44.4.6" class="anchored" data-anchor-id="implementazione-pratica-con-il-metodo-mcmc">
<span class="header-section-number">44.4.6</span> Implementazione pratica con il metodo MCMC</h3>
<p>L’integrale nell’nell’<a href="#eq-integral-p-h-mid-yi" class="quarto-xref">Equazione&nbsp;<span>44.5</span></a> raramente ha una soluzione analitica (cioè non si può calcolare con una formula esatta). Possiamo però stimarlo in modo pratico ed efficiente utilizzando i campioni generati da un algoritmo MCMC (Markov Chain Monte Carlo).</p>
<p>Supponiamo di avere una serie di campioni di parametri, <span class="math inline">\(\theta^{(1)},\dots,\theta^{(S)}\)</span>, estratti dalla distribuzione a posteriori <span class="math inline">\(p(\theta\mid y)\)</span>. Il procedimento per approssimare la probabilità predittiva si articola in due semplici passi:</p>
<section id="passo-1-calcolare-la-verosimiglianza-per-ogni-campione" class="level4" data-number="44.4.6.1"><h4 data-number="44.4.6.1" class="anchored" data-anchor-id="passo-1-calcolare-la-verosimiglianza-per-ogni-campione">
<span class="header-section-number">44.4.6.1</span> Passo 1: calcolare la verosimiglianza per ogni campione</h4>
<p>Per ogni set di parametri <span class="math inline">\(\theta^{(s)}\)</span> che abbiamo campionato, calcoliamo la probabilità (verosimiglianza) di osservare il dato <span class="math inline">\(y_i\)</span> sotto quei parametri:</p>
<p><span class="math display">\[
p\bigl(y\_i \mid \theta^{(s)}\bigr).
\]</span></p>
<p>Fare questo per tutti i campioni <span class="math inline">\(S\)</span> ci fornisce un insieme di valori:</p>
<p><span class="math display">\[
\bigl\{\, p(y\_i \mid \theta^{(1)}),\; p(y\_i \mid \theta^{(2)}),\; \dots,\; p(y\_i \mid \theta^{(S)}) \,\bigr\}
\]</span> Questa collezione rappresenta come la plausibilità del dato <span class="math inline">\(y_i\)</span> cambi al variare dei parametri, ponderata per la loro probabilità a posteriori.</p>
</section><section id="passo-2-calcolare-la-media-dei-valori-ottenuti" class="level4" data-number="44.4.6.2"><h4 data-number="44.4.6.2" class="anchored" data-anchor-id="passo-2-calcolare-la-media-dei-valori-ottenuti">
<span class="header-section-number">44.4.6.2</span> Passo 2: calcolare la media dei valori ottenuti</h4>
<p>La probabilità predittiva per <span class="math inline">\(y_i\)</span> (che tiene conto di tutta l’incertezza sui parametri) è approssimata semplicemente calcolando la media aritmetica dell’insieme di valori ottenuti nel passo precedente:</p>
<p><span id="eq-mcmc-posterior-parameter-distr"><span class="math display">\[
p(y_i \mid y) \;\approx\; \frac{1}{S}\sum_{s=1}^S p\bigl(y_i \mid \theta^{(s)}\bigr).
\tag{44.6}\]</span></span></p>
<p>Il risultato è un <em>singolo valore numerico</em> (uno scalare) che sintetizza in una previsione probabilistica tutto ciò che abbiamo appreso sull’incertezza dei parametri del modello.</p>
</section></section><section id="la-lppd-una-misura-bayesiana-di-bontà-della-previsione" class="level3" data-number="44.4.7"><h3 data-number="44.4.7" class="anchored" data-anchor-id="la-lppd-una-misura-bayesiana-di-bontà-della-previsione">
<span class="header-section-number">44.4.7</span> La LPPD: una misura bayesiana di bontà della previsione</h3>
<p>Per valutare la capacità predittiva dell’intero modello su tutti i dati, ripetiamo il procedimento per ogni osservazione <span class="math inline">\(y_i\)</span> e procediamo così:</p>
<ol type="1">
<li>per ogni osservazione <span class="math inline">\(y_i\)</span>, calcoliamo la sua probabilità predittiva media <span class="math inline">\(p(y_i \mid y)\)</span>;</li>
<li>prendiamo il logaritmo naturale di questa probabilità. (Usiamo il logaritmo per motivi computazionali e perché trasforma prodotti in somme);</li>
<li>sommiamo i logaritmi di tutte le <span class="math inline">\(n\)</span> osservazioni.</li>
</ol>
<p>Il risultato di questo processo è la <em>Log Pointwise Predictive Density (LPPD)</em>:</p>
<p><span id="eq-lppd-def"><span class="math display">\[
\text{LPPD} = \sum_{i=1}^n \log \left[ \frac{1}{S} \sum_{s=1}^S p\bigl(y_i \mid \theta^{(s)}\bigr) \right].
\tag{44.7}\]</span></span></p>
<p><strong>Confronto e Sintesi:</strong></p>
<ul>
<li>Il <em>log-score classico</em> (usato nella statistica frequentista) valuta la previsione utilizzando un unico valore stimato dei parametri (ad esempio, la stima di massima verosimiglianza <span class="math inline">\(\hat{\theta}\)</span>). Questo ignora completamente l’incertezza esistente sulla stima dei parametri.</li>
<li>La <em>LPPD</em> bayesiana compie la stessa operazione fondamentale, ma in modo più robusto: invece di usare un singolo valore dei parametri, <em>media le previsioni</em> su tutte le migliaia di valori plausibili dei parametri campionati dalla distribuzione a posteriori. In questo modo, la misura di bontà predittiva incorpora in modo naturale tutta l’incertezza del modello.</li>
</ul></section><section id="il-problema-nascosto-overfitting-in-sample" class="level3" data-number="44.4.8"><h3 data-number="44.4.8" class="anchored" data-anchor-id="il-problema-nascosto-overfitting-in-sample">
<span class="header-section-number">44.4.8</span> Il problema nascosto: overfitting in-sample</h3>
<p>La LPPD è calcolata sugli stessi dati usati per stimare il modello: modelli molto flessibili possono “scommettere bene” anche sul rumore, gonfiando la LPPD <em>in-sample</em>.</p>
<p><strong>Analogia:</strong> È come valutare uno studente facendogli ripetere gli stessi esercizi che ha già risolto durante lo studio. Otterrà un punteggio alto, ma non sappiamo se sarebbe altrettanto bravo con problemi nuovi.</p>
<p>Per valutare la capacità di generalizzazione, serve una stima <em>out-of-sample</em>. Nelle prossime sezioni introdurremo la validazione incrociata <em>leave-one-out</em> (LOO-CV) e l’<em>ELPD</em> (<em>Expected Log Pointwise Predictive Density</em>), che forniscono una versione “fuori campione” della LPPD per il confronto predittivo tra modelli.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio pratico del calcolo LPPD">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio pratico del calcolo LPPD
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Consideriamo un singolo dato <span class="math inline">\(y_i = 3\)</span> successi su <span class="math inline">\(n=5\)</span> tentativi (Binomiale). Abbiamo tre valori plausibili per <span class="math inline">\(\theta\)</span> dalla posterior, con pesi didattici <span class="math inline">\(w^{(s)}\)</span> (nella pratica MCMC i pesi sono uguali):</p>
<ul>
<li>
<span class="math inline">\(\theta^{(1)}=0.3\)</span> con <span class="math inline">\(w^{(1)}=0.2\)</span><br>
</li>
<li>
<span class="math inline">\(\theta^{(2)}=0.5\)</span> con <span class="math inline">\(w^{(2)}=0.5\)</span><br>
</li>
<li>
<span class="math inline">\(\theta^{(3)}=0.7\)</span> con <span class="math inline">\(w^{(3)}=0.3\)</span>
</li>
</ul>
<p>Per ogni campione <span class="math inline">\(\theta^{(s)}\)</span> calcoliamo <span class="math inline">\(p(y_i \mid \theta^{(s)})\)</span>, otteniamo la <em>collezione di likelihood</em> <span class="math inline">\(\{p(y_i\mid \theta^{(s)})\}_{s=1}^S\)</span>, poi facciamo la media pesata (eq. <a href="#eq-mcmc-posterior-parameter-distr" class="quarto-xref">Equazione&nbsp;<span>44.6</span></a>) per ottenere <span class="math inline">\(p(y_i\mid y)\)</span>, e infine il <em>log-score</em> <span class="math inline">\(\log p(y_i\mid y)\)</span> (eq. <a href="#eq-log-score-def" class="quarto-xref">Equazione&nbsp;<span>44.3</span></a>).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dato osservato (un solo punto)</span></span>
<span><span class="va">y_i</span>  <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span><span class="va">n_i</span>  <span class="op">&lt;-</span> <span class="fl">5</span></span>
<span></span>
<span><span class="co"># "Campioni" posteriori (qui pochi e con pesi espliciti per chiarezza didattica)</span></span>
<span><span class="va">theta_vals</span>        <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span><span class="op">)</span>      <span class="co"># θ^(1), θ^(2), θ^(3)</span></span>
<span><span class="va">posterior_weights</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span><span class="op">)</span>      <span class="co"># w^(1), w^(2), w^(3); in MCMC tipicamente uguali</span></span>
<span></span>
<span><span class="co"># (1) Likelihood punto-per-punto: p(y_i | θ^(s))</span></span>
<span><span class="va">likelihoods</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_i</span>, size <span class="op">=</span> <span class="va">n_i</span>, prob <span class="op">=</span> <span class="va">theta_vals</span><span class="op">)</span></span>
<span><span class="va">likelihoods</span>  <span class="co"># questa è la collezione { p(y_i | θ^(s)) }_s</span></span>
<span><span class="co">#&gt; [1] 0.132 0.312 0.309</span></span>
<span></span>
<span><span class="co"># (2) Media (pesata) sulle likelihood ⇒ p(y_i | y) ≈ Σ_s w^(s) p(y_i | θ^(s))</span></span>
<span><span class="va">p_yi_given_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">posterior_weights</span> <span class="op">*</span> <span class="va">likelihoods</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># (3) Log-score (per un solo dato coincide con la LPPD del singolo punto)</span></span>
<span><span class="va">log_score_i</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p_yi_given_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stampa riassuntiva con notazione coerente</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Campioni θ^{(s)}:        "</span>, <span class="va">theta_vals</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Campioni θ^{(s)}:         0.3 0.5 0.7</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Likelihood p(y_i|θ^{(s)}):"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">likelihoods</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Likelihood p(y_i|θ^{(s)}): 0.132 0.312 0.309</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"p(y_i|y) (media pesata):  "</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">p_yi_given_y</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; p(y_i|y) (media pesata):   0.275</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"log p(y_i|y):             "</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">log_score_i</span>, <span class="fl">4</span><span class="op">)</span>, <span class="st">"\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; log p(y_i|y):              -1.29</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Nota didattica:</strong> Nella pratica con MCMC i pesi sono uguali, <span class="math inline">\(w^{(s)}=\tfrac{1}{S}\)</span>, quindi <span class="math inline">\(p(y_i\mid y) \approx \tfrac{1}{S}\sum_{s=1}^S p(y_i\mid \theta^{(s)})\)</span> (<a href="#eq-mcmc-posterior-parameter-distr" class="quarto-xref">Equazione&nbsp;<span>44.6</span></a>). Con più osservazioni <span class="math inline">\(\{y_i\}_{i=1}^n\)</span>, la <em>LPPD</em> è la somma dei log-score punto-per-punto (<a href="#eq-lppd-def" class="quarto-xref">Equazione&nbsp;<span>44.7</span></a>).</p>
</div>
</div>
</div>
</section></section><section id="la-svolta-dalladattamento-alla-generalizzazione" class="level2" data-number="44.5"><h2 data-number="44.5" class="anchored" data-anchor-id="la-svolta-dalladattamento-alla-generalizzazione">
<span class="header-section-number">44.5</span> La svolta: dall’adattamento alla generalizzazione</h2>
<section id="il-problema-delloverfitting-spiegato" class="level3" data-number="44.5.1"><h3 data-number="44.5.1" class="anchored" data-anchor-id="il-problema-delloverfitting-spiegato">
<span class="header-section-number">44.5.1</span> Il problema dell’overfitting spiegato</h3>
<p>Immagina di voler valutare la capacità di uno studente di <em>riconoscere emozioni nei volti</em>.</p>
<ol type="1">
<li>
<em>Scenario A:</em> lo testi sempre con le <em>stesse fotografie</em> che ha già visto molte volte durante l’allenamento.</li>
<li>
<em>Scenario B:</em> lo testi con <em>nuove fotografie</em> di persone mai viste prima.</li>
</ol>
<p>Nel primo caso, lo studente probabilmente avrà un punteggio molto alto, ma non sapremo se ha davvero imparato a riconoscere le emozioni o se si limita a ricordare quelle immagini specifiche. Il secondo scenario, invece, è più onesto: misura la capacità di generalizzare la competenza a stimoli nuovi.</p>
<p>Lo stesso accade con i modelli statistici.</p>
<ul>
<li>La <em>LPPD</em> corrisponde allo <em>Scenario A</em>: valuta il modello sugli stessi dati usati per adattarlo, rischiando di dare un’illusione di performance eccellente.</li>
<li>Per sapere se il modello sa davvero “generalizzare”, serve testarlo come nello <em>Scenario B</em>: con dati nuovi o tramite tecniche di validazione incrociata.</li>
</ul></section><section id="guardare-oltre-i-dati-osservati" class="level3" data-number="44.5.2"><h3 data-number="44.5.2" class="anchored" data-anchor-id="guardare-oltre-i-dati-osservati">
<span class="header-section-number">44.5.2</span> Guardare oltre i dati osservati</h3>
<p>Quando valutiamo un modello, non ci basta sapere <em>quanto bene spiega i dati che ha già visto</em>. La vera domanda è: <em>quanto bene predirebbe dati nuovi, mai osservati?</em></p>
<p>La <em>Expected Log Predictive Density (ELPD)</em> risponde a questa domanda. La logica è la stessa della LPPD, ma con una differenza cruciale: la previsione di ogni osservazione <span class="math inline">\(y_i\)</span> viene fatta <em>senza usare <span class="math inline">\(y_i\)</span> per stimare il modello</em>. Questa tecnica si chiama <em>Leave-One-Out (LOO)</em>:</p>
<p><span id="eq-elpd-def"><span class="math display">\[
\text{ELPD} = \sum_{i=1}^n \log p(y_i \mid y_{-i}),
\tag{44.8}\]</span></span> dove <span class="math inline">\(y_{-i}\)</span> indica il dataset a cui è stata tolta l’osservazione <span class="math inline">\(i\)</span>.</p>
<section id="un-esempio-concreto-per-chiarire-la-differenza" class="level4" data-number="44.5.2.1"><h4 data-number="44.5.2.1" class="anchored" data-anchor-id="un-esempio-concreto-per-chiarire-la-differenza">
<span class="header-section-number">44.5.2.1</span> Un esempio concreto per chiarire la differenza</h4>
<p>Immagina di voler costruire un modello che predice i <em>punteggi di memoria a breve termine</em> degli studenti a partire dal loro livello di concentrazione.</p>
<ul>
<li>Con la <em>LPPD</em>, il modello viene valutato sugli stessi studenti che sono serviti per stimarlo. È come dire: <em>“quanto bene il modello spiega questi dati noti?”</em>.<br>
</li>
<li>Con la <em>ELPD</em>, invece, ogni volta togliamo uno studente dal campione, stimiamo il modello sugli altri e proviamo a predire il punteggio di quello escluso. È come chiedere: <em>“quanto bene il modello predirebbe un nuovo studente, mai visto prima?”</em>.</li>
</ul></section><section id="procedura-passo-per-passo" class="level4" data-number="44.5.2.2"><h4 data-number="44.5.2.2" class="anchored" data-anchor-id="procedura-passo-per-passo">
<span class="header-section-number">44.5.2.2</span> Procedura passo per passo</h4>
<ol type="1">
<li>Prendiamo il primo studente ed escludiamolo dal dataset.<br>
</li>
<li>Adattiamo il modello usando i dati dei rimanenti studenti.<br>
</li>
<li>Prediciamo il punteggio di memoria dello studente escluso.<br>
</li>
<li>Ripetiamo lo stesso procedimento per ogni studente, uno alla volta.<br>
</li>
<li>Sommiamo tutti i log-score ottenuti: questo è l’ELPD.</li>
</ol></section></section></section><section id="il-collegamento-con-la-divergenza-kl" class="level2" data-number="44.6"><h2 data-number="44.6" class="anchored" data-anchor-id="il-collegamento-con-la-divergenza-kl">
<span class="header-section-number">44.6</span> Il collegamento con la divergenza KL</h2>
<section id="la-teoria-che-unifica-tutto" class="level3" data-number="44.6.1"><h3 data-number="44.6.1" class="anchored" data-anchor-id="la-teoria-che-unifica-tutto">
<span class="header-section-number">44.6.1</span> La teoria che unifica tutto</h3>
<p>Abbiamo visto che l’ELPD fornisce una misura empirica della capacità predittiva di un modello. Esiste, tuttavia, una giustificazione teorica profonda e unificante che spiega il motivo per cui massimizzare l’ELPD è il principio corretto per la selezione dei modelli. Questa giustificazione poggia sul concetto di <em>divergenza di Kullback-Leibler (KL)</em>.</p>
<section id="cosa-misura-la-divergenza-kl" class="level4" data-number="44.6.1.1"><h4 data-number="44.6.1.1" class="anchored" data-anchor-id="cosa-misura-la-divergenza-kl">
<span class="header-section-number">44.6.1.1</span> Cosa misura la divergenza KL?</h4>
<p>La divergenza KL, indicata come <span class="math inline">\(D_{\text{KL}}\)</span>, misura la “distanza” informazionale tra la distribuzione vera dei dati, <span class="math inline">\(p(\tilde{y})\)</span> (la realtà che vogliamo catturare), e la distribuzione predittiva del nostro modello, <span class="math inline">\(q(\tilde{y} \mid y)\)</span> (la nostra approssimazione). È definita come:</p>
<p><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \mathbb{E}_p \left[ \log \frac{p(\tilde{y})}{q(\tilde{y} \mid y)} \right],
\]</span> dove l’aspettativa <span class="math inline">\(\mathbb{E}_p\)</span> è calcolata rispetto alla distribuzione vera <span class="math inline">\(p(\tilde{y})\)</span>.</p>
</section><section id="scomponiamo-la-divergenza-kl" class="level4" data-number="44.6.1.2"><h4 data-number="44.6.1.2" class="anchored" data-anchor-id="scomponiamo-la-divergenza-kl">
<span class="header-section-number">44.6.1.2</span> Scomponiamo la divergenza KL</h4>
<p>Per comprendere a fondo, espandiamo la definizione:</p>
<p><span id="eq-kl-decomposition"><span class="math display">\[
D_{\text{KL}}(p \parallel q) = \underbrace{\mathbb{E}_p[\log p(\tilde{y})]}_{\text{(1) Entropia}} - \underbrace{\mathbb{E}_p[\log q(\tilde{y} \mid y)]}_{\text{(2) Accuratezza predittiva}}.
\tag{44.9}\]</span></span></p>
<p>Analizziamo i due termini:</p>
<ol type="1">
<li>
<em><span class="math inline">\(\mathbb{E}_p[\log p(\tilde{y})]\)</span> (Entropia)</em>: Rappresenta il contenuto informativo intrinseco della distribuzione vera. È una quantità fissa, immutabile e, soprattutto, <em>indipendente dal modello</em> che stiamo considerando. È una costante.</li>
<li>
<em><span class="math inline">\(-\mathbb{E}_p[\log q(\tilde{y} \mid y)]\)</span> (Log-verosimiglianza attesa)</em>: Questo è il termine cruciale. Misura quanto è buona la nostra distribuzione predittiva <span class="math inline">\(q\)</span> nel prevedere nuovi dati provenienti dalla vera distribuzione <span class="math inline">\(p\)</span>. Nota: questo è esattamente l’opposto della quantità che <em>stimiamo con l’ELPD</em> <span class="math inline">\((\sum \log q(\tilde{y} \mid y))\)</span>.</li>
</ol></section><section id="il-collegamento-fondamentale" class="level4" data-number="44.6.1.3"><h4 data-number="44.6.1.3" class="anchored" data-anchor-id="il-collegamento-fondamentale">
<span class="header-section-number">44.6.1.3</span> Il collegamento fondamentale</h4>
<p>Poiché il primo termine dell’<a href="#eq-kl-decomposition" class="quarto-xref">Equazione&nbsp;<span>44.9</span></a> è una costante, <em>minimizzare la divergenza KL <span class="math inline">\(D_{\text{KL}}(p \parallel q)\)</span> equivale esattamente a massimizzare il secondo termine</em>, ovvero l’accuratezza predittiva attesa. Questo risultato si traduce in una regola pratica potentissima per il confronto tra modelli. Date due distribuzioni predittive, <span class="math inline">\(q_A\)</span> e <span class="math inline">\(q_B\)</span>, la differenza nelle loro divergenze KL è:</p>
<p><span class="math display">\[
\begin{aligned}
D_{\text{KL}}(p \parallel q_A) - D_{\text{KL}}(p \parallel q_B) &amp;= \left( \cancel{\mathbb{E}_p[\log p(\tilde{y})]} - \mathbb{E}_p[\log q_A(\tilde{y} \mid y)] \right) \notag\\
&amp;\qquad - \left( \cancel{\mathbb{E}_p[\log p(\tilde{y})]} - \mathbb{E}_p[\log q_B(\tilde{y} \mid y)] \right) \\
&amp;= \mathbb{E}_p[\log q_B(\tilde{y} \mid y)] - \mathbb{E}_p[\log q_A(\tilde{y} \mid y)] \\
&amp;= \text{ELPD}(q_B) - \text{ELPD}(q_A)
\end{aligned}
\]</span> dove abbiamo cancellato il termine entropia costante.</p>
</section><section id="conclusione-teorica-fondamentale" class="level4" data-number="44.6.1.4"><h4 data-number="44.6.1.4" class="anchored" data-anchor-id="conclusione-teorica-fondamentale">
<span class="header-section-number">44.6.1.4</span> Conclusione teorica fondamentale</h4>
<p>Il risultato precedente ci porta alla conclusione chiave di tutta la teoria:</p>
<p><span class="math display">\[
\text{Massimizzare l'ELPD} \;\; \equiv \;\; \text{Minimizzare la divergenza KL dalla verità}.
\]</span> In altre parole, quando preferiamo il modello con l’ELPD più alto, non stiamo solo seguendo un criterio empirico. Stiamo scegliendo consapevolmente il modello la cui distribuzione predittiva è, in media, <em>più vicina alla realtà sottostante</em> in senso informazionale. Questo principio unifica la teoria dell’informazione con la pratica della valutazione e selezione dei modelli predittivi.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: collegamento ELPD-KL in pratica">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: collegamento ELPD-KL in pratica
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Vogliamo confrontare due modelli predittivi per il numero di “teste” in <span class="math inline">\(n=10\)</span> lanci. Supponiamo che</p>
<ul>
<li>la <strong>distribuzione vera</strong> è <span class="math inline">\(p(y)=\text{Binom}(n=10,\;p=0.6)\)</span>,</li>
<li>il <strong>modello candidato</strong> prevede <span class="math inline">\(q(y)=\text{Binom}(n=10,\;q=0.5)\)</span>.</li>
</ul>
<p>L’<em>ELPD</em> di un modello è l’aspettativa, rispetto alla distribuzione vera <span class="math inline">\(p\)</span>, del <em>log-score</em> del modello: <span class="math inline">\(\mathrm{ELPD}(q)=\mathbb{E}_{p}[\log q(Y)]\)</span>. Nel caso discreto, l’aspettativa diventa una somma su tutti i possibili valori <span class="math inline">\(y=0,\dots,n\)</span>.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Parametri del problema</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">10</span>          <span class="co"># numero di lanci</span></span>
<span><span class="va">p</span> <span class="op">&lt;-</span> <span class="fl">0.6</span>         <span class="co"># probabilità vera di "testa"</span></span>
<span><span class="va">q</span> <span class="op">&lt;-</span> <span class="fl">0.5</span>         <span class="co"># probabilità ipotizzata dal modello candidato</span></span>
<span></span>
<span><span class="co"># 1) Supporto dei possibili esiti</span></span>
<span><span class="va">y_vals</span> <span class="op">&lt;-</span> <span class="fl">0</span><span class="op">:</span><span class="va">n</span></span>
<span></span>
<span><span class="co"># 2) Distribuzione vera p(y) su tutto il supporto</span></span>
<span><span class="va">p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 3) Log-predittiva del modello candidato q su tutto il supporto</span></span>
<span><span class="va">log_q_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">q</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 4) ELPD del modello candidato: somma dei log q(y) pesati da p(y)</span></span>
<span><span class="va">elpd_q</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_q_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 5) "Modello vero": usa q = p. Log-predittiva del modello vero</span></span>
<span><span class="va">log_p_y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Binomial.html">dbinom</a></span><span class="op">(</span><span class="va">y_vals</span>, size <span class="op">=</span> <span class="va">n</span>, prob <span class="op">=</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 6) ELPD del modello vero: somma dei log p(y) pesati da p(y)</span></span>
<span><span class="va">elpd_p</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="va">log_p_y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># 7) Divergenza KL tra p e q: somma p(y) * log [p(y)/q(y)]</span></span>
<span><span class="va">kl_pq</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">p_y</span> <span class="op">*</span> <span class="op">(</span><span class="va">log_p_y</span> <span class="op">-</span> <span class="va">log_q_y</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello candidato (q=0.5): %.4f\n"</span>, <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello candidato (q=0.5): -2.0549</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD modello vero      (q=0.6): %.4f\n"</span>, <span class="va">elpd_p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD modello vero      (q=0.6): -1.8536</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza ELPD (vero - candidato): %.4f\n"</span>, <span class="va">elpd_p</span> <span class="op">-</span> <span class="va">elpd_q</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza ELPD (vero - candidato): 0.2014</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"KL(p || q): %.4f\n"</span>, <span class="va">kl_pq</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; KL(p || q): 0.2014</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Cosa stiamo verificando?</strong></p>
<ol type="1">
<li><p><span class="math inline">\(\mathrm{ELPD}(q)=\sum_y p(y)\log q(y)\)</span> è <em>più basso</em> (più negativo) del valore ottenuto dal modello vero <span class="math inline">\(\mathrm{ELPD}(p)=\sum_y p(y)\log p(y)\)</span>. → Il modello con <span class="math inline">\(q=0.6\)</span> è <em>più predittivo</em> di quello con <span class="math inline">\(q=0.5\)</span>.</p></li>
<li><p>La differenza tra i due ELPD è <em>uguale</em> (vicina numericamente) alla divergenza di Kullback–Leibler:</p></li>
</ol>
<p><span class="math display">\[
\begin{align}
\mathrm{ELPD}(p)-\mathrm{ELPD}(q) &amp;= \sum_y p(y)\big[\log p(y)-\log q(y)\big] \notag\\
&amp;= D_{\mathrm{KL}}(p\|q)\;&gt;\;0.
\end{align}
\]</span> Questo mostra algebricamente e numericamente il legame: <em>massimizzare l’ELPD equivale a minimizzare la divergenza KL</em>.</p>
<p><strong>In pratica:</strong> In questo esempio abbiamo potuto calcolare l’ELPD <em>vero</em> perché conoscevamo l’intera distribuzione generatrice <span class="math inline">\(p(y)\)</span> e potevamo integrare esattamente. Nella realtà, <span class="math inline">\(p(y)\)</span> è sconosciuta: disponiamo solo di un campione osservato. In questi casi stimiamo l’ELPD <em>empiricamente</em>, ad esempio con la <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che sostituisce l’aspettativa rispetto a <span class="math inline">\(p\)</span> con una media sui dati raccolti, lasciando fuori una osservazione alla volta.</p>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon">
<div class="callout-body d-flex">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-body-container">
<p><strong>Collegamento chiave.</strong></p>
<p>L’ELPD è una stima empirica (con segno cambiato) della divergenza di Kullback–Leibler. Più alto è l’ELPD, migliore è la capacità predittiva del modello.</p>
</div>
</div>
</div>
</section></section></section><section id="stimare-lelpd-nella-pratica-la-leave-one-out-cross-validation" class="level2" data-number="44.7"><h2 data-number="44.7" class="anchored" data-anchor-id="stimare-lelpd-nella-pratica-la-leave-one-out-cross-validation">
<span class="header-section-number">44.7</span> Stimare l’ELPD nella pratica: la Leave-One-Out Cross-Validation</h2>
<p>Abbiamo chiarito che l’ELPD è la misura ideale della bontà predittiva di un modello, perché è direttamente collegata alla divergenza KL. Il problema è che, per definizione, richiede un’aspettativa rispetto alla vera distribuzione dei dati futuri <span class="math inline">\(p(\tilde{y})\)</span>, che non conosciamo mai.</p>
<p><em>Come possiamo allora stimarlo?</em> La soluzione più usata è la <em>Leave-One-Out Cross-Validation</em> (LOO-CV), che ci permette di avvicinarci all’ELPD teorico usando soltanto i dati osservati.</p>
<section id="lidea-alla-base-della-loo-cv" class="level3" data-number="44.7.1"><h3 data-number="44.7.1" class="anchored" data-anchor-id="lidea-alla-base-della-loo-cv">
<span class="header-section-number">44.7.1</span> L’idea alla base della LOO-CV</h3>
<p>Il principio è semplice: trattare ogni osservazione del dataset come se fosse “nuova” e verificare se il modello, addestrato sui dati rimanenti, riesce a prevederla.</p>
<p>Il procedimento è questo:</p>
<ol type="1">
<li>Si prende un’osservazione <span class="math inline">\(y_i\)</span>.</li>
<li>La si rimuove temporaneamente dal dataset.</li>
<li>Si stima il modello sui dati rimanenti <span class="math inline">\(y_{-i}\)</span>.</li>
<li>Si calcola la densità predittiva che il modello assegna al dato escluso: <span class="math inline">\(p(y\_i \mid y_{-i})\)</span>.</li>
<li>Si ripete per tutte le osservazioni e si sommano i logaritmi.</li>
</ol>
<p>In formula:</p>
<p><span id="eq-elpd-loo"><span class="math display">\[
\text{ELPD}_{\text{LOO}} = \sum_{i=1}^n \log p(y_i \mid y_{-i}) .
\tag{44.10}\]</span></span></p>
<p>Così otteniamo una stima <em>out-of-sample</em>: il modello viene valutato su dati che non ha mai visto.</p>
</section><section id="perché-funziona" class="level3" data-number="44.7.2"><h3 data-number="44.7.2" class="anchored" data-anchor-id="perché-funziona">
<span class="header-section-number">44.7.2</span> Perché funziona</h3>
<p>La LOO-CV funziona perché sostituisce l’aspettativa teorica rispetto a <span class="math inline">\(p(\tilde{y})\)</span> con una media empirica sulle osservazioni reali. Ogni <span class="math inline">\(y_i\)</span> viene trattata come un nuovo dato proveniente da <span class="math inline">\(p\)</span>, e la media dei log-score fuori campione fornisce una stima della capacità predittiva attesa:</p>
<p><span id="eq-elpd-loo-interpretation"><span class="math display">\[
\text{ELPD}_{\text{LOO}} \approx \mathbb{E}_p[\log q(\tilde{y}\mid y)] .
\tag{44.11}\]</span></span></p>
</section><section id="confrontare-i-modelli-con-loo-cv" class="level3" data-number="44.7.3"><h3 data-number="44.7.3" class="anchored" data-anchor-id="confrontare-i-modelli-con-loo-cv">
<span class="header-section-number">44.7.3</span> Confrontare i modelli con LOO-CV</h3>
<p>Una volta stimato l’ELPD-LOO, possiamo confrontare due modelli calcolando la differenza:</p>
<p><span id="eq-delta-elpd"><span class="math display">\[
\Delta \text{ELPD} = \text{ELPD}_{\text{LOO}}(M_1) - \text{ELPD}_{\text{LOO}}(M_2).
\tag{44.12}\]</span></span></p>
<p>Se la differenza è positiva, il modello <span class="math inline">\(M_1\)</span> ha una distribuzione predittiva più vicina alla realtà di quella di <span class="math inline">\(M_2\)</span>.</p>
<p>È utile stimare anche un errore standard della differenza. Come regola empirica, una differenza di almeno <em>due volte l’SE</em> indica un vantaggio credibile di un modello sull’altro.</p>
</section><section id="overfitting-e-vantaggio-della-loo-cv" class="level3" data-number="44.7.4"><h3 data-number="44.7.4" class="anchored" data-anchor-id="overfitting-e-vantaggio-della-loo-cv">
<span class="header-section-number">44.7.4</span> Overfitting e vantaggio della LOO-CV</h3>
<p>Se valutassimo un modello sugli stessi dati usati per addestrarlo, la sua performance apparirebbe gonfiata (<em>overfitting</em>). La LOO-CV aggira questo problema: ogni osservazione viene valutata solo con modelli che non l’hanno vista. Il punteggio ottenuto è quindi una misura più realistica della capacità di generalizzare a nuovi dati.</p>
</section><section id="psis-loo-la-scorciatoia-pratica" class="level3" data-number="44.7.5"><h3 data-number="44.7.5" class="anchored" data-anchor-id="psis-loo-la-scorciatoia-pratica">
<span class="header-section-number">44.7.5</span> PSIS-LOO: la scorciatoia pratica</h3>
<p>Un limite della LOO tradizionale è che richiederebbe di riadattare il modello <span class="math inline">\(n\)</span> volte, cosa spesso impraticabile. Per questo oggi si usa il metodo <em>Pareto-Smoothed Importance Sampling</em> (PSIS-LOO), che consente di stimare l’ELPD-LOO a partire da un unico adattamento del modello, sfruttando i campioni MCMC.</p>
<p>In R, tutto ciò è implementato nel pacchetto <code>loo</code>, già integrato in <code>brms</code> e <code>rstanarm</code>, attraverso funzioni come <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> e <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>. Oltre ai valori di ELPD, queste funzioni forniscono anche diagnostiche (le <em>Pareto k</em>) per capire se la stima è affidabile.</p>
<section id="in-sintesi" class="level4" data-number="44.7.5.1"><h4 data-number="44.7.5.1" class="anchored" data-anchor-id="in-sintesi">
<span class="header-section-number">44.7.5.1</span> In sintesi</h4>
<ul>
<li>L’ELPD misura la capacità predittiva del modello su dati futuri.</li>
<li>Non conoscendo la distribuzione vera, usiamo la LOO-CV per stimarlo.</li>
<li>La differenza di ELPD-LOO tra modelli approssima la differenza nelle loro divergenze KL.</li>
<li>PSIS-LOO rende il calcolo efficiente anche per modelli complessi.</li>
<li>La regola pratica: preferire il modello con <em>ELPD-LOO più alto</em>, tenendo conto anche della sua semplicità e interpretabilità.</li>
</ul>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Esempio: confronto ELPD-LOO tra due modelli">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esempio: confronto ELPD-LOO tra due modelli
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Questo esempio mostra come passare dalla definizione teorica dell’ELPD alla stima pratica via Leave-One-Out, usando un caso elementare Beta–Bernoulli.</p>
<p><strong>Dati:</strong> Cinque prove indipendenti: <span class="math inline">\(y=\{1,1,1,0,1\}\)</span> (quattro “successi”, un “insuccesso”).</p>
<p><strong>Modello A (Bayesiano adattato ai dati):</strong> Bernoulli<span class="math inline">\((\theta)\)</span> con prior <span class="math inline">\(\theta\sim \text{Beta}(1,1)\)</span> (uninformativa). Per LOO: * per ogni <span class="math inline">\(i\)</span>, escludiamo <span class="math inline">\(y_i\)</span>; * calcoliamo la posteriore <span class="math inline">\(\theta \mid y_{-i} \sim \text{Beta}(1+s_{-i},\,1+n_{-i}-s_{-i})\)</span>, dove <span class="math inline">\(s_{-i}\)</span> è il numero di successi tra i <span class="math inline">\(n-1\)</span> rimanenti; * calcoliamo la probabilità predittiva per <span class="math inline">\(y_i\)</span>.</p>
<p><strong>Modello B (di confronto):</strong> Moneta equa fissa (<span class="math inline">\(q=0.5\)</span>): la predittiva è sempre <span class="math inline">\(0.5\)</span>, indipendentemente dai dati.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Dati</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Log-predittiva LOO per Modello A (Beta(1,1) + Bernoulli)</span></span>
<span><span class="va">loo_log_pred_beta</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">i</span>, <span class="va">y</span>, <span class="va">a0</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">b0</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">yi</span> <span class="op">&lt;-</span> <span class="va">y</span><span class="op">[</span><span class="va">i</span><span class="op">]</span></span>
<span>  <span class="va">s_minus</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">-</span> <span class="va">yi</span></span>
<span>  <span class="va">n_minus</span> <span class="op">&lt;-</span> <span class="va">n</span> <span class="op">-</span> <span class="fl">1</span></span>
<span>  <span class="va">alpha</span> <span class="op">&lt;-</span> <span class="va">a0</span> <span class="op">+</span> <span class="va">s_minus</span></span>
<span>  <span class="va">beta</span>  <span class="op">&lt;-</span> <span class="va">b0</span> <span class="op">+</span> <span class="op">(</span><span class="va">n_minus</span> <span class="op">-</span> <span class="va">s_minus</span><span class="op">)</span></span>
<span>  <span class="va">p1</span> <span class="op">&lt;-</span> <span class="va">alpha</span> <span class="op">/</span> <span class="op">(</span><span class="va">alpha</span> <span class="op">+</span> <span class="va">beta</span><span class="op">)</span></span>
<span>  <span class="va">p</span>  <span class="op">&lt;-</span> <span class="kw">if</span> <span class="op">(</span><span class="va">yi</span> <span class="op">==</span> <span class="fl">1</span><span class="op">)</span> <span class="va">p1</span> <span class="kw">else</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p1</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Log-predittive punto-per-punto</span></span>
<span><span class="va">lp_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="va">loo_log_pred_beta</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span></span>
<span><span class="va">lp_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="fl">0.5</span><span class="op">)</span>, <span class="va">n</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># ELPD-LOO</span></span>
<span><span class="va">elpd_beta</span>  <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_beta</span><span class="op">)</span></span>
<span><span class="va">elpd_fixed</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">lp_fixed</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Differenza e SE</span></span>
<span><span class="va">diff_pt</span> <span class="op">&lt;-</span> <span class="va">lp_beta</span> <span class="op">-</span> <span class="va">lp_fixed</span></span>
<span><span class="va">se_diff</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="va">n</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html">var</a></span><span class="op">(</span><span class="va">diff_pt</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Tabella riassuntiva</span></span>
<span><span class="va">res</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  i <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span>, y <span class="op">=</span> <span class="va">y</span>,</span>
<span>  lp_beta <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_beta</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  lp_fixed <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">lp_fixed</span>, <span class="fl">6</span><span class="op">)</span>,</span>
<span>  diff <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">diff_pt</span>, <span class="fl">6</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="va">res</span><span class="op">)</span></span>
<span><span class="co">#&gt;   i y lp_beta lp_fixed   diff</span></span>
<span><span class="co">#&gt; 1 1 1  -0.405   -0.693  0.288</span></span>
<span><span class="co">#&gt; 2 2 1  -0.405   -0.693  0.288</span></span>
<span><span class="co">#&gt; 3 3 1  -0.405   -0.693  0.288</span></span>
<span><span class="co">#&gt; 4 4 0  -1.792   -0.693 -1.099</span></span>
<span><span class="co">#&gt; 5 5 1  -0.405   -0.693  0.288</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"\nELPD-LOO Modello A: %.6f\n"</span>, <span class="va">elpd_beta</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ELPD-LOO Modello A: -3.413620</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"ELPD-LOO Modello B: %.6f\n"</span>, <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; ELPD-LOO Modello B: -3.465736</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Differenza (A-B)  : %.6f\n"</span>, <span class="va">elpd_beta</span> <span class="op">-</span> <span class="va">elpd_fixed</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Differenza (A-B)  : 0.052116</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"SE differenza     : %.6f\n"</span>, <span class="va">se_diff</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; SE differenza     : 1.386294</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Interpretazione:</strong></p>
<ul>
<li>Ogni riga della tabella mostra la log-predittiva fuori campione per entrambi i modelli</li>
<li>In un campione con 4 successi su 5, il Modello A assegna più di 0.5 di probabilità ai successi, e meno di 0.5 all’unico insuccesso</li>
<li>L’ELPD-LOO di A può risultare leggermente più alto di quello di B, ma l’errore standard è grande perché <span class="math inline">\(n\)</span> è piccolo</li>
</ul>
<blockquote class="blockquote">
<p><strong>Regola pratica:</strong> una differenza <span class="math inline">\(|\Delta \text{ELPD}|\)</span> di almeno 2 volte l’SE fornisce un’indicazione più affidabile di superiorità del modello. In esempi così piccoli l’obiettivo è puramente didattico: capire come si calcola e cosa significa.</p>
</blockquote>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="In pratica: stimare e confrontare l'ELPD-LOO">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
In pratica: stimare e confrontare l’ELPD-LOO
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Concetto chiave</strong></p>
<ul>
<li>L’ELPD valuta la capacità predittiva su dati non visti.</li>
<li>La LOO-CV lo stima in modo efficiente con PSIS-LOO.</li>
</ul>
<p><strong>Strumenti</strong></p>
<ul>
<li>Funzione <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code> del pacchetto <em>loo</em>, integrata in <code>brms</code> e <code>rstanarm</code>.</li>
<li>Diagnostica con <em>Pareto k</em>, confronto con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</li>
</ul>
<p><strong>Workflow tipico in R</strong></p>
<ol type="1">
<li>Adattare ogni modello (<code><a href="https://paulbuerkner.com/brms/reference/brm.html">brm()</a></code> o <code>stan_glm()</code>).</li>
<li>Estrarre <code><a href="https://mc-stan.org/rstantools/reference/log_lik.html">log_lik()</a></code> e calcolare <code><a href="https://mc-stan.org/loo/reference/loo.html">loo()</a></code>.</li>
<li>Confrontare modelli con <code><a href="https://mc-stan.org/loo/reference/loo_compare.html">loo_compare()</a></code>.</li>
</ol>
<p><strong>Decisione</strong></p>
<ul>
<li>Preferire l’ELPD-LOO più alto.</li>
<li>Differenza ≥ 2×SE → indicazione di vantaggio sostanziale.</li>
<li>Valutare anche semplicità e interpretabilità.</li>
</ul>
</div>
</div>
</div>
</section></section></section><section id="criteri-di-informazione" class="level2" data-number="44.8"><h2 data-number="44.8" class="anchored" data-anchor-id="criteri-di-informazione">
<span class="header-section-number">44.8</span> Criteri di informazione</h2>
<p>Oltre alla convalida incrociata Leave-One-Out, la statistica offre altri strumenti per stimare la qualità predittiva di un modello senza conoscere la distribuzione vera dei dati. Molti di questi metodi affondano le loro radici teoriche nel concetto di divergenza di Kullback-Leibler, che misura la distanza tra la distribuzione generatrice dei dati e quella stimata dal nostro modello.</p>
<p>L’obiettivo comune è valutare la capacità di un modello di generalizzare, ovvero di fare buone previsioni su dati non osservati, senza farsi trarre in inganno dall’overfitting. Tutti i criteri seguono una logica simile, bilanciando due componenti: una misura della <em>bontà di adattamento</em> ai dati e una <em>penalità per la complessità</em> del modello stesso. I vari criteri si distinguono proprio per come definiscono queste due componenti e per le assunzioni su cui si basano.</p>
<section id="una-panoramica-dei-criteri-principali" class="level3" data-number="44.8.1"><h3 data-number="44.8.1" class="anchored" data-anchor-id="una-panoramica-dei-criteri-principali">
<span class="header-section-number">44.8.1</span> Una panoramica dei criteri principali</h3>
<p>L’<strong>AIC</strong> (Akaike Information Criterion) approssima la distanza di Kullback-Leibler utilizzando la verosimiglianza massimizzata e applica una penalità semplice, proporzionale al numero di parametri. È un criterio veloce e ampiamente utilizzato, particolarmente utile per modelli regolari con campioni non troppo piccoli e privi di struttura gerarchica.</p>
<p>Il <strong>BIC</strong> (Bayesian Information Criterion) segue una logica simile all’AIC, ma introduce una penalità per la complessità che cresce all’aumentare della dimensione del campione. Questo lo porta tendenzialmente a preferire modelli più parsimoniosi quando il numero di osservazioni è grande e, sotto specifiche ipotesi, può essere collegato alla verosimiglianza marginale.</p>
<p>Il <strong>WAIC</strong> (Widely Applicable Information Criterion) rappresenta una versione pienamente bayesiana. Utilizza l’intera distribuzione predittiva a posteriori per valutare il fit e stima una penalità per la complessità effettiva del modello, che può differire dal semplice numero di parametri. È particolarmente adatto per modelli complessi o non regolari ed è concettualmente molto vicino alla stima LOO.</p>
<p>Infine, il <strong>LOO-CV</strong> (Leave-One-Out Cross-Validation), specialmente nella sua efficiente implementazione PSIS-LOO, stima direttamente l’Expected Log Predictive Density (ELPD) escludendo un dato alla volta. È spesso considerato il gold standard per il confronto predittivo nell’ambito della modellazione bayesiana, grazie alla sua robustezza e alle utili diagnostiche che fornisce.</p>
<section id="come-orientarsi-nella-scelta" class="level4" data-number="44.8.1.1"><h4 data-number="44.8.1.1" class="anchored" data-anchor-id="come-orientarsi-nella-scelta">
<span class="header-section-number">44.8.1.1</span> Come orientarsi nella scelta</h4>
<p>Una regola pratica è che se l’obiettivo principale è la previsione fuori campione in un contesto bayesiano, il PSIS-LOO o il WAIC sono generalmente da preferire ad AIC e BIC. In un approccio frequentista classico, con modelli regolari e campioni di dimensioni medio-grandi, l’AIC rimane un buon compromesso, mentre il BIC può essere più appropriato quando si desidera enfatizzare la parsimonia.</p>
<p>Per modelli bayesiani con obiettivo predittivo e dati reali (spesso non iid o gerarchici), il PSIS-LOO è la prima scelta, con il WAIC utile come riscontro. Con campioni piccoli, strutture complesse o unità dipendenti, è bene evitare criteri puramente asintotici come AIC e BIC, preferendo invece LOO o definendo con attenzione l’unità di esclusione (ad esempio, per soggetto o per gruppo). Nei modelli gerarchici o multilivello, LOO e WAIC possono essere applicati in modo coerente, prestando attenzione a non escludere singole osservazioni se queste non sono indipendenti, ma piuttosto interi cluster.</p>
</section><section id="errori-comuni-e-best-practice" class="level4" data-number="44.8.1.2"><h4 data-number="44.8.1.2" class="anchored" data-anchor-id="errori-comuni-e-best-practice">
<span class="header-section-number">44.8.1.2</span> Errori comuni e best practice</h4>
<p>Un errore frequente è utilizzare il Mean Squared Error (MSE) sul campione di addestramento come metro di giudizio, poiché questo valore non penalizza la complessità e tende quindi a favorire modelli eccessivamente flessibili e soggetti a overfitting. Allo stesso modo, è importante ricordare che AIC e BIC si basano su stime puntuali (MLE o MAP) e non catturano l’incertezza completa sui parametri, il che li rende meno ideali in un contesto bayesiano puro. WAIC e LOOCV, al contrario, sono espressamente concepiti per stimare la performance predittiva su dati nuovi.</p>
<p>Quando si riporta un confronto tra modelli, è buona norma includere non solo il modello “vincente”, ma anche la differenza di ELPD con il suo errore standard, le diagnostiche sui parametri di Pareto-k, una stima della complessità effettiva e un commento sostantivo che spieghi il motivo della preferenza, che potrebbe risiedere nella parsimonia, nell’interpretabilità dei parametri o nella robustezza.</p>
</section><section id="in-sintesi-il-workflow-essenziale" class="level4" data-number="44.8.1.3"><h4 data-number="44.8.1.3" class="anchored" data-anchor-id="in-sintesi-il-workflow-essenziale">
<span class="header-section-number">44.8.1.3</span> In sintesi: il workflow essenziale</h4>
<p>Un mini-workflow consigliato per un approccio bayesiano prevede di: adattare i modelli; calcolare il LOO per ciascuno di essi e controllare i parametri di Pareto-k; se si riscontrano valori di k elevati, considerare una convalida incrociata K-fold o una LOO per cluster; confrontare i modelli con appositi strumenti e riportare le differenze di ELPD; opzionalmente, calcolare il WAIC come controllo incrociato; argomentare infine la scelta finale anche in base a parsimonia e interpretabilità.</p>
<p>La selezione del modello, in definitiva, ruota attorno a una domanda essenziale: quanto bene il modello predice dati che non ha mai visto? Il riferimento teorico è l’Expected Log Predictive Density (ELPD), che misura quanto la distribuzione predittiva del modello si avvicina alla vera distribuzione dei dati. Poiché quest’ultima è sconosciuta, l’ELPD va stimato con strumenti come LOO-CV e WAIC, che oggi rappresentano gli standard più affidabili per guidare una scelta consapevole, equilibrata e focalizzata sulla capacità di generalizzazione.</p>
</section></section></section><section id="riflessioni-conclusive" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="riflessioni-conclusive">Riflessioni conclusive</h2>
<p>Il principio fondamentale della modellazione bayesiana risiede nella valutazione della qualità di un modello attraverso la sua capacità di produrre previsioni probabilistiche robuste, rappresentate dalla distribuzione predittiva a posteriori <span class="math inline">\(p(\tilde{y} \mid y)\)</span>.</p>
<p>La misura che guida questa valutazione è l’<em>Expected Log Predictive Density</em> (ELPD), che quantifica la capacità predittiva del modello su dati non osservati. A differenza delle metriche in-sample, soggette a sovradattamento, l’ELPD fornisce una stima imparziale della capacità di generalizzazione. Teoricamente, massimizzare l’ELPD equivale a minimizzare la divergenza di Kullback-Leibler rispetto alla vera distribuzione generatrice dei dati.</p>
<p>Operativamente, l’ELPD viene stimato mediante PSIS-LOO, integrato con i diagnostici Pareto-k. Il WAIC costituisce un’alternativa bayesiana solida, spesso coerente con LOO. Al contrario, criteri come AIC e BIC, sebbene computazionalmente efficienti, si basano su stime puntuali e approssimazioni asintotiche, risultando meno affidabili in contesti di campioni piccoli o modelli gerarchici.</p>
<p>Nel confronto tra modelli, è essenziale riportare non solo l’ELPD-LOO, ma anche le differenze ΔELPD e i relativi errori standard. Tuttavia, la selezione del modello non dovrebbe ridursi a un esercizio meccanico: differenze trascurabili nell’ELPD, specialmente se associate ad alta incertezza, possono essere irrilevanti sul piano sostanziale. Modelli meno performanti ma più parsimoniosi o teoricamente fondati possono rappresentare scelte migliori.</p>
<p>L’obiettivo finale è bilanciare capacità predittiva e coerenza teorica, ricordando che lo scopo della modellazione non è solo prevedere, ma comprendere. La valutazione deve quindi integrare strumenti come il PSIS-LOO con considerazioni sull’incertezza statistica, la struttura dei dati e il contesto teorico di riferimento.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Informazioni sull'ambiente di sviluppo">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Informazioni sull’ambiente di sviluppo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Rome</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] gt_1.0.0              pillar_1.11.0         tinytable_0.13.0     </span></span>
<span><span class="co">#&gt;  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      </span></span>
<span><span class="co">#&gt;  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1</span></span>
<span><span class="co">#&gt; [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            </span></span>
<span><span class="co">#&gt; [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          </span></span>
<span><span class="co">#&gt; [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     </span></span>
<span><span class="co">#&gt; [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        </span></span>
<span><span class="co">#&gt; [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          </span></span>
<span><span class="co">#&gt; [25] rio_1.2.3             here_1.0.1           </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         </span></span>
<span><span class="co">#&gt;  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     </span></span>
<span><span class="co">#&gt;  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   </span></span>
<span><span class="co">#&gt; [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       </span></span>
<span><span class="co">#&gt; [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          </span></span>
<span><span class="co">#&gt; [16] knitr_1.50            labeling_0.4.3        bridgesampling_1.1-2 </span></span>
<span><span class="co">#&gt; [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       </span></span>
<span><span class="co">#&gt; [22] xml2_1.4.0            RColorBrewer_1.1-3    abind_1.4-8          </span></span>
<span><span class="co">#&gt; [25] multcomp_1.4-28       withr_3.0.2           purrr_1.1.0          </span></span>
<span><span class="co">#&gt; [28] grid_4.5.1            stats4_4.5.1          colorspace_2.1-1     </span></span>
<span><span class="co">#&gt; [31] xtable_1.8-4          inline_0.3.21         emmeans_1.11.2-8     </span></span>
<span><span class="co">#&gt; [34] scales_1.4.0          MASS_7.3-65           cli_3.6.5            </span></span>
<span><span class="co">#&gt; [37] mvtnorm_1.3-3         rmarkdown_2.29        ragg_1.5.0           </span></span>
<span><span class="co">#&gt; [40] generics_0.1.4        RcppParallel_5.1.11-1 cachem_1.1.0         </span></span>
<span><span class="co">#&gt; [43] stringr_1.5.1         splines_4.5.1         parallel_4.5.1       </span></span>
<span><span class="co">#&gt; [46] vctrs_0.6.5           V8_7.0.0              Matrix_1.7-4         </span></span>
<span><span class="co">#&gt; [49] sandwich_3.1-1        jsonlite_2.0.0        arrayhelpers_1.1-0   </span></span>
<span><span class="co">#&gt; [52] systemfonts_1.2.3     glue_1.8.0            codetools_0.2-20     </span></span>
<span><span class="co">#&gt; [55] distributional_0.5.0  lubridate_1.9.4       stringi_1.8.7        </span></span>
<span><span class="co">#&gt; [58] gtable_0.3.6          QuickJSR_1.8.0        htmltools_0.5.8.1    </span></span>
<span><span class="co">#&gt; [61] Brobdingnag_1.2-9     R6_2.6.1              textshaping_1.0.3    </span></span>
<span><span class="co">#&gt; [64] rprojroot_2.1.1       evaluate_1.0.5        lattice_0.22-7       </span></span>
<span><span class="co">#&gt; [67] backports_1.5.0       memoise_2.0.1         broom_1.0.9          </span></span>
<span><span class="co">#&gt; [70] snakecase_0.11.1      rstantools_2.5.0      coda_0.19-4.1        </span></span>
<span><span class="co">#&gt; [73] gridExtra_2.3         nlme_3.1-168          checkmate_2.3.3      </span></span>
<span><span class="co">#&gt; [76] xfun_0.53             zoo_1.8-14            pkgconfig_2.0.3</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section><section id="bibliografia" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-McElreath_rethinking" class="csl-entry" role="listitem">
McElreath, R. (2020). <em>Statistical rethinking: <span>A</span> <span>Bayesian</span> course with examples in <span>R</span> and <span>Stan</span></em> (2nd Edition). CRC Press.
</div>
</div>
</section></main><!-- /main --><script>
document.body.classList.add('classic-book');
document.addEventListener('DOMContentLoaded', function() {
  const paragraphs = document.querySelectorAll('p');
  paragraphs.forEach(p => {
    if (p.textContent.length > 200) {
      p.style.hyphens = 'auto';
      p.style.hyphenateCharacter = '-';
    }
  });
  const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
  headings.forEach(h => {
    h.style.fontFeatureSettings = '"liga" 1, "dlig" 1, "smcp" 1';
  });
});
</script><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/utet-companion\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/entropy/conclusions_sec.html" class="pagination-link" aria-label="Riflessioni conclusive della sezione">
        <span class="nav-page-text">Riflessioni conclusive della sezione</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Metodi bayesiani in psicologia — Companion site</strong>.<br>
Materiali di accompagnamento e approfondimento al manuale UTET.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/utet-companion/blob/main/chapters/entropy/03_model_comparison.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/utet-companion/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>