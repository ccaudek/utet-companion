<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="it" xml:lang="it"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.34">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>41&nbsp; Entropia e informazione di Shannon – Psicometria</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../chapters/entropy/02_kl.html" rel="next">
<link href="../../chapters/entropy/introduction_sec.html" rel="prev">
<link href="../../style/gauss.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-0a72236910a44089af39cd28873f322e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-9908c7b05874059c2106d454ac00f1d0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "Nessun risultato",
    "search-matching-documents-text": "documenti trovati",
    "search-copy-link-title": "Copiare il link nella ricerca",
    "search-hide-matches-text": "Nascondere i risultati aggiuntivi",
    "search-more-match-text": "ci sono altri risultati in questo documento",
    "search-more-matches-text": "ulteriori risultati in questo documento",
    "search-clear-button-title": "Pulire",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancellare",
    "search-submit-button-title": "Inviare",
    "search-label": "Ricerca"
  }
}</script><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-QT5S3P9D31"></script><script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-QT5S3P9D31', { 'anonymize_ip': true});
</script><style>html{ scroll-behavior: smooth; }</style>
<script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
</script><script>
window.MathJax = {
  tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
  svg: { scale: 1, mtextInheritFont: true, fontCache: 'none', minScale: 1 },
  options: { renderActions: { addMenu: [0, '', ''] } },
  loader: { load: ['input/tex','output/svg'] }
};
// Suggerimento CSS: vedi sezione 3 per gli spazi attorno a display math
</script><script>
window.MathJax = {
  tex: {
    packages: {'[+]': ['boldsymbol']},
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: { fontCache: 'global' }
};
</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="../../style/_typography-extras.css">
<link rel="stylesheet" href="../../style/_code-extras.css">
<link rel="stylesheet" href="../../style/_math-extras.css">
<link rel="stylesheet" href="../../style/styles.css">
</head>
<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/introduction_sec.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Attiva/disattiva la barra laterale" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Ricerca" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../../">Psicometria</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ccaudek/psicometria-r/" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Attiva/disattiva la modalità lettore">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Ricerca"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Informazioni generali</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../prefazione.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false">
 <span class="menu-text">Inferenza</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/01_bayes_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Inferenza bayesiana</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/02_uncertainty.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Abbracciare l’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/03_uncertainty_quantification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">La quantificazione dell’incertezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/04_statistical_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Modelli statistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/05_subj_prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Aggiornare le credenze su un parametro: dal prior alla posterior</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/06_conjugate_families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Distribuzioni coniugate</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/07_summary_posterior.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sintesi a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/08_balance_prior_post.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">L’influenza della distribuzione a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/09_prior_pred_check.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Controllo predittivo a priori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/10_post_pred_distr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Distribuzione predittiva a posteriori</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/bayesian_inference/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false">
 <span class="menu-text">MCMC</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/01_metropolis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">L’algoritmo di Metropolis-Hastings</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/02_ppl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Linguaggi di programmazione probabilistici</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/03_stan_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduzione pratica a Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/04_stan_diagnostics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Diagnostica delle catene markoviane</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/05_mcmc_prediction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Controlli predittivi bayesiani (a priori e a posteriori) con <code>cmdstanr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/06_stan_odds_ratio_stan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Analisi bayesiana dell’odds ratio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/07_stan_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Confrontare due medie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/08_stan_poisson_model_1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/09_stan_gaussian_mixture.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Modelli Mistura Gaussiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/10_stan_nuisance_parameters.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Modelli con più di un parametro</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/11_stan_hier_beta_binom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Modello gerarchico beta-binomiale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/12_stan_parametrization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Parametrizzazioni centered e non-centered</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/13_bayesian_workflow.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Flusso di lavoro bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/mcmc/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive sulla sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false">
 <span class="menu-text">Regressione</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/01_reglin_frequentist.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">La regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/02_regr_toward_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">La regressione verso la media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/03_reglin_bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Modello bayesiano di regressione lineare bivariata</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/04_synt_sugar_sea_ice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Zucchero sintattico</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/05_stan_regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Regressione lineare in Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/06_specification_error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Errore di specificazione e bias da variabile omessa</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/07_one_mean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Inferenza bayesiana su una media</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/08_two_means.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Confronto tra le medie di due gruppi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/09_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto: valutare la rilevanza pratica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/10_sample_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Pianificazione della dimensione campionaria</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/11_anova_1via.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">ANOVA ad una via</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/linear_models/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false">
 <span class="menu-text">GLM</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/01_logistic_regr.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">Regressione logistica con Stan</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/02_one_proportion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Inferenza sulle proporzioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/03_two_proportions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">Confronto tra due proporzioni con la regressione logistica</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/04_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">Modello di Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/05_logistic_process.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Dal GLM a un modello processuale per dati binari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/06_missing_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Dati mancanti in psicologia</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/glm/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive sulla sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Entropia</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/01_entropy.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/02_kl.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/03_model_comparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">43</span>&nbsp; <span class="chapter-title">Valutare i modelli bayesiani</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/entropy/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Modelli</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/introduction_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione alla sezione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/01_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">44</span>&nbsp; <span class="chapter-title">Il modello di revisione degli obiettivi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/02_dynamic_models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">45</span>&nbsp; <span class="chapter-title">Estensioni</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/03_rescorla_wagner.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">46</span>&nbsp; <span class="chapter-title">Il modello di Rescorla–Wagner</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/04_study_method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">47</span>&nbsp; <span class="chapter-title">Decisione ottimale e utilità attesa: l’approccio bayesiano</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/formal_models/conclusions_sec.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Riflessioni conclusive della sezione</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Crisi</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/introduction_replication_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduzione</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/01_crisis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">48</span>&nbsp; <span class="chapter-title">La crisi della replicazione</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/02_limits_stat_freq.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">49</span>&nbsp; <span class="chapter-title">Limiti dell’inferenza frequentista</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/03_effect_size.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">50</span>&nbsp; <span class="chapter-title">La grandezza dell’effetto</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/04_s_m_errors.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">51</span>&nbsp; <span class="chapter-title">Errori di segno e errori di grandezza</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/05_p_values.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">52</span>&nbsp; <span class="chapter-title">La fragilità del <em>p</em>-valore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/06_changes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">53</span>&nbsp; <span class="chapter-title">Riforma</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/07_degrees_of_freedom.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">54</span>&nbsp; <span class="chapter-title">I gradi di libertà del ricercatore</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/replication_crisis/08_integrity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">55</span>&nbsp; <span class="chapter-title">Integrità della ricerca</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false">
 <span class="menu-text">Epilogo</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/epiloque/epiloque.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Considerazioni Conclusive</span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" role="navigation" aria-expanded="false" aria-label="Attiva/disattiva sezione">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01_shell.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">La Shell</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a01a_files.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Cartelle e documenti</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a02_math_symbols.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Simbologia di base</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a03_latex.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Equazioni Matematiche in LaTeX</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a11_numbers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Numeri e intervalli</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a12_sum_notation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">F</span>&nbsp; <span class="chapter-title">Sommatorie</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a13_sets.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">G</span>&nbsp; <span class="chapter-title">Insiemi</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a14_combinatorics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">H</span>&nbsp; <span class="chapter-title">Calcolo combinatorio</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a15_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">I</span>&nbsp; <span class="chapter-title">Per liberarvi dai terrori preliminari</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a35_other_conjugate_families.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">J</span>&nbsp; <span class="chapter-title">Altre famiglie coniugate</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a36_gamma_poisson_model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">K</span>&nbsp; <span class="chapter-title">Modello coniugato Gamma-Poisson</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a40_metropolis_normal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">L</span>&nbsp; <span class="chapter-title">Metropolis: esempio Normale–Normale</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a41_cmdstanr_intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">M</span>&nbsp; <span class="chapter-title">Implementazione di modelli Bayesiani con Stan tramite <code>cmdstanr</code></span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a47_first_order_markov.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">N</span>&nbsp; <span class="chapter-title">Catene di Markov</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a50_lin_fun.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">O</span>&nbsp; <span class="chapter-title">La funzione lineare</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../chapters/appendix/a71_install_cmdstan.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">P</span>&nbsp; <span class="chapter-title">Come installare CmdStan</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Sommario</h2>
   
  <ul class="collapse">
<li><a href="#che-cos%C3%A8-linformazione" id="toc-che-cosè-linformazione" class="nav-link active" data-scroll-target="#che-cos%C3%A8-linformazione"><span class="header-section-number">41.1</span> Che cos’è l’informazione?</a></li>
  <li><a href="#la-sorpresa-e-linformazione-di-shannon" id="toc-la-sorpresa-e-linformazione-di-shannon" class="nav-link" data-scroll-target="#la-sorpresa-e-linformazione-di-shannon"><span class="header-section-number">41.2</span> La sorpresa e l’informazione di Shannon</a></li>
  <li><a href="#stimare-lentropia" id="toc-stimare-lentropia" class="nav-link" data-scroll-target="#stimare-lentropia"><span class="header-section-number">41.3</span> Stimare l’entropia</a></li>
  <li><a href="#la-codifica-di-huffman" id="toc-la-codifica-di-huffman" class="nav-link" data-scroll-target="#la-codifica-di-huffman"><span class="header-section-number">41.4</span> La codifica di Huffman</a></li>
  <li><a href="#applicazioni-psicologiche" id="toc-applicazioni-psicologiche" class="nav-link" data-scroll-target="#applicazioni-psicologiche"><span class="header-section-number">41.5</span> Applicazioni psicologiche</a></li>
  </ul><div class="toc-actions"><ul class="collapse"><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/01_entropy.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../chapters/entropy/introduction_sec.html">Entropia</a></li><li class="breadcrumb-item"><a href="../../chapters/entropy/01_entropy.html"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-entropy-shannon-information" class="quarto-section-identifier"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">Entropia e informazione di Shannon</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="epigraph">
<blockquote class="blockquote">
<p>“It is said that von Neumann recommended to Shannon that he use the term entropy, not only because of its similarity to the quantity used in physics, but also because ‘nobody knows what entropy really is, so in any discussion you will always have an advantage’.”</p>
<p>– <strong>C. M. Bishop</strong>, Pattern Recognition and Machine Learning (2006)</p>
</blockquote>
</div>
<section id="introduzione" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="introduzione">Introduzione</h2>
<p>Immagina di dover prevedere la risposta di uno studente a una domanda di un test a scelta multipla. Se non sai nulla dello studente, potresti pensare che ogni risposta sia ugualmente probabile: c’è quindi la massima incertezza. Se invece sai che quello studente è molto preparato e risponde quasi sempre correttamente, allora l’incertezza è bassa. Questa <em>quantificazione dell’incertezza</em> è esattamente ciò che chiamiamo <em>entropia</em>.</p>
<p>In termini qualitativi, l’entropia misura la quantità di “sorpresa” che ci aspettiamo:</p>
<ul>
<li>è <em>massima</em> quando tutti gli esiti sono equiprobabili (situazione di totale incertezza),<br>
</li>
<li>è <em>minima</em> quando uno degli esiti è praticamente certo.</li>
</ul>
<p><em>Un esempio psicologico:</em> nel lancio di una moneta equilibrata (<span class="math inline">\(p\)</span>=0.5), non possiamo sapere se uscirà testa o croce → entropia massima; nel comportamento di un paziente che mostra sempre la stessa risposta a un questionario → entropia minima.</p>
<section id="panoramica-del-capitolo" class="level3 unnumbered unlisted"><h3 class="unnumbered unlisted anchored" data-anchor-id="panoramica-del-capitolo">Panoramica del capitolo</h3>
<ul>
<li>Introdurre il concetto di informazione e la sua unità di misura (bit).</li>
<li>Definire l’entropia come media della sorpresa di Shannon.</li>
<li>Interpretare l’entropia in termini di incertezza e numero di alternative equiprobabili.<br>
</li>
<li>Stimare l’entropia da distribuzioni teoriche e da campioni osservati.<br>
</li>
<li>Collegare l’entropia alla codifica di Huffman e al limite teorico di compressione.</li>
</ul>
<div class="callout callout-style-simple callout-tip no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prerequisiti
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>Per i concetti di base sulla teoria dell’informazione, si rimanda ai primi due capitoli di <em>Information Theory: A Tutorial Introduction</em> <span class="citation" data-cites="stone2022information">(<a href="#ref-stone2022information" role="doc-biblioref">Stone, 2022</a>)</span>.</li>
</ul>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-caution no-icon callout-titled" title="Preparazione del Notebook">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Preparazione del Notebook
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">here</span><span class="fu">::</span><span class="fu"><a href="https://here.r-lib.org/reference/here.html">here</a></span><span class="op">(</span><span class="st">"code"</span>, <span class="st">"_common.R"</span><span class="op">)</span> <span class="op">|&gt;</span> </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/source.html">source</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://r.igraph.org/">igraph</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggraph.data-imaginist.com">ggraph</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://tidygraph.data-imaginist.com">tidygraph</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Funzione per calcolare la lunghezza media del codice di Huffman</span></span>
<span><span class="va">huffman_encoding</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="co"># Crea la "coda con priorità" iniziale come lista di liste</span></span>
<span>  <span class="va">heap</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">sym</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">sym</span>, <span class="st">""</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Funzione per ordinare la heap per probabilità (peso)</span></span>
<span>  <span class="va">sort_heap</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">heap</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">heap</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">heap</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># Costruzione dell'albero di Huffman</span></span>
<span>  <span class="kw">while</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">heap</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">heap</span> <span class="op">&lt;-</span> <span class="fu">sort_heap</span><span class="op">(</span><span class="va">heap</span><span class="op">)</span></span>
<span>    <span class="va">lo</span> <span class="op">&lt;-</span> <span class="va">heap</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">hi</span> <span class="op">&lt;-</span> <span class="va">heap</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">heap</span> <span class="op">&lt;-</span> <span class="va">heap</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">]</span></span>
<span></span>
<span>    <span class="co"># Aggiunge i prefissi "0" e "1" ai codici</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">lo</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">lo</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"0"</span>, <span class="va">lo</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq_along</a></span><span class="op">(</span><span class="va">hi</span><span class="op">)</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">hi</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"1"</span>, <span class="va">hi</span><span class="op">[[</span><span class="va">i</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span></span>
<span>    <span class="va">merged</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">lo</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span> <span class="op">+</span> <span class="va">hi</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, <span class="va">lo</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span>, <span class="va">hi</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span></span>
<span>    <span class="va">heap</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/append.html">append</a></span><span class="op">(</span><span class="va">heap</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">merged</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="co"># Estrai la lista finale dei simboli e codici</span></span>
<span>  <span class="va">final</span> <span class="op">&lt;-</span> <span class="va">heap</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="op">-</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">final</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">final</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Crea dizionario con codici</span></span>
<span>  <span class="va">huffman_dict</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="va">final</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span> <span class="va">x</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="co"># Calcolo della lunghezza media del codice</span></span>
<span>  <span class="va">avg_length</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mapply.html">mapply</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">sym</span>, <span class="va">code</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">probabilities</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/nchar.html">nchar</a></span><span class="op">(</span><span class="va">code</span><span class="op">)</span></span>
<span>  <span class="op">}</span>, <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">huffman_dict</span><span class="op">)</span>, <span class="va">huffman_dict</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>avg_length <span class="op">=</span> <span class="va">avg_length</span>, codes <span class="op">=</span> <span class="va">huffman_dict</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section></section><section id="che-cosè-linformazione" class="level2" data-number="41.1"><h2 data-number="41.1" class="anchored" data-anchor-id="che-cosè-linformazione">
<span class="header-section-number">41.1</span> Che cos’è l’informazione?</h2>
<p>Un bit è l’unità elementare di informazione: rappresenta la scelta tra due possibilità ugualmente probabili. Ogni volta che raddoppiamo il numero di alternative, serve un bit in più per identificarle. Il logaritmo in base 2 (<span class="math inline">\(\log_2\)</span>) indica esattamente quanti bit sono necessari per distinguere un certo numero di alternative.</p>
<section id="dalle-scelte-ai-bit-un-esempio-visivo" class="level3" data-number="41.1.1"><h3 data-number="41.1.1" class="anchored" data-anchor-id="dalle-scelte-ai-bit-un-esempio-visivo">
<span class="header-section-number">41.1.1</span> Dalle scelte ai bit: un esempio visivo</h3>
<p>Per capire come l’informazione possa essere misurata in bit, consideriamo il seguente esempio. Immaginiamo di trovarci a un incrocio e di dover scegliere una strada tra due possibilità. Ogni volta che ci troviamo di fronte a un incrocio, dobbiamo prendere una decisione: andare a destra o a sinistra. Ogni decisione può essere codificata con un bit: ad esempio, 0 per andare a sinistra e 1 per andare a destra.</p>
<p>Consideriamo il percorso con più incroci rappresentato nell’immagine seguente. Ogni percorso completo può essere codificato da una sequenza di bit, dove ogni bit corrisponde a una decisione (binaria) presa a un incrocio. Ad esempio, per raggiungere il punto D011, la sequenza di bit corretta è 011.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="01_entropy_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<section id="quanti-bit-sono-necessari-per-identificare-una-destinazione-specifica" class="level4" data-number="41.1.1.1"><h4 data-number="41.1.1.1" class="anchored" data-anchor-id="quanti-bit-sono-necessari-per-identificare-una-destinazione-specifica">
<span class="header-section-number">41.1.1.1</span> Quanti bit sono necessari per identificare una destinazione specifica?</h4>
<p>Ogni decisione aggiunge un bit alla sequenza che descrive il percorso. Se ci sono <span class="math inline">\(m\)</span> destinazioni possibili, servono</p>
<p><span class="math display">\[
n = \log_2 m
\]</span> bit per identificarne una in modo univoco. Nel nostro esempio, abbiamo otto destinazioni finali. Pertanto, sono necessari 3 bit (3 decisioni binarie) per identificarne una in modo univoco.</p>
</section><section id="cosa-rappresenta-un-bit-in-questo-contesto" class="level4" data-number="41.1.1.2"><h4 data-number="41.1.1.2" class="anchored" data-anchor-id="cosa-rappresenta-un-bit-in-questo-contesto">
<span class="header-section-number">41.1.1.2</span> Cosa rappresenta un bit in questo contesto?</h4>
<p>Un bit rappresenta un’unità elementare di informazione. In questo caso, ogni bit risponde alla domanda: “Devo andare a destra o a sinistra?”.</p>
</section><section id="perché-utilizziamo-i-logaritmi" class="level4" data-number="41.1.1.3"><h4 data-number="41.1.1.3" class="anchored" data-anchor-id="perché-utilizziamo-i-logaritmi">
<span class="header-section-number">41.1.1.3</span> Perché utilizziamo i logaritmi?</h4>
<p>Il logaritmo in base 2 ci permette di calcolare l’esponente a cui elevare 2 per ottenere un dato numero. In altre parole, ci indica quanti bit sono necessari per rappresentare un certo numero di destinazioni. Per l’esempio considerato, per arrivare a <span class="math inline">\(D011\)</span> partendo da <span class="math inline">\(A\)</span>, sono necessarie 3 domande la cui risposta è binaria (destra/sinistra).</p>
<p>Per riassumere:</p>
<ul>
<li>per raggiungere il punto D011 partendo da A, abbiamo bisogno di prendere tre decisioni binarie (sinistra o destra) in corrispondenza di tre incroci;</li>
<li>ogni decisione binaria può essere rappresentata da un bit (0 o 1). Quindi, per l’intero percorso, abbiamo bisogno di una sequenza di tre bit: 011;</li>
<li>per rispondere alla domanda “Come si va da A a D011?”, abbiamo dunque bisogno di 3 bit di informazione.</li>
</ul>
<p>In sintesi, esiste una relazione diretta tra il numero di bit di informazione e il numero di possibili destinazioni in un percorso decisionale binario. Ogni bit ci permette di scegliere tra due alternative, raddoppiando così il numero di possibili percorsi.</p>
</section></section></section><section id="la-sorpresa-e-linformazione-di-shannon" class="level2" data-number="41.2"><h2 data-number="41.2" class="anchored" data-anchor-id="la-sorpresa-e-linformazione-di-shannon">
<span class="header-section-number">41.2</span> La sorpresa e l’informazione di Shannon</h2>
<p>Introduciamo ora un elemento cruciale: la <em>probabilità dell’evento</em>. Quando due eventi hanno probabilità diverse, anche la quantità di informazione che trasmettono è diversa. Un evento molto probabile suscita poca sorpresa e, di conseguenza, veicola poca informazione. Al contrario, un evento raro produce una sorpresa maggiore e trasmette più informazione.</p>
<p>Shannon tradusse questa intuizione in una formula matematica, definendo l’informazione (o “sorpresa”) associata a un evento <span class="math inline">\(x\)</span> come</p>
<p><span id="eq-shannon-information-def"><span class="math display">\[
h(x) = \log_2 \frac{1}{p(x)} = -\log_2 p(x) \ \text{bit}.
\tag{41.1}\]</span></span> Questa espressione mostra chiaramente come l’informazione associata a un evento dipenda in modo inverso dalla sua probabilità: più l’evento è raro, maggiore sarà il valore di <span class="math inline">\(h(x)\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>Per rendere l’idea, immaginiamo tre eventi con probabilità rispettivamente pari a 0.5, 0.25 e 0.10. Applicando la formula di Shannon, otteniamo che la sorpresa corrisponde rispettivamente a 1.00 bit, 2.00 bit e 3.32 bit. Si vede così che, man mano che la probabilità diminuisce, la quantità di informazione – misurata in bit – cresce. In altre parole, un’osservazione inattesa “pesa” di più, perché modifica in misura maggiore le nostre conoscenze sul sistema in esame.</p>
<section id="entropia-come-media-dellinformazione-di-shannon" class="level3" data-number="41.2.1"><h3 data-number="41.2.1" class="anchored" data-anchor-id="entropia-come-media-dellinformazione-di-shannon">
<span class="header-section-number">41.2.1</span> Entropia come media dell’informazione di Shannon</h3>
<p>Finora abbiamo considerato la sorpresa associata a un singolo evento. In molti casi, però, non ci interessa un esito isolato, ma vogliamo descrivere l’incertezza complessiva di un sistema che può produrre esiti diversi. Per farlo, occorre calcolare la <em>sorpresa media</em> tenendo conto di tutti i possibili risultati e delle rispettive probabilità. È proprio questo il significato dell’<em>entropia</em>.</p>
<p>Dal punto di vista matematico, l’entropia è la sorpresa media attesa, calcolata come media pesata dell’informazione di Shannon di tutti i possibili esiti di una variabile casuale <span class="math inline">\(X\)</span>:</p>
<p><span id="eq-entropy-weighted-info-mean-def"><span class="math display">\[
H(X) \approx \frac{1}{n} \sum_{i=1}^{n} h(x_i).
\tag{41.2}\]</span></span></p>
<p>In questa espressione, <span class="math inline">\(h(x_i)\)</span> rappresenta la quantità di informazione trasmessa da un singolo esito <span class="math inline">\(x_i\)</span>, secondo la definizione di Shannon vista in precedenza. L’entropia non si riferisce dunque a un evento specifico, ma alla sorpresa media che ci aspettiamo di provare osservando ripetutamente la variabile.</p>
<p>Se la distribuzione delle probabilità è perfettamente equilibrata – ad esempio in una distribuzione uniforme, dove tutti i risultati sono ugualmente probabili – l’entropia è massima, poiché ogni osservazione fornisce una quantità simile e relativamente alta di informazione. Se invece la distribuzione è sbilanciata – per esempio nel caso di una moneta truccata che dà quasi sempre “testa” – l’entropia è più bassa, perché la prevedibilità aumenta e la quantità media di informazione fornita da ciascuna osservazione diminuisce.</p>
<p>Il grafico seguente illustra come la sorpresa di Shannon varia in funzione della probabilità di un evento: eventi rari producono un valore elevato di sorpresa, mentre eventi comuni producono un valore basso.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">p_vals</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0.001</span>, <span class="fl">1</span>, by <span class="op">=</span> <span class="fl">0.001</span><span class="op">)</span></span>
<span><span class="va">surprise</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="va">p_vals</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>p <span class="op">=</span> <span class="va">p_vals</span>, h <span class="op">=</span> <span class="va">surprise</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">p</span>, y <span class="op">=</span> <span class="va">h</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"Probabilità dell'evento p(x)"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Sorpresa h(x) [bit]"</span></span>
<span>  <span class="op">)</span> </span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="01_entropy_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
</section><section id="interpretazione-dellentropia" class="level3" data-number="41.2.2"><h3 data-number="41.2.2" class="anchored" data-anchor-id="interpretazione-dellentropia">
<span class="header-section-number">41.2.2</span> Interpretazione dell’entropia</h3>
<p>Diamo ora un significato concreto al valore numerico dell’entropia. Poiché essa rappresenta la media della sorpresa attesa osservando la realizzazione di una variabile casuale, tenendo conto di tutti i possibili esiti e delle loro probabilità, può essere interpretata come il <em>numero medio di bit necessari per descrivere un’osservazione</em> della variabile <span class="math inline">\(X\)</span>.</p>
<p>Quando l’entropia è espressa in bit, possiamo tradurla in un numero equivalente di alternative equiprobabili utilizzando la relazione</p>
<p><span id="eq-entroy-bits-surprise"><span class="math display">\[
m = 2^{H(X)} .
\tag{41.3}\]</span></span> Questo significa che un’entropia di <span class="math inline">\(H(X)\)</span> bit corrisponde alla stessa incertezza che avremmo se dovessimo distinguere tra <span class="math inline">\(m\)</span> esiti tutti ugualmente probabili. In questo senso, l’entropia misura la quantità di informazione contenuta in una variabile, esprimendola in termini del numero di scelte equiprobabili che la variabile potrebbe assumere.</p>
<div id="exercise-entropy-discrete-rv" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio — Interpretazione dell’entropia.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>1. Caso di riferimento: moneta equa.</strong></p>
<p>Se una variabile casuale può assumere due valori <em>ugualmente probabili</em>, come una moneta equa con <span class="math inline">\(p(\text{testa}) = p(\text{croce}) = 0.5\)</span>, la sua entropia è:</p>
<p><span class="math display">\[
H(X) = 0.5 \log_2\frac{1}{0.5} + 0.5 \log_2\frac{1}{0.5}
      = 0.5 \times 1 + 0.5 \times 1
      = 1 \ \text{bit}.
\]</span> Questo è il <em>valore massimo</em> di entropia per una variabile con due soli esiti: 1 bit è l’informazione necessaria per distinguere tra due alternative equiprobabili.</p>
<p><strong>2. Moneta sbilanciata: singolo lancio.</strong></p>
<p>Quando la moneta è sbilanciata, l’informazione media diminuisce. Supponiamo <span class="math inline">\(p(\text{testa}) = 0.9\)</span> e <span class="math inline">\(p(\text{croce}) = 0.1\)</span>.</p>
<p>La <em>sorpresa</em> associata a ciascun esito è:</p>
<p><span class="math display">\[
h(\text{testa}) = \log_2\frac{1}{0.9} \approx 0.15 \ \text{bit},
\]</span></p>
<p><span class="math display">\[
h(\text{croce}) = \log_2\frac{1}{0.1} \approx 3.32 \ \text{bit}.
\]</span></p>
<p>Pesando queste sorprese con le rispettive probabilità otteniamo l’entropia media:</p>
<p><span class="math display">\[
H(X) = 0.9 \times 0.15 + 0.1 \times 3.32 \approx 0.469 \ \text{bit}.
\]</span> Questa entropia è <em>inferiore a 1 bit</em>, nonostante l’esito raro (“croce”) sia molto più sorprendente di quello di una moneta equa. In generale, <em>nessuna moneta sbilanciata</em> può avere un’entropia media superiore a quella di una moneta equa.</p>
<p><strong>3. Più lanci: interpretazione pratica.</strong></p>
<p>Se lanciamo questa moneta 1000 volte, l’informazione totale prodotta sarà:</p>
<p><span class="math display">\[
1000 \times 0.469 \approx 469 \ \text{bit}.
\]</span> Quindi, rispetto alla moneta equa (1000 bit), otteniamo meno della metà dell’informazione.</p>
<p><strong>4. Numero equivalente di alternative equiprobabili.</strong></p>
<p>L’entropia può essere anche interpretata come il <em>numero equivalente di alternative tutte equiprobabili</em>:</p>
<p><span class="math display">\[
m = 2^{H(X)} = 2^{0.469} \approx 1.38.
\]</span> Questo <em>non significa</em> che esista un dado fisico con 1.38 facce: è solo un modo per dire che la quantità di incertezza media di questa moneta è la stessa di una variabile che può assumere circa 1.38 valori tutti con la stessa probabilità.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Funzione per calcolare l'entropia di una moneta</span></span>
<span><span class="va">entropy_coin</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">p</span> <span class="op">==</span> <span class="fl">0</span> <span class="op">|</span> <span class="va">p</span> <span class="op">==</span> <span class="fl">1</span>, <span class="fl">0</span>,</span>
<span>         <span class="op">-</span><span class="va">p</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Sequenza di probabilità</span></span>
<span><span class="va">p_values</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, by <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span><span class="va">H_values</span> <span class="op">&lt;-</span> <span class="fu">entropy_coin</span><span class="op">(</span><span class="va">p_values</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Dati per i punti di esempio</span></span>
<span><span class="va">points_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">0.9</span><span class="op">)</span>,</span>
<span>  H <span class="op">=</span> <span class="fu">entropy_coin</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">0.9</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  label <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Moneta equa\nH=1 bit"</span>, <span class="st">"Moneta sbilanciata\nH=0.469 bit"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Grafico</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>p <span class="op">=</span> <span class="va">p_values</span>, H <span class="op">=</span> <span class="va">H_values</span><span class="op">)</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">p</span>, y <span class="op">=</span> <span class="va">H</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">points_df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">p</span>, y <span class="op">=</span> <span class="va">H</span><span class="op">)</span>, color <span class="op">=</span> <span class="st">"brown"</span>, size <span class="op">=</span> <span class="fl">3</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_text.html">geom_text</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">points_df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>label <span class="op">=</span> <span class="va">label</span><span class="op">)</span>, vjust <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, hjust <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Probabilità di testa, "</span>, <span class="va">p</span><span class="op">)</span><span class="op">)</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Entropia H(X) [bit]"</span></span>
<span>  <span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="01_entropy_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="caratteristiche-dellentropia" class="level3" data-number="41.2.3"><h3 data-number="41.2.3" class="anchored" data-anchor-id="caratteristiche-dellentropia">
<span class="header-section-number">41.2.3</span> Caratteristiche dell’entropia</h3>
<p>L’entropia raggiunge il suo valore massimo quando tutti gli esiti possibili hanno la stessa probabilità di verificarsi. In questa condizione, l’incertezza è totale: non esiste alcun indizio che permetta di prevedere il risultato meglio del puro caso, e il grado di imprevedibilità è al massimo.</p>
<p>All’opposto, l’entropia è minima quando l’esito è completamente certo, cioè quando un evento ha probabilità pari a 1 e tutti gli altri hanno probabilità pari a 0. In tali circostanze non vi è alcuna incertezza, nessuna sorpresa e quindi nessuna informazione aggiuntiva ottenibile dall’osservazione.</p>
<p>Un’ulteriore caratteristica fondamentale è l’<em>additività</em> per eventi indipendenti: quando due o più eventi sono indipendenti, l’entropia complessiva della loro combinazione è pari alla somma delle entropie dei singoli eventi. Questa proprietà deriva direttamente dall’additività dei logaritmi nella formula di Shannon e riflette il fatto che, nel caso di eventi indipendenti, l’incertezza complessiva si ottiene sommando le incertezze prodotte da ciascun evento considerato separatamente.</p>
</section></section><section id="stimare-lentropia" class="level2" data-number="41.3"><h2 data-number="41.3" class="anchored" data-anchor-id="stimare-lentropia">
<span class="header-section-number">41.3</span> Stimare l’entropia</h2>
<p>Nelle sezioni precedenti abbiamo visto che l’entropia esprime la sorpresa media attesa quando osserviamo una variabile casuale, ed è strettamente legata all’informazione di Shannon dei singoli eventi. Passiamo ora dal concetto alla sua applicazione pratica, illustrando come calcolare l’entropia sia a partire da una distribuzione di probabilità teorica, sia da un insieme di dati osservati.</p>
<section id="lentropia-di-una-distribuzione-di-probabilità" class="level3" data-number="41.3.1"><h3 data-number="41.3.1" class="anchored" data-anchor-id="lentropia-di-una-distribuzione-di-probabilità">
<span class="header-section-number">41.3.1</span> L’entropia di una distribuzione di probabilità</h3>
<p>Immaginiamo una variabile casuale discreta <span class="math inline">\(X\)</span>, che può assumere un insieme di valori distinti <span class="math inline">\(x_1, x_2, \dots, x_n\)</span>, ciascuno con probabilità <span class="math inline">\(p(x) = \Pr\{X = x\}\)</span>. Quando osserviamo un particolare valore di <span class="math inline">\(X\)</span>, riceviamo una certa quantità di informazione, che possiamo interpretare come il grado di sorpresa associato a quell’esito. Un evento molto improbabile produce un’alta sorpresa, mentre un evento quasi certo trasmette poca o nessuna informazione.</p>
<p>Per tradurre questa intuizione in termini matematici, definiamo la <em>sorpresa</em> di un esito <span class="math inline">\(x\)</span> come</p>
<p><span class="math display">\[
h(x) = -\log_2 p(x).
\]</span> Questa funzione ha le proprietà desiderate: è tanto più grande quanto minore è la probabilità di <span class="math inline">\(x\)</span>, e vale zero se l’evento è certo (<span class="math inline">\(p(x) = 1\)</span>).</p>
<p>Poiché siamo interessati non a un singolo esito ma all’incertezza complessiva della distribuzione, calcoliamo la media della sorpresa rispetto alle probabilità dei diversi esiti. Otteniamo così la definizione di <em>entropia di Shannon</em>:</p>
<p><span id="eq-entropy-prob-distr-def"><span class="math display">\[
H(X) = -\sum_{x \in X} p(x) \log_2 p(x).
\tag{41.4}\]</span></span> Ogni termine <span class="math inline">\(-p(x)\log_2 p(x)\)</span> rappresenta il contributo informativo medio di un esito, ponderato in base alla sua probabilità.</p>
<p>Alcune proprietà fondamentali:</p>
<ul>
<li>L’entropia è massima quando la distribuzione è uniforme, cioè quando tutti gli esiti sono equiprobabili: in questo caso, l’incertezza è al suo livello più alto.<br>
</li>
<li>L’entropia si riduce man mano che la distribuzione diventa più sbilanciata: se alcuni esiti hanno probabilità molto elevate, il grado di sorpresa complessiva diminuisce.<br>
</li>
<li>Se un esito è certo, l’entropia si annulla: non c’è incertezza e nessuna nuova informazione viene trasmessa dall’osservazione.</li>
</ul>
<p>In breve, l’entropia <span class="math inline">\(H(X)\)</span> misura l’incertezza media di una variabile casuale e può essere interpretata come il <em>numero medio di bit necessari per descrivere un’osservazione di <span class="math inline">\(X\)</span></em>.</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Ripasso matematico
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>La somma indica che calcoliamo il contributo di ciascun esito possibile.<br>
</li>
<li>Il logaritmo (in base 2) ci dice quanta “informazione” porta ogni esito.<br>
</li>
<li>Il segno meno serve perché i logaritmi di numeri tra 0 e 1 sono negativi.<br>
</li>
</ul>
</div>
</div>
<div id="exercise-entropyunbalanced-coin" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio — Entropia di un dado con otto facce.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere un dado con otto facce. Ci sono <span class="math inline">\(m = 8\)</span> esiti possibili:</p>
<p><span class="math display">\[
A_x = \{1,2,3,4,5,6,7,8\}.
\]</span></p>
<p>Poiché il dado è equo, tutti gli otto esiti hanno la stessa probabilità di <span class="math inline">\(p(x) = 1/8\)</span>, definendo così una distribuzione di probabilità uniforme:</p>
<p><span class="math display">\[
p(X) = \left\{\frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}, \frac{1}{8}\right\}.
\]</span></p>
<p>L’entropia di questa distribuzione può essere calcolata come:</p>
<p><span class="math display">\[
H(X) = - \sum_{i=1}^{8} \frac{1}{8} \log_2 \frac{1}{8} = \log_2 8 = 3 \text{ bit}.
\]</span> Poiché l’informazione associata a ciascun esito è esattamente 3 bit, anche l’entropia media è di 3 bit, che rappresenta l’incertezza complessiva della variabile <span class="math inline">\(X\)</span>.</p>
<p>Dato che <span class="math inline">\(X\)</span> ha un’entropia di <span class="math inline">\(H(X) = 3\)</span> bit, possiamo dire che <span class="math inline">\(X\)</span> può rappresentare fino a:</p>
<p><span class="math display">\[
m = 2^{H(X)} = 2^3 = 8
\]</span> esiti equiprobabili.</p>
</div>
</div>
</div>
<div id="exercise-entropy-discrete-rv" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio — Entropia di un variabile casuale discreta.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Sia <span class="math inline">\(X\)</span> una variabile casuale discreta che può assumere i valori <span class="math inline">\(a, b, c,\)</span> e <span class="math inline">\(d\)</span> con una distribuzione di probabilità di massa <span class="math inline">\(p(a) = \frac{1}{2}\)</span>, <span class="math inline">\(p(b) = \frac{1}{4}\)</span>, <span class="math inline">\(p(c) = \frac{1}{8}\)</span>, e <span class="math inline">\(p(d) = \frac{1}{8}\)</span>, rispettivamente. L’entropia di <span class="math inline">\(X\)</span>, che misura l’incertezza associata alla distribuzione di probabilità, è calcolata come:</p>
<p><span class="math display">\[
H(X) = -\left(\frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{4} \log_2 \frac{1}{4} + \frac{1}{8} \log_2 \frac{1}{8} + \frac{1}{8} \log_2 \frac{1}{8}\right).
\]</span> Calcolando i singoli termini, otteniamo:</p>
<p><span class="math display">\[
H(X) = -\left(\frac{1}{2} \cdot (-1) + \frac{1}{4} \cdot (-2) + \frac{1}{8} \cdot (-3) + \frac{1}{8} \cdot (-3)\right) = \frac{7}{4} \text{ bits}.
\]</span> È importante notare che l’entropia <span class="math inline">\(H(X)\)</span> dipende esclusivamente dalla distribuzione di probabilità dei valori di <span class="math inline">\(X\)</span> e non dai valori stessi.</p>
</div>
</div>
</div>
</section><section id="lentropia-in-un-campione-di-osservazioni" class="level3" data-number="41.3.2"><h3 data-number="41.3.2" class="anchored" data-anchor-id="lentropia-in-un-campione-di-osservazioni">
<span class="header-section-number">41.3.2</span> L’entropia in un campione di osservazioni</h3>
<p>Finora abbiamo considerato il caso in cui la distribuzione di probabilità sia nota a priori. Nella pratica della ricerca psicologica, tuttavia, disponiamo spesso soltanto di un campione di osservazioni. In questo caso possiamo stimare l’entropia calcolando le frequenze relative di ciascun valore osservato e utilizzandole come stima empirica delle probabilità.</p>
<p>Il risultato misura quanto la distribuzione dei valori nel campione sia incerta o imprevedibile. Un campione in cui le frequenze siano simili per tutti i valori possibili mostrerà un’entropia stimata elevata; al contrario, se nel campione un valore domina nettamente sugli altri, l’entropia stimata sarà bassa, indicando una distribuzione più prevedibile.</p>
<div id="exercise-entropy-sample-obs" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio — Entropia di un campione di osservazioni.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Per comprendere meglio questo concetto, possiamo calcolare l’entropia associata a insiemi di osservazioni. Consideriamo i due vettori seguenti:</p>
<p><span class="math display">\[
\begin{align}
x &amp;= \{1, 2, 3, 3, 3, 3, 2, 1, 3, 3, 2, 1, 1, 4, 4, 3, 1, 2\}, \notag\\
y &amp;= \{3, 4, 1, 1, 1, 1, 4, 3, 1, 1, 4, 3, 3, 2, 2, 1, 3, 4\}. \notag
\end{align}
\]</span></p>
<p>Troviamo l’entropia associata a ciascuno di essi.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Vettori x e y</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">4</span>, <span class="fl">3</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">3</span>, <span class="fl">4</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Conta le frequenze</span></span>
<span><span class="va">x_counts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">y_counts</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calcola le probabilità relative</span></span>
<span><span class="va">x_probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">x_counts</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span><span class="va">y_probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="va">y_counts</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Funzione per calcolare l'entropia (log in base 2)</span></span>
<span><span class="va">calculate_entropy</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">probabilities</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># Calcolo dell'entropia</span></span>
<span><span class="va">x_entropy</span> <span class="op">&lt;-</span> <span class="fu">calculate_entropy</span><span class="op">(</span><span class="va">x_probabilities</span><span class="op">)</span></span>
<span><span class="va">y_entropy</span> <span class="op">&lt;-</span> <span class="fu">calculate_entropy</span><span class="op">(</span><span class="va">y_probabilities</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Stampa i risultati</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Entropia di x: %.4f bit\n"</span>, <span class="va">x_entropy</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Entropia di x: 1.8776 bit</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Entropia di y: %.4f bit\n"</span>, <span class="va">y_entropy</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Entropia di y: 1.8776 bit</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Entrambi i vettori hanno la stessa entropia di 1.8776 bit.</p>
</div>
</div>
</div>
<section id="interpretazione-finale" class="level4" data-number="41.3.2.1"><h4 data-number="41.3.2.1" class="anchored" data-anchor-id="interpretazione-finale">
<span class="header-section-number">41.3.2.1</span> Interpretazione finale</h4>
<p>L’entropia <span class="math inline">\(H(X)\)</span> misura dunque l’incertezza media associata a una distribuzione di probabilità. Possiamo leggerla anche come il <em>numero medio di bit necessari per descrivere un’osservazione di <span class="math inline">\(X\)</span></em>.</p>
<p>In altre parole, l’entropia ci dice quanta informazione, in media, otteniamo osservando il risultato di una variabile casuale: più alta è l’entropia, maggiore è l’imprevedibilità del fenomeno.</p>
</section></section><section id="lentropia-di-una-variabile-casuale-continua" class="level3" data-number="41.3.3"><h3 data-number="41.3.3" class="anchored" data-anchor-id="lentropia-di-una-variabile-casuale-continua">
<span class="header-section-number">41.3.3</span> L’entropia di una variabile casuale continua</h3>
<p>Anche per le variabili casuali continue possiamo definire l’entropia, estendendo il caso discreto: la somma sui possibili esiti viene semplicemente sostituita da un integrale. Questa generalizzazione è necessaria perché una variabile continua può assumere un numero infinito di valori. In questo caso, la probabilità che <span class="math inline">\(X\)</span> assuma un valore esatto è sempre zero: ciò che conta non è la probabilità puntuale, ma la densità di probabilità nei diversi punti del dominio.</p>
<p>Per una variabile casuale continua <span class="math inline">\(X\)</span>, con funzione di densità di probabilità <span class="math inline">\(p(x)\)</span>, l’entropia, detta in questo caso <em>entropia differenziale</em>, è definita come</p>
<p><span id="eq-entropy-density-distr-def"><span class="math display">\[
H(X) = -\int p(x) \log_2 p(x) \, dx ,
\tag{41.5}\]</span></span> dove <span class="math inline">\(p(x)\)</span> rappresenta la densità di probabilità di <span class="math inline">\(X\)</span> e l’integrale è calcolato su tutto il dominio della variabile.</p>
<p>Come nel caso discreto, l’entropia differenziale fornisce una misura dell’incertezza media associata alla distribuzione di probabilità. Se la densità è molto concentrata attorno a pochi valori (ad esempio un picco stretto), l’entropia è bassa: sappiamo già “dove aspettarci” la variabile, quindi l’incertezza è ridotta. Al contrario, una densità più “sparsa” e distribuita uniformemente implica un’entropia più alta, segnalando maggiore imprevedibilità.</p>
<p>Il segno negativo nella formula deriva dal fatto che, per probabilità comprese tra 0 e 1, il logaritmo è negativo: in questo modo l’entropia assume valori positivi e può essere interpretata, in analogia al caso discreto, come il numero medio di bit necessari per codificare un’osservazione della variabile continua <span class="math inline">\(X\)</span>.</p>
<div id="exercise-entropy-sample-obs" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio — Un confronto numerico: normali più “strette” e più “larghe”.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Per la distribuzione normale <span class="math inline">\(X \sim \mathcal N(\mu,\sigma^2)\)</span> l’entropia differenziale ha una forma chiusa:</p>
<p><span class="math display">\[
H(X)=\tfrac12 \log_2\!\big(2\pi e\,\sigma^2\big)\ \text{bit}.
\]</span> La dipendenza è tutta nella scala <span class="math inline">\(\sigma\)</span>: raddoppiare <span class="math inline">\(\sigma\)</span> aggiunge esattamente 1 bit di entropia, perché la massa di probabilità si “spalma” su un intervallo più ampio. Numericamente, con <span class="math inline">\(\sigma=0{,}5\)</span>, <span class="math inline">\(H(X)\approx 1{,}047\)</span> bit; con <span class="math inline">\(\sigma=1\)</span>, <span class="math inline">\(H(X)\approx 2{,}047\)</span> bit; con <span class="math inline">\(\sigma=2\)</span>, <span class="math inline">\(H(X)\approx 3{,}047\)</span> bit. L’aumento regolare di un bit per ogni raddoppio di <span class="math inline">\(\sigma\)</span> rende molto trasparente l’idea che una densità più concentrata (piccola <span class="math inline">\(\sigma\)</span>) produce minore incertezza, mentre una densità più diffusa (grande <span class="math inline">\(\sigma\)</span>) produce maggiore incertezza.</p>
<p>Ecco un frammento R che replica il calcolo e mostra le tre densità normalizzate sulla stessa scala, così che la relazione tra forma della densità e entropia sia visibile a colpo d’occhio.</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Entropia differenziale (in bit) per N(mu, sigma^2)</span></span>
<span><span class="va">h_norm_bits</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="fl">2</span> <span class="op">*</span> <span class="va">pi</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">exp</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span> <span class="op">*</span> <span class="va">sigma</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span></span>
<span><span class="va">sigmas</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0.5</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">entropie</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">sigmas</span>, <span class="va">h_norm_bits</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">entropie</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 1.05 2.05 3.05</span></span>
<span><span class="co"># atteso: 1.047, 2.047, 3.047</span></span>
<span></span>
<span><span class="co"># Visualizzazione delle densità</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">6</span>, <span class="fl">6</span>, length.out <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>, times <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">sigmas</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  sigma <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="va">sigmas</span>, each <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">df</span><span class="op">$</span><span class="va">dens</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/mapply.html">mapply</a></span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">s</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">x</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="va">s</span><span class="op">)</span>, <span class="va">df</span><span class="op">$</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/numeric.html">as.numeric</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/character.html">as.character</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">sigma</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">dens</span>, group <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>linetype <span class="op">=</span> <span class="va">sigma</span><span class="op">)</span>, linewidth <span class="op">=</span> <span class="fl">1</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    subtitle <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="st">"H(σ=0.5)≈"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">entropie</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>,<span class="fl">3</span><span class="op">)</span>, <span class="st">" bit; "</span>,</span>
<span>                      <span class="st">"H(σ=1)≈"</span>,   <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">entropie</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,<span class="fl">3</span><span class="op">)</span>, <span class="st">" bit; "</span>,</span>
<span>                      <span class="st">"H(σ=2)≈"</span>,   <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">entropie</span><span class="op">[</span><span class="fl">3</span><span class="op">]</span>,<span class="fl">3</span><span class="op">)</span>, <span class="st">" bit"</span><span class="op">)</span>,</span>
<span>    x <span class="op">=</span> <span class="st">"x"</span>, y <span class="op">=</span> <span class="st">"densità"</span></span>
<span>  <span class="op">)</span> </span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="01_entropy_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<p>Nell’analisi di dati psicologici, la stessa variabile misurata con una scala più “compressa” (varianza più piccola, punteggi concentrati) porta a una minore entropia differenziale rispetto alla stessa variabile osservata con maggiore dispersione. Questo legame diretto tra dispersione e entropia chiarisce perché, in presenza di eterogeneità individuale o situazionale, la “quantità di incertezza” da descrivere aumenti con la variabilità del fenomeno.</p>
</div>
</div>
</div>
</section></section><section id="la-codifica-di-huffman" class="level2" data-number="41.4"><h2 data-number="41.4" class="anchored" data-anchor-id="la-codifica-di-huffman">
<span class="header-section-number">41.4</span> La codifica di Huffman</h2>
<p>Abbiamo visto che l’entropia <span class="math inline">\(H(X)\)</span> di una variabile casuale <span class="math inline">\(X\)</span> misura la sorpresa media di un esito. Un risultato fondamentale è che l’entropia rappresenta anche il limite teorico inferiore alla <em>lunghezza media</em>, in bit, di un codice binario che descrive gli esiti di <span class="math inline">\(X\)</span>. In altre parole: è impossibile creare un sistema di codifica (una “scorciatoia” per rappresentare l’informazione) che, in media, usi meno bit di <span class="math inline">\(H(X)\)</span> per simbolo, senza perdere informazioni.</p>
<p>L’algoritmo di <em>Huffman</em>, sviluppato da David A. Huffman nel 1952, fornisce un metodo pratico per costruire un codice che si avvicina moltissimo a questo limite teorico.</p>
<section id="lidea-di-base" class="level3" data-number="41.4.1"><h3 data-number="41.4.1" class="anchored" data-anchor-id="lidea-di-base">
<span class="header-section-number">41.4.1</span> L’idea di base</h3>
<p>L’idea centrale è semplice e intuitiva, e riflette una strategia di ottimizzazione che anche la nostra mente potrebbe usare: <em>assegna “etichette” mentali corte agli eventi comuni e etichette più lunghe agli eventi rari.</em></p>
<p>Pensate a come abbreviate le parole che usate più spesso in un messaggio di testo (“tvb”, “xké”, “nn”) mentre scrivete per intero quelle più rare. State applicando un principio simile a quello di Huffman per risparmiare tempo (bit cognitivi)!</p>
</section><section id="come-funziona-lalgoritmo-passo-dopo-passo" class="level3" data-number="41.4.2"><h3 data-number="41.4.2" class="anchored" data-anchor-id="come-funziona-lalgoritmo-passo-dopo-passo">
<span class="header-section-number">41.4.2</span> Come funziona l’algoritmo, passo dopo passo</h3>
<p>L’obiettivo è costruire un <em>albero binario</em> le cui foglie sono i simboli da codificare. La procedura è la seguente:</p>
<ol type="1">
<li>
<strong>Lista di partenza:</strong> Si parte da un elenco di tutti i simboli con le loro probabilità (o frequenze). Ogni simbolo è un piccolo “nodo”.</li>
<li>
<strong>Unire i più rari:</strong> Si identificano i <em>due nodi con la probabilità più bassa</em> e si uniscono per creare un nuovo nodo. A questo nuovo nodo si associa una probabilità pari alla <em>somma delle probabilità</em> dei due nodi figli.</li>
<li>
<strong>Ripetere:</strong> Si ripete il passo 2, unendo sempre i due nodi con la probabilità più bassa (considerando anche i nuovi nodi creati), finché non rimane un unico nodo finale, chiamato <em>radice</em>. Questo è l’albero completo.</li>
<li>
<strong>Assegnare i codici:</strong> Si percorre l’albero dalla radice fino a ciascun simbolo (foglia). Ad ogni ramo sinistro si assegna il valore <code>0</code> e ad ogni ramo destro il valore <code>1</code>. La sequenza di <code>0</code> e <code>1</code> incontrata nel percorso dalla radice alla foglia è il <em>codice di Huffman</em> per quel simbolo.</li>
</ol>
<p>La caratteristica geniale di questo codice è che è un <em>codice prefisso</em>: nessun codice è l’inizio (il “prefisso”) di un altro. Questo elimina ogni ambiguità durante la decodifica, permettendo di leggere il messaggio senza bisogno di simboli separatori.</p>
</section><section id="esempio-concreto-codificare-un-messaggio" class="level3" data-number="41.4.3"><h3 data-number="41.4.3" class="anchored" data-anchor-id="esempio-concreto-codificare-un-messaggio">
<span class="header-section-number">41.4.3</span> Esempio concreto: codificare un messaggio</h3>
<p>Immaginiamo di dover codificare un messaggio composto da 43 caratteri, usando solo quattro lettere con queste frequenze:</p>
<table class="caption-top table">
<thead><tr class="header">
<th style="text-align: left;">Simbolo</th>
<th style="text-align: left;">Frequenza</th>
<th style="text-align: left;">Probabilità</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">~0.47</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">~0.23</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: left;">8</td>
<td style="text-align: left;">~0.19</td>
</tr>
<tr class="even">
<td style="text-align: left;">D</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">~0.12</td>
</tr>
</tbody>
</table>
<p><strong>Costruiamo l’albero:</strong></p>
<ul>
<li>
<strong>Passo 1:</strong> Uniamo i due simboli meno frequenti, <em>D (5)</em> e <em>C (8)</em>, in un nuovo nodo che chiamiamo temporaneamente <em>N1</em> con frequenza <em>13</em>.</li>
<li>
<strong>Passo 2:</strong> Ora i nodi disponibili sono A(20), B(10) e N1(13). I due meno frequenti sono <em>B (10)</em> e <em>N1 (13)</em>. Li uniamo in un nuovo nodo <em>N2</em> con frequenza <em>23</em>.</li>
<li>
<strong>Passo 3:</strong> Restano solo A(20) e N2(23). Li uniamo per formare la <em>radice</em> con frequenza <em>43</em>.</li>
</ul>
<p>L’albero risultante è:</p>
<pre><code>         (Radice:43)
         /         \
       0/           \1
      (A:20)      (N2:23)
                 /       \
               0/         \1
            (B:10)      (N1:13)
                       /       \
                     0/         \1
                   (D:5)       (C:8)</code></pre>
<p><strong>Assegniamo i codici</strong> (percorrendo il percorso dalla Radice alla foglia):</p>
<ul>
<li>A: il percorso è solo <code>0</code> → <strong>Codice: <code>0</code></strong>
</li>
<li>B: il percorso è Radice → N2 (<code>1</code>) → B (<code>0</code>) → <strong>Codice: <code>10</code></strong>
</li>
<li>D: il percorso è Radice → N2 (<code>1</code>) → N1 (<code>1</code>) → D (<code>0</code>) → <strong>Codice: <code>110</code></strong>
</li>
<li>C: il percorso è Radice → N2 (<code>1</code>) → N1 (<code>1</code>) → C (<code>1</code>) → <strong>Codice: <code>111</code></strong>
</li>
</ul>
<p>Ecco la nostra tabella di codifica finale:</p>
<table class="caption-top table">
<thead><tr class="header">
<th style="text-align: left;">Simbolo</th>
<th style="text-align: left;">Codice</th>
<th style="text-align: left;">Lunghezza</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">1 bit</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">2 bit</td>
</tr>
<tr class="odd">
<td style="text-align: left;">D</td>
<td style="text-align: left;">110</td>
<td style="text-align: left;">3 bit</td>
</tr>
<tr class="even">
<td style="text-align: left;">C</td>
<td style="text-align: left;">111</td>
<td style="text-align: left;">3 bit</td>
</tr>
</tbody>
</table>
<p>Notate come il simbolo più frequente (A) ha ottenuto il codice più corto (1 bit), mentre quelli più rari (C e D) hanno codici più lunghi (3 bit).</p>
</section><section id="collegamento-con-lentropia-quanto-ci-siamo-avvicinati-al-limite" class="level3" data-number="41.4.4"><h3 data-number="41.4.4" class="anchored" data-anchor-id="collegamento-con-lentropia-quanto-ci-siamo-avvicinati-al-limite">
<span class="header-section-number">41.4.4</span> Collegamento con l’entropia: quanto ci siamo avvicinati al limite?</h3>
<p>Torniamo alla teoria. Usando le probabilità dell’esempio, possiamo calcolare:</p>
<ul>
<li>
<strong>Lunghezza media del codice (<span class="math inline">\(L\)</span>):</strong> quanti bit usiamo in media per simbolo?</li>
</ul>
<p><span class="math display">\[
\begin{align}
L &amp;= (p(A)\cdot 1) + (p(B)\cdot 2) + (p(C)\cdot 3) + (p(D)\cdot 3) \notag\\
&amp;= (0.47\cdot 1) + (0.23\cdot 2) + (0.19\cdot 3) + (0.12\cdot 3) \notag \\
   &amp;\approx 1.9 \ \text{bit}
\end{align}
\]</span></p>
<ul>
<li>
<strong>Entropia (<span class="math inline">\(H(X)\)</span>):</strong> il limite teorico minimo di bit per simbolo.</li>
</ul>
<p><span class="math display">\[
\begin{align}
H(X) &amp;= -\big[\,0.47\log_2(0.47) + 0.23\log_2(0.23) \notag\\
&amp;\qquad + 0.19\log_2(0.19) + 0.12\log_2(0.12)\,\big] \notag\\
&amp; \quad\approx 1.85 \ \text{bit}
\end{align}
\]</span></p>
<p><strong>Risultato:</strong> La nostra codifica di Huffman (<code>1.9 bit/simbolo</code>) è estremamente vicina al limite teorico dell’entropia (<code>1.85 bit/simbolo</code>). La piccola differenza è dovuta al fatto che i codici devono avere una lunghezza intera (non possiamo avere un codice di 1.85 bit!), mentre l’entropia è un valore medio che può essere decimale.</p>
<section id="in-sintesi" class="level4" data-number="41.4.4.1"><h4 data-number="41.4.4.1" class="anchored" data-anchor-id="in-sintesi">
<span class="header-section-number">41.4.4.1</span> In sintesi</h4>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 46%">
<col style="width: 36%">
</colgroup>
<thead><tr class="header">
<th style="text-align: left;">Concetto</th>
<th style="text-align: left;">Significato Teorico</th>
<th style="text-align: left;">Analogia Psicologica (Approssimativa)</th>
</tr></thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Entropia H(X)</strong></td>
<td style="text-align: left;">Limite teorico assoluto di compressione. Misura l’incertezza/intrinseca.</td>
<td style="text-align: left;">Il “carico cognitivo” minimo necessario per rappresentare uno stimolo.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Codifica di Huffman</strong></td>
<td style="text-align: left;">Metodo pratico per costruire un codice ottimale che si avvicina al limite <code>H(X)</code>.</td>
<td style="text-align: left;">Una strategia cognitiva efficiente per categorizzare informazioni (es. etichette mentali corte per concetti comuni).</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Lunghezza media L</strong></td>
<td style="text-align: left;">Il risultato pratico ottenuto con Huffman.</td>
<td style="text-align: left;">Il reale “costo” cognitivo della strategia adottata.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Differenza (L - H(X))</strong></td>
<td style="text-align: left;">Quanto il metodo pratico si discosta dal limite teorico ideale.</td>
<td style="text-align: left;">Quanto la nostra strategia cognitiva è efficiente rispetto all’ideale.</td>
</tr>
</tbody>
</table>
<p>In sintesi, l’algoritmo di Huffman rappresenta un ponte tra la teoria e la pratica. Esso dimostra in modo tangibile come il principio astratto dell’entropia—il limite teorico di compressione—possa essere realizzato in una strategia concreta. Questo processo di ottimizzazione offre una potente analogia per ipotizzare come la nostra mente potrebbe elaborare le informazioni in modo efficiente, privilegiando gli stimoli più frequenti per risparmiare risorse cognitive.</p>
<div id="exercise-entropy-huffman" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio — Entropia e codifica di Huffman.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Supponiamo di avere una variabile casuale <span class="math inline">\(X\)</span> che può assumere quattro valori: <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, e <span class="math inline">\(D\)</span>, con le seguenti probabilità:</p>
<ul>
<li><span class="math inline">\(p(A) = 0.4\)</span></li>
<li><span class="math inline">\(p(B) = 0.3\)</span></li>
<li><span class="math inline">\(p(C) = 0.2\)</span></li>
<li><span class="math inline">\(p(D) = 0.1\)</span></li>
</ul>
<p>Per rappresentare questi esiti con un codice binario efficiente possiamo usare la <em>codifica di Huffman</em>, che assegna codici più brevi ai simboli più probabili, e codici più lunghi a quelli meno probabili.</p>
<p>Supponiamo che Huffman produca la seguente codifica:</p>
<ul>
<li>A = <code>0</code> (1 bit)</li>
<li>B = <code>10</code> (2 bit)</li>
<li>C = <code>110</code> (3 bit)</li>
<li>D = <code>111</code> (3 bit)</li>
</ul>
<p>La <em>lunghezza media del codice</em> si ottiene moltiplicando la probabilità di ciascun simbolo per la lunghezza del suo codice binario, e poi sommando:</p>
<p><span class="math display">\[
\begin{align}
\text{Lunghezza media} &amp;= (0.4 \times 1) + (0.3 \times 2) + (0.2 \times 3) + (0.1 \times 3) \\
&amp;= 0.4 + 0.6 + 0.6 + 0.3 = 1.9 \text{ bit}.
\end{align}
\]</span></p>
<p>Questo significa che, in media, servono <em>1.9 bit</em> per rappresentare un’osservazione della variabile <span class="math inline">\(X\)</span> usando la codifica di Huffman.</p>
<p>Confermiamo il risultato con il seguente codice R:</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Definizione delle probabilità</span></span>
<span><span class="va">probabilities</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>A <span class="op">=</span> <span class="fl">0.4</span>, B <span class="op">=</span> <span class="fl">0.3</span>, C <span class="op">=</span> <span class="fl">0.2</span>, D <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Funzione per la codifica di Huffman</span></span>
<span><span class="va">huffman_encoding</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">nodes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html">lapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">sym</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>symbol <span class="op">=</span> <span class="va">sym</span>, prob <span class="op">=</span> <span class="va">probabilities</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span>, left <span class="op">=</span> <span class="cn">NULL</span>, right <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw">while</span> <span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">nodes</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">nodes</span> <span class="op">&lt;-</span> <span class="va">nodes</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="va">nodes</span>, <span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="va">n</span><span class="op">$</span><span class="va">prob</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span>    <span class="va">left</span> <span class="op">&lt;-</span> <span class="va">nodes</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">right</span> <span class="op">&lt;-</span> <span class="va">nodes</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>    <span class="va">merged</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>symbol <span class="op">=</span> <span class="cn">NULL</span>, prob <span class="op">=</span> <span class="va">left</span><span class="op">$</span><span class="va">prob</span> <span class="op">+</span> <span class="va">right</span><span class="op">$</span><span class="va">prob</span>, left <span class="op">=</span> <span class="va">left</span>, right <span class="op">=</span> <span class="va">right</span><span class="op">)</span></span>
<span>    <span class="va">nodes</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">nodes</span><span class="op">[</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">]</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="va">merged</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">assign_codes</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">node</span>, <span class="va">prefix</span> <span class="op">=</span> <span class="st">""</span>, <span class="va">code_map</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NULL.html">is.null</a></span><span class="op">(</span><span class="va">node</span><span class="op">$</span><span class="va">symbol</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>      <span class="va">code_map</span><span class="op">[[</span><span class="va">node</span><span class="op">$</span><span class="va">symbol</span><span class="op">]</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="va">prefix</span></span>
<span>    <span class="op">}</span> <span class="kw">else</span> <span class="op">{</span></span>
<span>      <span class="va">code_map</span> <span class="op">&lt;-</span> <span class="fu">assign_codes</span><span class="op">(</span><span class="va">node</span><span class="op">$</span><span class="va">left</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">prefix</span>, <span class="st">"0"</span><span class="op">)</span>, <span class="va">code_map</span><span class="op">)</span></span>
<span>      <span class="va">code_map</span> <span class="op">&lt;-</span> <span class="fu">assign_codes</span><span class="op">(</span><span class="va">node</span><span class="op">$</span><span class="va">right</span>, <span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste0</a></span><span class="op">(</span><span class="va">prefix</span>, <span class="st">"1"</span><span class="op">)</span>, <span class="va">code_map</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">code_map</span><span class="op">)</span></span>
<span>  <span class="op">}</span></span>
<span></span>
<span>  <span class="va">code_map</span> <span class="op">&lt;-</span> <span class="fu">assign_codes</span><span class="op">(</span><span class="va">nodes</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span>  <span class="va">avg_length</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html">sapply</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span>, <span class="kw">function</span><span class="op">(</span><span class="va">sym</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">probabilities</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/nchar.html">nchar</a></span><span class="op">(</span><span class="va">code_map</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span>  <span class="op">}</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>avg_length <span class="op">=</span> <span class="va">avg_length</span>, huffman_dict <span class="op">=</span> <span class="va">code_map</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># Applicazione e stampa dei risultati</span></span>
<span><span class="va">result</span> <span class="op">&lt;-</span> <span class="fu">huffman_encoding</span><span class="op">(</span><span class="va">probabilities</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"Lunghezza media del codice di Huffman: %.2f bit/simbolo\n"</span>, <span class="va">result</span><span class="op">$</span><span class="va">avg_length</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; Lunghezza media del codice di Huffman: 1.90 bit/simbolo</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="st">"Codici di Huffman:\n"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Codici di Huffman:</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">sym</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">result</span><span class="op">$</span><span class="va">huffman_dict</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/cat.html">cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sprintf.html">sprintf</a></span><span class="op">(</span><span class="st">"%s: %s\n"</span>, <span class="va">sym</span>, <span class="va">result</span><span class="op">$</span><span class="va">huffman_dict</span><span class="op">[[</span><span class="va">sym</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="co">#&gt; A: 0</span></span>
<span><span class="co">#&gt; B: 10</span></span>
<span><span class="co">#&gt; D: 110</span></span>
<span><span class="co">#&gt; C: 111</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Ora calcoliamo l’entropia teorica della variabile <span class="math inline">\(X\)</span>, cioè la lunghezza media <strong>minima</strong> che qualsiasi codifica binaria può raggiungere:</p>
<p><span class="math display">\[
\begin{align}
H(X) &amp;= - \sum p(x) \log_2 p(x) \\
     &amp;= -[0.4 \log_2 0.4 + 0.3 \log_2 0.3 + 0.2 \log_2 0.2 + 0.1 \log_2 0.1] \\
     &amp;= 1.8465 \text{ bit}.
\end{align}
\]</span> Il valore dell’entropia è leggermente <em>inferiore alla lunghezza media di Huffman</em> (1.9 bit). Questo è normale: Huffman fornisce <em>codici con lunghezza intera in bit</em>, mentre l’entropia può assumere valori decimali. La codifica di Huffman è quindi <em>quasi ottimale</em>.</p>
<p>In sintesi:</p>
<ul>
<li>
<em>l’entropia <span class="math inline">\(H(X)\)</span></em> rappresenta la <em>lunghezza media teorica minima</em> (in bit) per codificare una variabile casuale;</li>
<li>la <em>codifica di Huffman</em> costruisce un codice binario che si avvicina molto a questo limite, usando <em>più bit per i simboli rari</em> e <em>meno bit per quelli frequenti</em>;</li>
<li>in questo modo, l’entropia ci offre un criterio per valutare <em>quanto efficiente</em> è una codifica: <em>più la lunghezza media si avvicina all’entropia, più è efficiente</em>.</li>
</ul>
</div>
</div>
</div>
</section></section></section><section id="applicazioni-psicologiche" class="level2" data-number="41.5"><h2 data-number="41.5" class="anchored" data-anchor-id="applicazioni-psicologiche">
<span class="header-section-number">41.5</span> Applicazioni psicologiche</h2>
<p>Il concetto di entropia, inteso come misura della sorpresa media associata a un evento, trova applicazioni dirette anche nello studio di fenomeni psicologici. In particolare, la sorpresa — formalizzabile in termini di informazione di Shannon — è stata associata a cambiamenti emotivi, processi di apprendimento e modulazione della motivazione.</p>
<p>Un esempio classico è fornito da <span class="citation" data-cites="spector1956expectations">Spector (<a href="#ref-spector1956expectations" role="doc-biblioref">1956</a>)</span>, che studiò l’effetto della probabilità a priori sulla soddisfazione dei soggetti in seguito a una promozione lavorativa. I risultati mostrarono che esiti inizialmente percepiti come poco probabili — e quindi più sorprendenti quando si verificano — producevano un impatto emotivo maggiore rispetto a esiti attesi. In altre parole, la sorpresa amplificava la risposta affettiva, confermando l’idea che l’entropia non sia solo una misura astratta, ma un indicatore della potenziale intensità della reazione emotiva.</p>
<p>Ricerche più recenti, in contesti sia sperimentali che ecologici, hanno confermato questo legame. Ad esempio, studi nell’ambito delle neuroscienze cognitive hanno mostrato che eventi ad alta sorpresa modulano l’attività di aree cerebrali legate all’elaborazione emotiva, come l’amigdala e la corteccia prefrontale ventromediale, influenzando sia l’umore immediato sia l’apprendimento successivo. Allo stesso modo, nell’analisi dei dati di <em>Ecological Momentary Assessment</em> (EMA), la probabilità soggettiva di un evento può essere messa in relazione alla variazione momentanea dell’umore, mostrando che episodi rari o inattesi tendono a generare oscillazioni emotive più marcate.</p>
<p>Questi risultati illustrano bene come il concetto di entropia possa essere utilizzato in psicologia non solo come strumento di misura della distribuzione di probabilità degli eventi, ma anche come variabile esplicativa in modelli che indagano il legame tra aspettative, sorpresa e stati emotivi. Questo stesso legame sarà centrale quando, nelle prossime sezioni, introdurremo la divergenza di Kullback–Leibler e la utilizzeremo per confrontare modelli in un’ottica bayesiana.</p>
<div id="exercise-entropy-huffman" class="exercise callout callout-style-simple callout-none no-icon callout-titled" data-collapsed="true">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Esercizio – probabilità, sorpresa e umore.
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>In questo esempio, simuliamo 200 osservazioni in cui ogni partecipante sperimenta un evento con probabilità variabile. La <em>sorpresa</em> di ciascun evento viene calcolata con la formula di Shannon, e l’effetto sull’umore viene simulato assumendo che eventi più sorprendenti producano, in media, variazioni di umore più ampie (positive o negative).</p>
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Numero di osservazioni</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span></span>
<span><span class="co"># Probabilità percepita dell'evento (da molto probabile a molto improbabile)</span></span>
<span><span class="va">p_event</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html">runif</a></span><span class="op">(</span><span class="va">n</span>, min <span class="op">=</span> <span class="fl">0.05</span>, max <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Sorpresa di Shannon (in bit)</span></span>
<span><span class="va">surprise</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html">log2</a></span><span class="op">(</span><span class="va">p_event</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Variazione di umore simulata:</span></span>
<span><span class="co"># partiamo da un effetto medio proporzionale alla sorpresa, con rumore casuale</span></span>
<span><span class="va">delta_mood</span> <span class="op">&lt;-</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">surprise</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">rnorm</a></span><span class="op">(</span><span class="va">n</span>, mean <span class="op">=</span> <span class="fl">0</span>, sd <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Mettiamo tutto in un data frame</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span></span>
<span>  p_event <span class="op">=</span> <span class="va">p_event</span>,</span>
<span>  surprise <span class="op">=</span> <span class="va">surprise</span>,</span>
<span>  delta_mood <span class="op">=</span> <span class="va">delta_mood</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Visualizzazione</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span><span class="va">df</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">surprise</span>, y <span class="op">=</span> <span class="va">delta_mood</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span>alpha <span class="op">=</span> <span class="fl">0.6</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_smooth.html">geom_smooth</a></span><span class="op">(</span>method <span class="op">=</span> <span class="st">"lm"</span>, se <span class="op">=</span> <span class="cn">TRUE</span>, color <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span></span>
<span>    x <span class="op">=</span> <span class="st">"Sorpresa (bit)"</span>,</span>
<span>    y <span class="op">=</span> <span class="st">"Δ Umore"</span></span>
<span>  <span class="op">)</span> </span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="01_entropy_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<p><strong>Interpretazione.</strong> Il grafico mostra che, in questa simulazione, eventi più sorprendenti (bit più alti) tendono a produrre variazioni di umore maggiori. Questo illustra visivamente l’idea, già documentata empiricamente, che la sorpresa può amplificare la risposta emotiva.</p>
</div>
</div>
</div>
</section><section id="riflessioni-conclusive" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="riflessioni-conclusive">Riflessioni conclusive</h2>
<p>In questo capitolo abbiamo esplorato come l’<em>entropia</em> ci permetta di misurare quantitativamente l’incertezza e l’informazione in sistemi complessi. Attraverso esempi concreti - dal lancio di una moneta alla codifica di messaggi - abbiamo visto come questo concetto matematico possa essere applicato in modo pratico e intuitivo.</p>
<section id="cosa-abbiamo-imparato" class="level3" data-number="41.5.1"><h3 data-number="41.5.1" class="anchored" data-anchor-id="cosa-abbiamo-imparato">
<span class="header-section-number">41.5.1</span> Cosa abbiamo imparato</h3>
<ol type="1">
<li>
<em>L’entropia misura l’incertezza</em>: Più una situazione è imprevedibile (come una moneta equilibrata), maggiore è la sua entropia. Situazioni prevedibili (come un comportamento stereotipato) hanno invece entropia bassa.</li>
<li>
<em>L’informazione è sorpresa</em>: Eventi rari e inaspettati ci forniscono più informazione rispetto a eventi comuni. La formula di Shannon cattura precisamente questa intuizione quotidiana.</li>
<li>
<em>Esiste un limite alla compressione</em>: L’entropia rappresenta il numero minimo di bit necessari per descrivere un’informazione senza perdite. L’algoritmo di Huffman ci mostra come avvicinarci a questo limite nella pratica.</li>
</ol>
<section id="perché-è-rilevante-per-la-psicologia" class="level4" data-number="41.5.1.1"><h4 data-number="41.5.1.1" class="anchored" data-anchor-id="perché-è-rilevante-per-la-psicologia">
<span class="header-section-number">41.5.1.1</span> Perché è rilevante per la psicologia?</h4>
<p>Questi concetti non sono solo astratti, ma trovano applicazioni concrete nella ricerca psicologica:</p>
<ul>
<li>
<em>Modellizzazione cognitiva</em>: I processi mentali possono essere visti come sistemi che elaborano informazione. L’entropia ci aiuta a quantificare quanto “lavoro” cognitivo sia necessario per processare stimoli diversi.</li>
<li>
<em>Emozioni e sorpresa</em>: Come abbiamo visto nell’esempio finale, eventi sorprendenti (alta entropia) tendono a produrre risposte emotive più intense. Questo collegamento tra probabilità e emozione è un campo di ricerca attivo.</li>
<li>
<em>Valutazione dei modelli</em>: Nei prossimi capitoli vedremo come l’entropia sia la base per strumenti che ci permettono di confrontare modelli psicologici e valutarne la capacità predittiva.</li>
</ul></section><section id="uno-sguardo-al-futuro" class="level4" data-number="41.5.1.2"><h4 data-number="41.5.1.2" class="anchored" data-anchor-id="uno-sguardo-al-futuro">
<span class="header-section-number">41.5.1.2</span> Uno sguardo al futuro</h4>
<p>L’entropia non è solo un concetto isolato, ma il fondamento per strumenti più avanzati che incontreremo:</p>
<ul>
<li>la <em>divergenza di Kullback-Leibler</em> (nel prossimo capitolo) misura quanto un modello si discosta dalla realtà, usando proprio i concetti di entropia che abbiamo appreso;</li>
<li>l’<em>ELPD</em> (Expected Log Predictive Density) ci aiuterà a confrontare modelli bayesiani valutando la loro capacità predittiva.</li>
</ul>
<p>Comprendere l’entropia significa quindi possedere una chiave interpretativa potente: ci permette di passare dall’osservazione qualitativa (“questo comportamento è più variabile”) alla misurazione quantitativa (“l’entropia di questo comportamento è X bit”).</p>
<div class="callout callout-style-simple callout-note no-icon callout-titled" title="Mappa concettuale: dall'entropia alla valutazione dei modelli">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Mappa concettuale: dall’entropia alla valutazione dei modelli
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Entropia <span class="math inline">\(H(X)\)</span></strong><br>
→ Misura l’incertezza intrinseca di una variabile casuale.<br>
→ Interpretabile come la sorpresa media o la lunghezza media minima (in bit) necessaria per codificare gli esiti di <span class="math inline">\(X\)</span>.</p>
<p><strong>Divergenza di Kullback–Leibler <span class="math inline">\(D_{KL}(P \parallel Q)\)</span></strong><br>
→ Confronta due distribuzioni di probabilità <span class="math inline">\(P\)</span> (la “vera” distribuzione) e <span class="math inline">\(Q\)</span> (il modello).<br>
→ Misura <em>quanto</em> il modello <span class="math inline">\(Q\)</span> si discosta da <span class="math inline">\(P\)</span> in termini di inefficienza nel codificare i dati.</p>
<p><strong>Expected Log Predictive Density (ELPD)</strong><br>
→ Valuta la capacità predittiva di un modello su dati nuovi.<br>
→ Collegata alla minimizzazione della KL tra la distribuzione dei dati e la distribuzione predittiva del modello.<br>
→ Più alto è l’ELPD, migliore è la capacità del modello di rappresentare e prevedere i dati.</p>
<p><strong>Collegamento logico:</strong><br>
Entropia → ci dice quanta incertezza c’è nei dati.<br>
KL → ci dice quanto un modello spreca informazione rispetto a quella incertezza.<br>
ELPD → ci dice quanto bene il modello prevede, riducendo quello spreco.</p>
<div id="fig-h-kl-elpd" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-h-kl-elpd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../figures/entropy_kl_elpd.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-h-kl-elpd-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figura&nbsp;41.1: Diagramma visivo che collega Entropia → Divergenza KL → ELPD.
</figcaption></figure>
</div>
</div>
</div>
</div>
<div class="callout callout-style-simple callout-note no-icon callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Informazioni sull’ambiente di sviluppo
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell" data-layout-align="center">
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/sessionInfo.html">sessionInfo</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; R version 4.5.1 (2025-06-13)</span></span>
<span><span class="co">#&gt; Platform: aarch64-apple-darwin20</span></span>
<span><span class="co">#&gt; Running under: macOS Sequoia 15.6.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Matrix products: default</span></span>
<span><span class="co">#&gt; BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib </span></span>
<span><span class="co">#&gt; LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; locale:</span></span>
<span><span class="co">#&gt; [1] C/UTF-8/C/C/C/C</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; time zone: Europe/Rome</span></span>
<span><span class="co">#&gt; tzcode source: internal</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; attached base packages:</span></span>
<span><span class="co">#&gt; [1] stats     graphics  grDevices utils     datasets  methods   base     </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; other attached packages:</span></span>
<span><span class="co">#&gt;  [1] tidygraph_1.3.1       ggraph_2.2.2          igraph_2.1.4         </span></span>
<span><span class="co">#&gt;  [4] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      </span></span>
<span><span class="co">#&gt;  [7] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     </span></span>
<span><span class="co">#&gt; [10] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     </span></span>
<span><span class="co">#&gt; [13] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         </span></span>
<span><span class="co">#&gt; [16] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           </span></span>
<span><span class="co">#&gt; [19] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        </span></span>
<span><span class="co">#&gt; [22] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         </span></span>
<span><span class="co">#&gt; [25] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            </span></span>
<span><span class="co">#&gt; [28] here_1.0.1           </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; loaded via a namespace (and not attached):</span></span>
<span><span class="co">#&gt;  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       </span></span>
<span><span class="co">#&gt;  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      </span></span>
<span><span class="co">#&gt;  [7] snakecase_0.11.1      compiler_4.5.1        mgcv_1.9-3           </span></span>
<span><span class="co">#&gt; [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        </span></span>
<span><span class="co">#&gt; [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        </span></span>
<span><span class="co">#&gt; [16] backports_1.5.0       labeling_0.4.3        rmarkdown_2.29       </span></span>
<span><span class="co">#&gt; [19] ragg_1.5.0            purrr_1.1.0           xfun_0.53            </span></span>
<span><span class="co">#&gt; [22] cachem_1.1.0          jsonlite_2.0.0        tweenr_2.0.3         </span></span>
<span><span class="co">#&gt; [25] broom_1.0.9           parallel_4.5.1        R6_2.6.1             </span></span>
<span><span class="co">#&gt; [28] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      </span></span>
<span><span class="co">#&gt; [31] estimability_1.5.1    knitr_1.50            zoo_1.8-14           </span></span>
<span><span class="co">#&gt; [34] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     </span></span>
<span><span class="co">#&gt; [37] tidyselect_1.2.1      viridis_0.6.5         abind_1.4-8          </span></span>
<span><span class="co">#&gt; [40] yaml_2.3.10           codetools_0.2-20      curl_7.0.0           </span></span>
<span><span class="co">#&gt; [43] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          </span></span>
<span><span class="co">#&gt; [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       </span></span>
<span><span class="co">#&gt; [49] survival_3.8-3        RcppParallel_5.1.11-1 polyclip_1.10-7      </span></span>
<span><span class="co">#&gt; [52] tensorA_0.36.2.1      checkmate_2.3.3       stats4_4.5.1         </span></span>
<span><span class="co">#&gt; [55] distributional_0.5.0  generics_0.1.4        rprojroot_2.1.1      </span></span>
<span><span class="co">#&gt; [58] rstantools_2.5.0      scales_1.4.0          xtable_1.8-4         </span></span>
<span><span class="co">#&gt; [61] glue_1.8.0            emmeans_1.11.2-8      tools_4.5.1          </span></span>
<span><span class="co">#&gt; [64] graphlayouts_1.2.2    mvtnorm_1.3-3         grid_4.5.1           </span></span>
<span><span class="co">#&gt; [67] QuickJSR_1.8.0        colorspace_2.1-1      nlme_3.1-168         </span></span>
<span><span class="co">#&gt; [70] ggforce_0.5.0         cli_3.6.5             textshaping_1.0.3    </span></span>
<span><span class="co">#&gt; [73] svUnit_1.0.8          viridisLite_0.4.2     Brobdingnag_1.2-9    </span></span>
<span><span class="co">#&gt; [76] V8_7.0.0              gtable_0.3.6          digest_0.6.37        </span></span>
<span><span class="co">#&gt; [79] ggrepel_0.9.6         TH.data_1.1-4         htmlwidgets_1.6.4    </span></span>
<span><span class="co">#&gt; [82] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    </span></span>
<span><span class="co">#&gt; [85] lifecycle_1.0.4       MASS_7.3-65</span></span></code><button title="Copia negli appunti" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
</section></section></section><section id="bibliografia" class="level2 unnumbered unlisted"><h2 class="unnumbered unlisted anchored" data-anchor-id="bibliografia">Bibliografia</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-spector1956expectations" class="csl-entry" role="listitem">
Spector, A. J. (1956). Expectations, fulfillment, and morale. <em>The Journal of Abnormal and Social Psychology</em>, <em>52</em>(1), 51–56.
</div>
<div id="ref-stone2022information" class="csl-entry" role="listitem">
Stone, J. V. (2022). <em>Information theory: a tutorial introduction, 2nd edition</em>.
</div>
</div>
</section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><hr>
<ol>
<li id="fn1"><p>Ricorda che per le proprietà dei logaritmi: <span class="math inline">\(\log(1/x) = -\log(x)\)</span>, perché <span class="math inline">\(\log(1/x) = \log(1) - \log(x) = 0 - \log(x)\)</span>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol></section></main><!-- /main --><script>
document.body.classList.add('classic-book');
document.addEventListener('DOMContentLoaded', function() {
  const paragraphs = document.querySelectorAll('p');
  paragraphs.forEach(p => {
    if (p.textContent.length > 200) {
      p.style.hyphens = 'auto';
      p.style.hyphenateCharacter = '-';
    }
  });
  const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
  headings.forEach(h => {
    h.style.fontFeatureSettings = '"liga" 1, "dlig" 1, "smcp" 1';
  });
});
</script><script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copiato!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copiato!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/ccaudek\.github\.io\/psicometria-r\/intro\.html");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
            // target, if specified
            link.setAttribute("target", "_blank");
            if (link.getAttribute("rel") === null) {
              link.setAttribute("rel", "noopener");
            }
            // default icon
            link.classList.add("external");
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="../../chapters/entropy/introduction_sec.html" class="pagination-link" aria-label="Introduzione alla sezione">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduzione alla sezione</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../../chapters/entropy/02_kl.html" class="pagination-link" aria-label="La divergenza di Kullback-Leibler">
        <span class="nav-page-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">La divergenza di Kullback-Leibler</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p><strong>Psicometria</strong> è una risorsa didattica creata per il corso di Scienze e Tecniche Psicologiche dell’Università degli Studi di Firenze.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ccaudek/psicometria-r/blob/main/chapters/entropy/01_entropy.qmd" class="toc-action"><i class="bi bi-github"></i>Mostra il codice</a></li><li><a href="https://github.com/ccaudek/psicometria-r/issues/new" class="toc-action"><i class="bi empty"></i>Segnala un problema</a></li></ul></div></div>
    <div class="nav-footer-right">
<p>Realizzato con <a href="https://quarto.org/">Quarto</a>.</p>
</div>
  </div>
</footer>


<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>