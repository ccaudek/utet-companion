{
  "hash": "3d9038e3d9de0ddbb5fe6bafa9896176",
  "result": {
    "engine": "knitr",
    "markdown": "# Parametrizzazioni centered e non-centered {#sec-mcmc-ncp}\n\n## Introduzione {.unnumbered .unlisted}\n\nQuando si lavora con modelli gerarchici in Stan, una delle prime scelte riguarda il modo in cui vengono parametrizzate le variabili latenti. Questa decisione, che a prima vista può sembrare di natura puramente tecnica, ha invece conseguenze sostanziali sia sulla stabilità computazionale della stima sia sull’interpretazione dei risultati. In particolare, la distinzione tra *parametrizzazione centrata* e *parametrizzazione non centrata* rappresenta un passaggio cruciale per comprendere perché alcuni modelli convergono rapidamente mentre altri incontrano difficoltà anche su dataset apparentemente semplici.\n\nIl cuore della questione risiede nel modo in cui le variabili latenti vengono messe in relazione con i parametri iperpriors. Nella parametrizzazione centrata si assume che le osservazioni dipendano direttamente dalla distribuzione a priori, il che funziona bene quando i dati forniscono informazioni forti e coerenti. Tuttavia, in presenza di dati deboli o altamente variabili, questa scelta può produrre catene MCMC lente e autocorrelate. La parametrizzazione non centrata, al contrario, introduce una trasformazione che disaccoppia i livelli del modello e riduce la dipendenza diretta dai parametri iperpriors. Questa semplice modifica può rendere l’inferenza molto più efficiente, specialmente nei contesti in cui le informazioni empiriche non sono sufficienti a “guidare” l’algoritmo verso una stima stabile.\n\nPer comprendere appieno la differenza tra queste due strategie, è utile partire da un esempio elementare, quello di una singola media normale con varianza nota. Questo caso, apparentemente banale, permette di isolare i meccanismi in gioco e di mostrare come la scelta della parametrizzazione influenzi direttamente il comportamento del campionatore. Nei paragrafi che seguono analizzeremo dunque questo scenario, prima dal punto di vista teorico e poi con esempi numerici e implementazioni concrete in Stan.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\nSi veda il capitolo 13 \"Models with memory\" di *Statistical rethinking* [@McElreath_rethinking].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(cmdstanr, posterior, bayesplot, ggplot2, dplyr, tibble, forcats)\nconflicts_prefer(posterior::ess_bulk)\nconflicts_prefer(posterior::ess_tail)\n```\n:::\n\n:::\n\n## Il caso di una media normale\n\nImmaginiamo di voler stimare la media di una distribuzione normale. Supponiamo che i dati $y_1, y_2, \\ldots, y_n$ provengano da una distribuzione normale con varianza nota $\\sigma^2$ e media incognita $\\mu$. Questo è uno dei problemi più elementari dell’inferenza statistica, ma ci permette di illustrare con chiarezza la differenza tra parametrizzazione centrata e non centrata.\n\nIn un’impostazione bayesiana assegniamo a $\\mu$ una distribuzione a priori, ad esempio una normale con media $\\mu_0$ e deviazione standard $\\tau$. La struttura gerarchica del modello è dunque molto semplice: a un primo livello si collocano i dati osservati, distribuiti come $\\mathcal{N}(\\mu, \\sigma^2)$; a un secondo livello, la media $\\mu$ è a sua volta distribuita come $\\mathcal{N}(\\mu_0, \\tau^2)$.\n\nNella *parametrizzazione centrata*, $\\mu$ viene campionato direttamente da questa distribuzione a priori. La relazione tra iperparametri e parametri è quindi immediata: si parte da $\\mu_0$ e si aggiunge rumore gaussiano con deviazione standard $\\tau$. Questa scelta appare naturale ed è perfettamente adeguata quando i dati sono molto informativi, cioè quando la varianza campionaria è piccola e il numero di osservazioni è grande. In tali condizioni, la distribuzione a posteriori di $\\mu$ è ben identificata e il campionatore esplora rapidamente lo spazio dei parametri.\n\nLe difficoltà emergono quando i dati forniscono informazioni scarse o contraddittorie. In questi casi, la catena MCMC può soffrire di forti autocorrelazioni: il campionatore si muove lentamente, resta intrappolato in alcune regioni dello spazio dei parametri e richiede un numero molto elevato di iterazioni per ottenere stime affidabili.\n\nLa *parametrizzazione non centrata* affronta questo problema con un piccolo ma decisivo cambiamento. Invece di campionare $\\mu$ direttamente, si introduce una variabile ausiliaria $z \\sim \\mathcal{N}(0,1)$ e si definisce $\\mu = \\mu_0 + \\tau z$. In questo modo, $\\mu$ è ancora distribuita secondo $\\mathcal{N}(\\mu_0, \\tau^2)$, ma la relazione con gli iperparametri è mediata dalla variabile standardizzata $z$. Questa trasformazione spezza la dipendenza diretta tra i livelli del modello e consente al campionatore di esplorare lo spazio parametrico in maniera più efficiente, specialmente quando i dati sono poco informativi.\n\nIl caso della media normale rappresenta dunque un laboratorio ideale per osservare le implicazioni delle due strategie. Nelle sezioni successive mostreremo, attraverso esempi numerici e codici Stan, come queste differenze teoriche si traducano in prestazioni computazionali e qualità dell’inferenza.\n\n## Esempi numerici\n\nPer rendere più concreta la distinzione tra parametrizzazione centrata e non centrata, consideriamo un semplice set di dati simulati. Supponiamo di avere cinque osservazioni generate da una distribuzione normale con media sconosciuta $\\mu$ e varianza fissata a $\\sigma^2 = 1$. Assegniamo a $\\mu$ un prior gaussiano con media zero e deviazione standard pari a dieci. Questo scenario, volutamente essenziale, ci permette di osservare senza distrazioni gli effetti della scelta di parametrizzazione.\n\nNel caso centrato, il modello in Stan assume la forma più diretta: si dichiara il parametro $\\mu$, lo si vincola a una distribuzione a priori $\\mathcal{N}(0, 10^2)$ e si specifica la distribuzione dei dati come $\\mathcal{N}(\\mu, 1)$. In altre parole, il campionatore deve muoversi nello spazio dei parametri partendo da un collegamento immediato tra la media a priori e le osservazioni.\n\nIl modello non centrato introduce invece una variabile ausiliaria $z$, distribuita come $\\mathcal{N}(0,1)$. La media diventa $\\mu = 0 + 10z$, e i dati vengono modellati ancora come $\\mathcal{N}(\\mu, 1)$. Sebbene i due modelli siano matematicamente equivalenti, dal punto di vista computazionale il secondo spesso si comporta meglio quando i dati non forniscono informazioni sufficienti per “ancorare” $\\mu$.\n\nPer illustrare questa differenza possiamo confrontare i risultati delle due implementazioni. Con dati molto informativi — ad esempio con un numero elevato di osservazioni — i due approcci producono catene MCMC di qualità simile: le stime della media e l’esplorazione dello spazio dei parametri avvengono senza difficoltà. Quando però il numero di osservazioni è ridotto, oppure la varianza è elevata, la parametrizzazione centrata mostra spesso autocorrelazioni marcate e una scarsa efficienza campionaria. La parametrizzazione non centrata, al contrario, mantiene catene più stabili e riduce la necessità di un numero eccessivo di iterazioni.\n\nQuesti esempi, seppur elementari, mettono in evidenza un principio generale: la scelta della parametrizzazione non modifica la sostanza del modello, ma può determinare una differenza sostanziale nella sua resa pratica. Nei modelli gerarchici più complessi, la distinzione tra centrato e non centrato diventa cruciale per rendere l’inferenza non solo più efficiente, ma talvolta persino possibile.\n\n## Codice Stan\n\nNel modello centrato la media $\\mu$ è trattata direttamente come parametro, dotato di una distribuzione a priori normale; i dati sono condizionati su $\\mu$ con varianza nota. Questa scrittura è la più naturale e, quando i dati sono informativi, conduce spesso a catene ben miscelate.\n\n```stan\n// file: mean_centered.stan\ndata {\n  int<lower=1> N;            // numero di osservazioni\n  vector[N] y;               // dati osservati\n  real<lower=0> sigma;       // deviazione standard nota del likelihood\n  real mu0;                  // media del prior su mu\n  real<lower=0> tau;         // deviazione standard del prior su mu\n}\nparameters {\n  real mu;                   // media incognita (parametrizzazione centrata)\n}\nmodel {\n  mu ~ normal(mu0, tau);     // prior centrato\n  y  ~ normal(mu, sigma);    // modello di verosimiglianza\n}\ngenerated quantities {\n  vector[N] y_rep;           // repliche predittive\n  vector[N] log_lik;         // log-verosimiglianza punto-a-punto\n  for (n in 1:N) {\n    y_rep[n] = normal_rng(mu, sigma);\n    log_lik[n] = normal_lpdf(y[n] | mu, sigma);\n  }\n}\n```\n\nLa versione non centrata introduce una variabile standardizzata $z \\sim \\mathcal{N}(0,1)$ e definisce $\\mu = \\mu_0 + \\tau z$. La distribuzione implicita di $\\mu$ rimane identica a quella del modello centrato, ma la geometria dello spazio dei parametri cambia in modo decisivo: la dipendenza tra livelli gerarchici si allenta e, in scenari a bassa informatività, il campionatore può muoversi con maggiore efficienza.\n\n```stan\n// file: mean_noncentered.stan\ndata {\n  int<lower=1> N;           \n  vector[N] y;              \n  real<lower=0> sigma;      \n  real mu0;                 \n  real<lower=0> tau;        \n}\nparameters {\n  real z;                   // variabile standardizzata (non centrata)\n}\ntransformed parameters {\n  real mu = mu0 + tau * z;  // ricostruzione della media sullo scale del prior\n}\nmodel {\n  z ~ normal(0, 1);         // prior equivalente su mu, espresso via z\n  y ~ normal(mu, sigma);    // verosimiglianza invariata\n}\ngenerated quantities {\n  vector[N] y_rep;\n  vector[N] log_lik;\n  for (n in 1:N) {\n    y_rep[n] = normal_rng(mu, sigma);\n    log_lik[n] = normal_lpdf(y[n] | mu, sigma);\n  }\n}\n```\n\n## Esecuzione con `cmdstanr` e confronto tra centrato e non centrato\n\nPer apprezzare le differenze tra le due parametrizzazioni, conviene fissare uno scenario leggermente “difficile” per il campionatore: pochi dati e un prior deliberatamente ampio sulla media. In questo modo la geometria della posterior risulta poco concentrata e gli effetti della scelta di parametrizzazione emergono con maggiore nettezza.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1234)\n\n# Dati simulati: pochi punti e varianza del likelihood modesta\nN     <- 5\nsigma <- 1.0\nmu_true <- -0.5\ny <- rnorm(N, mean = mu_true, sd = sigma)\n\n# Prior ampio sulla media\nmu0 <- 0\ntau <- 20\n\nstan_centered <- '\ndata {\n  int<lower=1> N;\n  vector[N] y;\n  real<lower=0> sigma;\n  real mu0;\n  real<lower=0> tau;\n}\nparameters {\n  real mu;\n}\nmodel {\n  mu ~ normal(mu0, tau);\n  y  ~ normal(mu, sigma);\n}\ngenerated quantities {\n  vector[N] y_rep;\n  vector[N] log_lik;\n  for (n in 1:N) {\n    y_rep[n] = normal_rng(mu, sigma);\n    log_lik[n] = normal_lpdf(y[n] | mu, sigma);\n  }\n}\n'\n\nstan_noncentered <- '\ndata {\n  int<lower=1> N;\n  vector[N] y;\n  real<lower=0> sigma;\n  real mu0;\n  real<lower=0> tau;\n}\nparameters {\n  real z;\n}\ntransformed parameters {\n  real mu = mu0 + tau * z;\n}\nmodel {\n  z ~ normal(0, 1);\n  y ~ normal(mu, sigma);\n}\ngenerated quantities {\n  vector[N] y_rep;\n  vector[N] log_lik;\n  for (n in 1:N) {\n    y_rep[n] = normal_rng(mu, sigma);\n    log_lik[n] = normal_lpdf(y[n] | mu, sigma);\n  }\n}\n'\n\nwriteLines(stan_centered, here(\"stan\",\" mean_centered.stan\"))\nwriteLines(stan_noncentered, here(\"stan\", \"mean_noncentered.stan\"))\n\n# Compilazione\nmod_c  <- cmdstan_model(here(\"stan\",\" mean_centered.stan\"))\nmod_nc <- cmdstan_model(here(\"stan\", \"mean_noncentered.stan\"))\n\n# Dati per Stan\ndat <- list(\n  N = N, \n  y = as.numeric(y), \n  sigma = sigma, \n  mu0 = mu0, \n  tau = tau\n  )\n\n# Scelte HMC caute, per evitare divergenze in scenari più spigolosi\nhmc_args <- list(adapt_delta = 0.95, max_treedepth = 12)\n\nfit_c <- mod_c$sample(\n  data = dat, seed = 6543, chains = 4, parallel_chains = 4,\n  iter_warmup = 1000, iter_sampling = 1000,\n  refresh = 0, \n  adapt_delta = hmc_args$adapt_delta, max_treedepth = hmc_args$max_treedepth\n)\n\nfit_nc <- mod_nc$sample(\n  data = dat, seed = 7654, chains = 4, parallel_chains = 4,\n  iter_warmup = 1000, iter_sampling = 1000,\n  refresh = 0, \n  adapt_delta = hmc_args$adapt_delta, max_treedepth = hmc_args$max_treedepth\n)\n\n# Estratti posteriori in formato draws\ndraws_c  <- fit_c$draws()\ndraws_nc <- fit_nc$draws()\n\n# Riassunti diagnostici essenziali\n# Centered: esiste mu ma NON esiste z\nsum_c  <- posterior::summarise_draws(fit_c$draws(variables = \"mu\"))\n\n# Non-centered: esiste z; mu esiste se lo hai definito nei transformed parameters\n# (nel nostro codice sì: `real mu = mu0 + tau * z;`)\nsum_nc <- posterior::summarise_draws(fit_nc$draws(variables = c(\"mu\", \"z\")))\n```\n:::\n\n\nL’output di `summarise_draws` fornisce, oltre alle stime a posteriori, lo $\\widehat{R}$ e due misure di dimensione campionaria effettiva: *ESS bulk* ed *ESS tail*. La prima descrive la qualità della miscela nelle *regioni centrali* della distribuzione a posteriori; quando il campionatore esplora con facilità il cuore della distribuzione, l’ESS bulk tende a valori elevati. La seconda, l’ESS tail, quantifica la qualità della miscela nelle *code*; è sensibile a esplorazioni lente o a salti rari verso le regioni estreme, fenomeni che spesso accompagnano geometrie mal condizionate o curvature accentuate. Nella pratica interpretativa, si guarda *a entrambe*: una buona miscela nel bulk senza un’adeguata esplorazione delle code può indurre a sottostimare l’incertezza, mentre una buona copertura delle code con un bulk carente suggerisce lentezze diffuse e stime instabili anche in prossimità dei valori più plausibili. In condizioni regolari, ci si attende $\\widehat{R}$ prossimo a 1 per tutti i parametri, ESS bulk e ESS tail di ordine almeno qualche centinaio con 4000 draw totali, e l’assenza di divergenze nelle diagnostiche di `cmdstanr`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum_c\n#> # A tibble: 1 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 mu       -0.858 -0.858 0.445 0.439 -1.587 -0.148 1.002 1222.890 1215.074\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum_nc\n#> # A tibble: 2 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 mu       -0.864 -0.868 0.439 0.442 -1.569 -0.135 1.005 1341.017 1224.185\n#> 2 z        -0.043 -0.043 0.022 0.022 -0.078 -0.007 1.005 1341.018 1224.185\n```\n:::\n\n\nUn altro punto di osservazione utile riguarda la predizione. Le repliche posteriori, pur generate dalla stessa struttura del likelihood, possono risentire indirettamente dell’efficienza campionaria: quando la catena si muove con difficoltà, la variabilità predittiva ricostruita dai campioni è più rumorosa e meno stabile da una replica all’altra; quando invece la miscela è buona, gli stessi indici predittivi tendono a stabilizzarsi più rapidamente.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Estrazione delle repliche predittive\nyrep_c  <- fit_c$draws(\"y_rep\",  format = \"matrix\")\nyrep_nc <- fit_nc$draws(\"y_rep\", format = \"matrix\")\n\n# Un rapido overlay delle densità\np1 <- bayesplot::ppc_dens_overlay(y, yrep_c[1:100, , drop = FALSE]) +\n  ggplot2::labs(title = \"PPC — CP\")\np2 <- bayesplot::ppc_dens_overlay(y, yrep_nc[1:100, , drop = FALSE]) +\n  ggplot2::labs(title = \"PPC — NCP\")\n\np1\n```\n\n::: {.cell-output-display}\n![](12_stan_parametrization_files/figure-html/ppc-1.png){fig-align='center' width=85%}\n:::\n\n```{.r .cell-code}\np2\n```\n\n::: {.cell-output-display}\n![](12_stan_parametrization_files/figure-html/ppc-2.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn sintesi, a parità di specifica modellistica, la differenza che interessa è esclusivamente computazionale. Con pochi dati e prior larghi, la formulazione non centrata tende ad aumentare l’ESS, a ridurre l’autocorrelazione e a produrre catene più regolari. Se aumentiamo l’informatività dei dati, ampliando la numerosità campionaria o riducendo la varianza del likelihood, la situazione si riequilibra e la parametrizzazione centrata risulta spesso del tutto adeguata, talvolta persino leggermente più rapida da campionare per via della sua immediatezza geometrica. \n\nNel caso presente, di un modello estremamente semplice, i due approcci producono gli stessi risultati, anche con un campione estremamente ridotto. Nel caso di modelli più complessi, invece, la parametrizzazione non centrata può essere utile.\n\n## Riflessoni conclusive\n\nLa distinzione tra parametrizzazione centrata e non centrata non riguarda soltanto un dettaglio tecnico del codice Stan, ma tocca un aspetto fondamentale della modellazione gerarchica: il modo in cui rappresentiamo la relazione tra parametri a livello superiore e variabili latenti a livello inferiore. I due approcci sono matematicamente equivalenti, ma generano geometrie molto diverse nello spazio dei parametri, con conseguenze pratiche notevoli per l’efficienza del campionamento.\n\nLa parametrizzazione centrata è la più intuitiva. Essa esprime i parametri latenti come realizzazioni dirette della distribuzione a priori e funziona particolarmente bene quando i dati sono molto informativi. In questi casi la posterior è ben concentrata, il campionatore si muove agevolmente nello spazio parametrico e la semplicità della formulazione costituisce un vantaggio. Tuttavia, quando i dati sono scarsi o rumorosi, la dipendenza stretta tra i livelli gerarchici può rendere l’inferenza difficile: le catene diventano lente, autocorrelate e richiedono molte iterazioni per produrre stime affidabili.\n\nLa parametrizzazione non centrata affronta proprio questo problema. Introdurre una variabile standardizzata e ricostruire i parametri a partire da essa riduce la correlazione tra livelli, semplificando la geometria che il campionatore deve esplorare. In contesti a bassa informatività, ciò si traduce in un guadagno sostanziale di efficienza, con catene più stabili e un numero effettivo di campioni più elevato. È per questo motivo che, nella pratica quotidiana, i modelli non centrati sono spesso la scelta più sicura, soprattutto quando si lavora con strutture complesse o con dati limitati.\n\nÈ importante sottolineare che non esiste una regola assoluta: in scenari molto informativi, la parametrizzazione centrata può risultare persino più rapida, proprio grazie alla sua immediatezza. Per questo motivo, nelle applicazioni reali conviene spesso provare entrambe le formulazioni o ricorrere a strategie miste, note come “parametrizzazioni parzialmente centrate”, che combinano i vantaggi dei due approcci adattandosi alle caratteristiche dei dati.\n\nIl caso della media normale con varianza nota ha rappresentato un laboratorio ideale per illustrare i meccanismi di base. Nei capitoli successivi, la stessa logica tornerà in modelli più articolati — ad esempio nella regressione gerarchica e nei modelli a effetti misti — dove la scelta della parametrizzazione diventa ancora più cruciale. Ciò che qui abbiamo osservato in forma elementare si generalizza infatti a ogni contesto in cui esistono livelli multipli di variabilità e legami gerarchici tra parametri.\n\nIn sintesi, la distinzione tra parametrizzazioni centered e non-centered non riguarda la sostanza del modello, ma la sua implementazione computazionale. Comprendere questa differenza è cruciale per usare Stan in modo efficace, soprattutto nei modelli gerarchici psicologici, dove i dati individuali sono spesso scarsi e i prior giocano un ruolo importante.\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] forcats_1.0.0         cmdstanr_0.9.0        pillar_1.11.0        \n#>  [4] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#>  [7] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#> [10] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#> [13] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#> [16] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#> [19] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#> [22] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#> [25] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        reshape2_1.4.4       \n#> [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        \n#> [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        \n#> [16] backports_1.5.0       labeling_0.4.3        utf8_1.2.6           \n#> [19] rmarkdown_2.29        ps_1.9.1              ragg_1.5.0           \n#> [22] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#> [25] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#> [28] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#> [31] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#> [34] zoo_1.8-14            pacman_0.5.1          Matrix_1.7-4         \n#> [37] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#> [40] abind_1.4-8           yaml_2.3.10           codetools_0.2-20     \n#> [43] curl_7.0.0            processx_3.8.6        pkgbuild_1.4.8       \n#> [46] plyr_1.8.9            lattice_0.22-7        withr_3.0.2          \n#> [49] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [52] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [55] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [58] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [61] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [64] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#> [67] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#> [70] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#> [73] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#> [76] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#> [79] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#> [82] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#> [85] MASS_7.3-65\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n\n",
    "supporting": [
      "12_stan_parametrization_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}