{
  "hash": "2d9968e8f16388eca66004a5308b71fe",
  "result": {
    "engine": "knitr",
    "markdown": "# Introduzione pratica a Stan {#sec-mcmc-stan-intro}\n\n## Introduzione {.unnumbered .unlisted}\n\nNei capitoli precedenti abbiamo visto come l’algoritmo di Metropolis fornisca una soluzione generale al problema dell’inferenza bayesiana e come i linguaggi di programmazione probabilistica abbiano reso questa soluzione praticabile nella ricerca quotidiana. Ora è il momento di incontrare lo strumento che utilizzeremo concretamente nel nostro percorso: *Stan*.\n\nStan non è semplicemente un altro software statistico. È un linguaggio di programmazione probabilistica progettato per esprimere modelli bayesiani in modo chiaro e flessibile e per eseguire l’inferenza con algoritmi di campionamento allo stato dell’arte. In particolare, Stan utilizza varianti avanzate dell’Hamiltonian Monte Carlo (HMC), come l’algoritmo NUTS, che offrono efficienza e affidabilità molto superiori rispetto al semplice Metropolis.\n\nDal punto di vista concettuale, però, nulla cambia: la logica rimane la stessa che abbiamo già compreso. Stan non fa “magia”, ma implementa con grande efficacia ciò che Metropolis aveva già reso possibile. Per questo è importante vederlo come la *naturale evoluzione pratica* del percorso che abbiamo seguito fin qui.\n\nPer la ricerca psicologica, Stan ha un vantaggio particolare. Molti dei modelli che ci interessano – modelli gerarchici, modelli di apprendimento, modelli dinamici – richiedono più parametri e strutture complesse. Implementarli a mano sarebbe quasi impossibile. Con Stan, invece, possiamo concentrarci sul modello teorico e tradurlo in codice relativamente semplice, lasciando al software la gestione dei dettagli computazionali.\n\nNei prossimi capitoli vedremo come iniziare a scrivere modelli in Stan, partendo da esempi elementari per arrivare a strutture più articolate. L’obiettivo non è soltanto imparare un nuovo linguaggio, ma acquisire la capacità di formalizzare i modelli psicologici come processi generativi, traducendoli in analisi statistiche riproducibili e trasparenti.\n\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- I blocchi del codice Stan.  \n- Scrivere e stimare modelli semplici con `cmdstanr`.  \n- Interpretare i risultati MCMC tramite riassunti e diagnostiche.  \n- Valutare la coerenza del modello con prior e posterior predictive check.  \n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(cmdstanr, posterior, insight)\n```\n:::\n\n:::\n\n\n## Programmazione probabilistica con Stan\n\nOgni programma Stan è organizzato in blocchi distinti, che corrispondono a funzioni precise [@nicenboim2025introduction]. Nel blocco `data` dichiariamo le variabili osservate che passiamo dall’esterno; in `parameters` indichiamo le quantità ignote che vogliamo stimare; nel blocco `model` specifichiamo le assunzioni probabilistiche — cioè le distribuzioni a priori e la verosimiglianza; infine, in `generated quantities` possiamo calcolare misure derivate, come predizioni o log-likelihood, che non influiscono sulla stima ma sono utili per l’analisi successiva. Questa architettura modulare rende Stan intuitivo e adattabile a un'ampia gamma di applicazioni statistiche.\n\n\n### Lavorare con Stan in R\n\nL'interfaccia `cmdstanr` per R segue un workflow ben definito:\n\n1. scrittura del modello in un file `.stan`,  \n2. compilazione del modello,  \n3. passaggio dei dati come lista R , \n4. esecuzione del campionamento con `sample()`,  \n5. analisi dei risultati mediante pacchetti specializzati (`posterior`, `bayesplot`).  \n\nPossiamo pensare a un programma Stan come a una “ricetta”. Nel blocco `data` mettiamo gli ingredienti che già conosciamo (i dati osservati), in `parameters` dichiariamo gli ingredienti mancanti (i parametri da stimare), in `model` scriviamo le regole della preparazione (le distribuzioni a priori e la verosimiglianza) e in `generated quantities` prepariamo i contorni (diagnostiche, predizioni) che non cambiano la ricetta principale, ma la rendono più completa.\n\nIn pratica, lavorare con Stan da R segue sempre lo stesso schema. Prima si scrive il modello in un file `.stan`; poi lo si compila, cioè lo si traduce in un eseguibile; quindi si preparano i dati in una lista R con gli stessi nomi dichiarati nel modello; infine si lancia il campionamento con la funzione `sample()`. Una volta ottenuti i campioni a posteriori, possiamo analizzarli con pacchetti come `posterior` o `bayesplot`, che facilitano sia i riassunti numerici sia le visualizzazioni.\n\nStan utilizza un sistema di tipizzazione statica: tutte le variabili devono essere dichiarate con tipi specifici (`int` per interi, `real` per valori reali, `vector` per vettori) e possono includere vincoli (es. `lower=0` per valori positivi). Questo approccio aumenta la robustezza del codice e previene errori comuni nella specificazione dei modelli.\n\n\n## Modello Beta–Binomiale\n\nCome primo esempio costruiamo un modello molto semplice, ma già sufficiente per illustrare i principi fondamentali della programmazione in Stan. L’obiettivo è stimare la probabilità di successo $\\theta$ in una sequenza di prove Bernoulliane indipendenti.\n\nPer rendere l’idea concreta, immaginiamo di lanciare un dado mille volte e di registrare il risultato come variabile dicotomica: assegniamo il valore `1` se esce il numero 6 (considerato “successo”), e `0` in tutti gli altri casi (“fallimento”). Se il dado fosse perfettamente equilibrato, la probabilità di successo sarebbe $\\theta = 1/6 \\approx 0.167$. Tuttavia, nell’approccio bayesiano non assumiamo a priori che il dado sia equo: lasciamo che siano i dati, in combinazione con una distribuzione *a priori* esplicita, a informare il valore di $\\theta$.\n\nEcco un esempio di generazione dei dati in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 1000\ndice_df <- tibble(res = sample(1:6, size = n, replace = TRUE))\ny <- as.integer(dice_df$res == 6)    # 1 se esce “6”, 0 altrimenti\nmean(y)                              # frequenza relativa di “6”\n#> [1] 0.164\n```\n:::\n\n\nIl modello statistico che sottende a questa situazione è molto semplice:\n\n* ogni osservazione segue una distribuzione Bernoulliana, $y_i \\sim \\text{Bernoulli}(\\theta)$ per $i = 1, \\dots, N$;\n* equivalendo a dire che il numero totale di successi $k = \\sum_i y_i$ segue una distribuzione Binomiale, $k \\sim \\text{Binomiale}(N,\\theta)$;\n* come prior adottiamo una distribuzione uniforme su $[0,1]$, cioè $\\theta \\sim \\text{Beta}(1,1)$.\n\nVale la pena notare che, se in Stan dichiariamo un parametro vincolato all’intervallo $[0,1]$ ma non specifichiamo alcun prior, il linguaggio assegna automaticamente proprio questa distribuzione uniforme, che corrisponde a una Beta(1,1).\n\n\n### Prima versione: modello Bernoulliano vettoriale\n\nIl modo più diretto di tradurre il modello in Stan è scrivere la verosimiglianza come sequenza di esiti Bernoulliani. Questo approccio ha anche un valore didattico, perché mostra chiaramente la corrispondenza tra dati osservati e modello probabilistico. Stan, inoltre, vettorializza automaticamente le operazioni sugli array, rendendo il codice conciso:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstancode <- \"\ndata {\n  int<lower=1> N;                    // numero di prove\n  array[N] int<lower=0, upper=1> y;  // esiti (0/1)\n}\nparameters {\n  real<lower=0, upper=1> theta;      // probabilità di successo\n}\nmodel {\n  theta ~ beta(1, 1);                // prior uniforme su [0,1]\n  y ~ bernoulli(theta);              // verosimiglianza\n}\n\"\n```\n:::\n\n\nIl funzionamento dei blocchi è intuitivo: nel blocco `data` dichiariamo le variabili osservate (`N` e il vettore binario `y`); nel blocco `parameters` indichiamo il parametro da stimare, $\\theta$; infine, nel blocco `model` specifichiamo sia la distribuzione a priori sia la verosimiglianza.\n\nDal punto di vista interno, ogni riga del tipo `x ~ distribuzione(...)` aggiunge la log-densità (o log-massa) alla quantità interna `target`, che rappresenta la log-posterior. Se vogliamo essere più espliciti, possiamo scrivere il codice in forma equivalente:\n\n```stan\ntarget += beta_lpdf(theta | 1, 1);\ntarget += bernoulli_lpmf(y | theta);\n```\n\nQuesta seconda scrittura, sebbene meno compatta, è particolarmente utile quando vogliamo costruire verosimiglianze personalizzate o introdurre modifiche non standard.\n\n\n### Seconda versione: modello binomiale sui successi totali\n\nL’approccio Bernoulliano visto in precedenza ha il pregio della chiarezza, ma può risultare ridondante: stiamo in realtà scrivendo la stessa formula mille volte, una per ciascun lancio del dado. In termini statistici, però, sappiamo che non è necessario conservare l’intera sequenza di zeri e uno: ai fini della stima di $\\theta$ conta solo il numero totale di successi osservati. Questa proprietà prende il nome di *sufficienza* della statistica $k = \\sum_i y_i$ per la distribuzione binomiale.\n\nSe dunque nei mille lanci abbiamo osservato, ad esempio, 170 “6”, tutta l’informazione rilevante per stimare $\\theta$ è contenuta in quel singolo numero, non nella sequenza dettagliata dei lanci. La distribuzione binomiale ci permette di formalizzare questa idea in modo compatto, portando a una specificazione del modello più efficiente, ma del tutto equivalente sul piano inferenziale.\n\nEcco la traduzione in Stan:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstancode <- \"\ndata {\n  int<lower=1> N;                      // numero di prove\n  array[N] int<lower=0, upper=1> y;    // esiti (0/1)\n}\ntransformed data {\n  int<lower=0, upper=N> k = sum(y);    // numero totale di successi\n}\nparameters {\n  real<lower=0, upper=1> theta;        // probabilità di successo\n}\nmodel {\n  theta ~ beta(1, 1);                  // prior uniforme\n  k ~ binomial(N, theta);              // verosimiglianza sui successi totali\n}\n\"\n```\n:::\n\n\nQuesta versione concentra la verosimiglianza in un’unica riga, calcolata direttamente sul numero complessivo di successi. Le catene MCMC che otteniamo da questo modello coincidono, entro il rumore Monte Carlo, con quelle prodotte dalla versione Bernoulliana, ma richiedono meno operazioni e risultano quindi più efficienti dal punto di vista computazionale.\n\nCome per la prima versione, possiamo arricchire il modello con un blocco `generated quantities` per ottenere log-likelihood o repliche predittive. Queste quantità derivate non modificano l’inferenza su $\\theta$, ma ci consentono di eseguire controlli diagnostici, confrontare modelli alternativi o visualizzare come il modello riproduce i dati osservati.\n\n\n::: {.callout-note collapse=\"true\"}\n## Perché “Beta–Binomiale”?\n\nRicordiamo che, con prior $\\theta \\sim \\text{Beta}(a,b)$ e $k \\sim \\text{Binomiale}(N,\\theta)$, la posterior è\n\n$$\n\\theta \\mid y \\sim \\text{Beta}\\big(a+k,\\; b+N-k\\big), \n$$\n\\noindent\ncon $a=b=1$: $\\text{Beta}(1+k,\\; 1+N-k)$.\n\nQuesto ci consente un *controllo didattico*: possiamo confrontare media e IC ottenuti da Stan con quelli della Beta a posteriori.\n\nEsempio in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- sum(y)\na <- 1\nb <- 1\npost_mean_closed <- (a + k) / (a + b + n)\npost_ci95_closed  <- qbeta(c(0.025, 0.975), a + k, b + n - k)\npost_mean_closed; \n#> [1] 0.165\npost_ci95_closed\n#> [1] 0.142 0.188\n```\n:::\n\n\nLe stime via Stan (campioni MCMC della `theta`) devono coincidere (entro il rumore Monte Carlo) con queste quantità analitiche.\n:::\n\n\nUna volta scritto il modello, il passo successivo è la *compilazione*. Stan traduce il codice in linguaggio C++ e lo trasforma in un piccolo eseguibile che potrà essere richiamato ogni volta che lanceremo le stime. Questo passaggio richiede qualche secondo solo la prima volta; in seguito, il modello compilato può essere riutilizzato con dataset diversi senza dover ricompilare da capo, con un notevole risparmio di tempo.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstanmod <- cmdstanr::cmdstan_model(\n  write_stan_file(stancode),\n  compile = TRUE\n)\n```\n:::\n\n\nPreparato l’eseguibile, dobbiamo passare a Stan i dati necessari. In questo caso servono due elementi: il numero totale di prove e il vettore con gli esiti dei lanci. È importante che i nomi e i tipi corrispondano esattamente a quanto dichiarato nel blocco `data` del modello Stan, altrimenti il programma restituirà un errore.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_list <- list(\n  N = length(y),\n  y = y\n)\nstr(data_list)\n#> List of 2\n#>  $ N: int 1000\n#>  $ y: int [1:1000] 0 1 0 0 0 1 0 0 0 1 ...\n```\n:::\n\n\nA questo punto siamo pronti per il *campionamento MCMC*. Nella chiamata a `sample()` specifichiamo quante iterazioni dedicare alla fase di warmup (che serve per adattare l’algoritmo), quante iterazioni utilizzare effettivamente per l’inferenza, e quante catene indipendenti far partire in parallelo. Due parametri aggiuntivi – `adapt_delta` e `max_treedepth` – aiutano a rendere più stabili e accurati i passi dell’algoritmo NUTS, soprattutto in modelli più complessi.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit1 <- stanmod$sample(\n  data = data_list,\n  iter_warmup = 1000,\n  iter_sampling = 4000,\n  chains = 4,\n  parallel_chains = 4,\n  seed = 4790,\n  refresh = 0,                 # meno output a schermo\n  adapt_delta = 0.9,           # maggiore prudenza nel passo HMC\n  max_treedepth = 12           # profondità massima dell’albero NUTS\n)\n```\n:::\n\n\nCon il modello stimato, possiamo guardare un *riepilogo sintetico* delle stime. La funzione `summary()` di Stan mostra media, deviazione standard, quantili e diagnostiche come $\\hat R$ ed ESS.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(fit1$summary(variables = \"theta\"), n = Inf)\n#> # A tibble: 1 × 10\n#>   variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n#>   <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n#> 1 theta    0.165  0.165 0.012 0.012 0.146 0.185 1.001 5070.123 4701.800\n```\n:::\n\n\nIn alternativa, il pacchetto *posterior* permette di estrarre i campioni e calcolare con maggiore flessibilità le statistiche che ci interessano:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraws <- fit1$draws(variables = \"theta\", format = \"draws_matrix\")\nposterior::summarise_draws(\n  draws,\n  mean, sd, ~quantile(.x, c(0.025, 0.5, 0.975)), rhat, ess_bulk, ess_tail\n)\n#> # A tibble: 1 × 9\n#>   variable  mean    sd `2.5%` `50%` `97.5%`  rhat ess_bulk ess_tail\n#>   <chr>    <dbl> <dbl>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n#> 1 theta    0.165 0.012  0.143 0.165   0.189 1.001 5070.123 4701.800\n```\n:::\n\n\nCome regola generale, valori di $\\hat R$ molto vicini a 1 (idealmente < 1.01) e un numero elevato di campioni effettivi (ESS) indicano che le catene hanno esplorato bene la distribuzione a posteriori.\n\nOltre alle statistiche numeriche, le *diagnostiche grafiche* aiutano a valutare a colpo d’occhio la qualità del campionamento.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(fit1$draws(\"theta\"), n_warmup = 1000)\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens_overlay(fit1$draws(\"theta\"))\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nNel traceplot, le catene dovrebbero mescolarsi bene senza mostrare trend persistenti; nel grafico delle densità, le distribuzioni delle diverse catene dovrebbero risultare sovrapposte.\n\nIn sintesi: la media a posteriori di $\\theta$ fornisce una stima puntuale della probabilità di ottenere un “6”, l’intervallo di credibilità quantifica l’incertezza residua, e le diagnostiche (R-hat vicino a 1, ESS alto, catene ben mescolate) garantiscono che i campioni siano affidabili per trarre conclusioni.\n\n\n::: callout-note\n**Confronto con la frequenza relativa**\n\nLa frequenza `mean(y)` è *solo una stima puntuale*. L’analisi bayesiana restituisce *un’intera distribuzione* per $\\theta$, utile per *propagare l’incertezza* in previsioni e decisioni.\n:::\n\n\n### Distribuzione predittiva posteriore\n\nStimare $\\theta$ non basta: spesso ciò che ci interessa davvero è capire quali conseguenze pratiche derivano dai risultati. In altre parole, vogliamo sapere cosa il modello ci dice su dati futuri. Per esempio: se rilanciassimo il dado altre mille volte, quanti “6” potremmo aspettarci?\n\nLa risposta si ottiene con la *distribuzione predittiva posteriore*. Condizionatamente a un valore di $\\theta$, il numero di successi nei nuovi lanci segue una distribuzione binomiale:\n\n$$\ny_{\\text{rep}} \\mid \\theta \\sim \\text{Binomiale}(n_{\\text{new}}, \\theta).\n$$\n\nMa noi non conosciamo $\\theta$ con certezza: ne abbiamo solo una distribuzione a posteriori. Per questo motivo, la distribuzione predittiva si ottiene integrando la probabilità condizionata $p(y_{\\text{rep}} \\mid \\theta)$ rispetto alla distribuzione a posteriori di $\\theta$:\n\n$$\np(y_{\\text{rep}} \\mid y) = \\int p(y_{\\text{rep}} \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta.\n$$\n\nDal punto di vista operativo, il procedimento è semplice: estraiamo valori di $\\theta$ dalla posterior e, per ciascuno di essi, simuliamo un nuovo conteggio $y_{\\text{rep}}$. L’insieme di queste simulazioni costituisce la distribuzione predittiva.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Numero di futuri lanci da simulare\nn_new <- 1000\n\n# Estrazione dei campioni di theta (vettore numerico)\ntheta_draws <- as.numeric(draws[, \"theta\"])\n\n# Simulazione predittiva\nyrep_count <- rbinom(n = length(theta_draws), size = n_new, prob = theta_draws)\n\n# Riassunti predittivi\nmean(yrep_count)                           # valore atteso di \"6\" su n_new lanci\n#> [1] 165\nquantile(yrep_count, c(0.025, 0.5, 0.975)) # intervallo predittivo 95%\n#>  2.5%   50% 97.5% \n#>   134   164   199\n```\n:::\n\n\nPer visualizzare i risultati, rappresentiamo la distribuzione dei conteggi simulati con un istogramma, segnando con una linea verticale tratteggiata il numero di successi realmente osservati.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntibble(count = yrep_count) |>\n  ggplot(aes(x = count)) +\n  geom_histogram(bins = 30, color = \"white\") +\n  geom_vline(xintercept = sum(y), linetype = \"dashed\") +\n  labs(\n    x = \"Numero di '6' osservati\",\n    y = \"Frequenza\"\n  )\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Come leggere il grafico.**\nL’istogramma mostra la variabilità attesa del numero di “6” su mille lanci futuri. La distribuzione è centrata intorno al valore atteso $n_{\\text{new}} \\cdot \\mathbb{E}[\\theta \\mid y]$, cioè il numero medio di successi secondo la stima a posteriori. Se il dado fosse perfettamente equilibrato ($\\theta = 1/6$), ci aspetteremmo circa 167 successi su 1000, con un intervallo predittivo di circa 150–185. Se i dati simulati si discostano molto da questo scenario, il modello ci sta suggerendo che il dado potrebbe non essere equo.\n\n> Come controllo aggiuntivo, possiamo sfruttare la formula chiusa della distribuzione Beta–Binomiale: con prior $\\text{Beta}(1,1)$ e $k$ successi osservati, la predittiva per $n_{\\text{new}}$ prove segue una distribuzione $\\text{Beta–Binomiale}(n_{\\text{new}}, 1+k, 1+N-k)$. I suoi quantili dovrebbero essere coerenti con quelli ottenuti tramite simulazione.\n\n\n### Mini check analitico\n\nUn ulteriore modo per verificare i risultati è confrontare le stime prodotte da Stan con quelle ottenute direttamente dalla forma analitica della distribuzione a posteriori. In questo caso, con prior uniforme e verosimiglianza binomiale, sappiamo che la distribuzione a posteriori di $\\theta$ è ancora una Beta. Possiamo quindi calcolare media e intervallo di credibilità in chiuso e confrontarli con i valori ricavati dal campionamento MCMC:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nk <- sum(y); N <- length(y)\na <- 1; b <- 1\n\n# Posterior di theta (soluzione analitica)\npost_mean_closed <- (a + k) / (a + b + N)\npost_ci95_closed <- qbeta(c(0.025, 0.975), a + k, b + N - k)\n\nc(post_mean_closed = post_mean_closed)\n#> post_mean_closed \n#>            0.165\npost_ci95_closed\n#> [1] 0.142 0.188\n```\n:::\n\n\nLe differenze rispetto ai risultati di Stan dovrebbero essere minime e spiegabili unicamente con il normale rumore Monte Carlo. Questo confronto fornisce quindi una garanzia ulteriore che il modello sia stato implementato e stimato correttamente.\n\n\n\n## Dati continui: stima della media con varianza nota\n\nPassiamo ora a un caso molto comune nelle scienze psicologiche: la stima della *media di popolazione* di una variabile continua. Pensiamo, ad esempio, ai punteggi ottenuti in un test di intelligenza (QI).\n\nPer semplicità ipotizziamo che la deviazione standard $\\sigma$ sia nota. È un’ipotesi forte, certo, ma ci permette di concentrare l’attenzione su un solo parametro incognito: la media $\\mu$. Questa assunzione semplifica il modello e rende più chiaro il passaggio dall’impostazione classica a quella bayesiana.\n\nMolti dati psicologici possono essere approssimati da una distribuzione normale: è il caso dei punteggi standardizzati come il QI, che hanno distribuzioni simili a una gaussiana in popolazione. Immaginiamo quindi di raccogliere i punteggi di $n=30$ persone, supponendo che la deviazione standard sia $\\sigma = 15$ e che la media reale sia 105.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nn     <- 30\nsigma <- 15\ny_cont <- rnorm(n, mean = 105, sd = sigma)\n\ntibble(y = y_cont) |>\n  ggplot(aes(x = y)) +\n  geom_histogram(bins = 15) +\n  labs(x = \"Punteggio\", y = \"Frequenza\")\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo scenario, il modello statistico si scrive così:\n\n* *Verosimiglianza*: $y_i \\sim \\mathcal{N}(\\mu, \\sigma)$, con $\\sigma$ noto.\n* *Prior su $\\mu$*: $\\mu \\sim \\mathcal{N}(\\mu_0, \\tau)$, con $\\mu_0 = 100$ come media attesa a priori e $\\tau = 30$ come deviazione standard del prior, scelta piuttosto ampia per riflettere un’incertezza elevata.\n\nDa notare che $\\mu$ può assumere qualsiasi valore reale: per questo il prior corretto è una distribuzione normale definita su tutto $\\mathbb{R}$.\n\nEcco la traduzione del modello in Stan:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstancode_norm <- \"\ndata {\n  int<lower=1> N;\n  vector[N] y;             // dati continui\n  real<lower=0> sigma;     // sd nota\n  real mu0;                // media del prior su mu\n  real<lower=0> tau;       // sd del prior su mu\n}\nparameters {\n  real mu;                 // media: parametro reale non vincolato\n}\nmodel {\n  mu ~ normal(mu0, tau);   // prior corretto su mu\n  y  ~ normal(mu, sigma);  // likelihood\n}\n\"\n```\n:::\n\n\nPrepariamo i dati in R in modo che siano coerenti con quanto richiesto dal blocco `data` del modello:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata_list2 <- list(\n  N     = length(y_cont),\n  y     = y_cont,\n  sigma = sigma,\n  mu0   = 100,\n  tau   = 30\n)\n```\n:::\n\n\nCompiliamo il modello:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstanmod2 <- cmdstanr::cmdstan_model(write_stan_file(stancode_norm), compile = TRUE)\n```\n:::\n\n\nE infine lanciamo il campionamento MCMC:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit2 <- stanmod2$sample(\n  data = data_list2,\n  iter_warmup     = 1000,\n  iter_sampling   = 4000,\n  chains          = 4,\n  parallel_chains = 4,\n  seed            = 4790,\n  refresh         = 0\n)\n```\n:::\n\n\nI risultati possono essere riepilogati in forma sintetica:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprint(fit2$summary(variables = \"mu\"), n = Inf)\n#> # A tibble: 1 × 10\n#>   variable    mean  median    sd   mad     q5     q95  rhat ess_bulk ess_tail\n#>   <chr>      <dbl>   <dbl> <dbl> <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n#> 1 mu       104.240 104.236 2.721 2.740 99.751 108.710 1.001 5499.842 7540.041\n```\n:::\n\n\nOppure, con il pacchetto *posterior*, possiamo estrarre i campioni e calcolare statistiche più flessibili, come medie, quantili e diagnostiche:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraws2 <- fit2$draws(variables = \"mu\", format = \"draws_matrix\")\nposterior::summarise_draws(\n  draws2, mean, sd, ~quantile(.x, c(0.025, 0.5, 0.975)), rhat, ess_bulk, ess_tail\n)\n#> # A tibble: 1 × 9\n#>   variable    mean    sd `2.5%`   `50%` `97.5%`  rhat ess_bulk ess_tail\n#>   <chr>      <dbl> <dbl>  <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n#> 1 mu       104.240 2.721 98.916 104.236 109.627 1.001 5499.842 7540.041\n```\n:::\n\n\nL’interpretazione è immediata:\n\n* valori di $\\hat R$ vicini a 1 e *effective sample size* elevato ci dicono che le catene sono ben miscelate e la stima è affidabile;\n* la *media a posteriori* di $\\mu$ rappresenta la nostra miglior stima puntuale della media di popolazione;\n* l’*intervallo di credibilità* (ad esempio al 95%) quantifica l’incertezza residua su $\\mu$.\n\nIn questo modo otteniamo non solo una stima centrale, ma un quadro completo della plausibilità dei valori possibili per la media di popolazione, dato quanto osservato.\n\n\n### Controllo analitico della coniugatezza\n\nIl modello che abbiamo specificato ha una proprietà molto comoda: la *coniugatezza*.\nQuando usiamo un prior normale per la media $\\mu$ e assumiamo nota la varianza $\\sigma^2$, anche la distribuzione *a posteriori* di $\\mu$ rimane normale. Questo ci permette di calcolare media e varianza della posterior in forma chiusa, senza bisogno di simulazioni MCMC.\n\nIn particolare, la varianza e la media della distribuzione a posteriori si ottengono come:\n\n$$\n\\text{Var}(\\mu \\mid y) \\;=\\; \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}, \n\\quad\n\\mathbb{E}[\\mu \\mid y] \\;=\\; \\text{Var}(\\mu \\mid y)\\,\\left(\\frac{n\\bar y}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}\\right).\n$$\n\nCon i nostri dati possiamo calcolare questi valori direttamente:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nybar    <- mean(y_cont)\ntau2    <- 30^2\nsigma2  <- sigma^2\npost_var  <- 1 / (n / sigma2 + 1 / tau2)\npost_sd   <- sqrt(post_var)\npost_mean <- post_var * (n * ybar / sigma2 + 100 / tau2)\n\nc(post_mean_closed = post_mean, post_sd_closed = post_sd)\n#> post_mean_closed   post_sd_closed \n#>           104.26             2.73\n```\n:::\n\n\nOra possiamo confrontare questi risultati analitici con quanto ottenuto tramite campionamento MCMC:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_draws <- as.numeric(draws2[, \"mu\"])\nc(mean_mcmc = mean(mu_draws), sd_mcmc = sd(mu_draws))\n#> mean_mcmc   sd_mcmc \n#>    104.24      2.72\n```\n:::\n\n\nE per completezza, guardiamo anche i quantili della distribuzione campionata:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nquantile(mu_draws, c(0.025, 0.5, 0.975))\n#>  2.5%   50% 97.5% \n#>  98.9 104.2 109.6\n```\n:::\n\n\nLe due soluzioni dovrebbero concordare molto bene: la formula chiusa ci dà il risultato “esatto”, mentre l’MCMC lo approssima tramite simulazione. Le piccole discrepanze sono attribuibili al normale *rumore Monte Carlo*, che diminuisce all’aumentare del numero di campioni.\n\n\n### Visualizzazione\n\nUna volta stimato il modello, è molto utile osservare graficamente i campioni MCMC per verificare sia la qualità del campionamento sia la forma della distribuzione a posteriori. Il pacchetto `bayesplot` fornisce strumenti immediati per questo scopo.\n\nUn primo passo è guardare l’*istogramma dei campioni*:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_hist(fit2$draws(\"mu\"))\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-26-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nQuesto grafico mostra la distribuzione stimata della media $\\mu$: non un singolo numero, ma un ventaglio di valori plausibili con le loro frequenze relative.\n\nPossiamo poi controllare l’andamento delle catene con un *traceplot*:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_trace(fit2$draws(\"mu\"), n_warmup = 1000)\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nQui l’idea è semplice: le linee delle catene devono sembrare “ben mescolate”, senza trend evidenti o zone piatte. È un indicatore visivo della corretta esplorazione dello spazio dei parametri.\n\nInfine, è utile sovrapporre le distribuzioni stimate dalle diverse catene per verificarne la concordanza:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbayesplot::mcmc_dens_overlay(fit2$draws(\"mu\"))\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nSe le curve si sovrappongono bene, abbiamo un’ulteriore conferma che le catene hanno raggiunto la stessa distribuzione stazionaria.\n\n\nIn sintesi, questo esempio rappresenta una sorta di *“test bayesiano della media”* con deviazione standard nota e un prior normale ampio su $\\mu$. A differenza dell’approccio frequentista, non otteniamo soltanto una stima puntuale o un intervallo, ma un’intera *distribuzione a posteriori* della media. Questo è un vantaggio importante: l’incertezza stimata può essere comunicata in modo trasparente e, soprattutto, può essere *propagata* nelle fasi successive dell’analisi, ad esempio in previsioni o decisioni basate sul modello.\n\n> Una nota pratica su Stan: se omettessimo la prior su $\\mu$, il software assumerebbe implicitamente una prior impropria piatta su $\\mathbb{R}$. Sebbene ciò possa funzionare in casi semplici, didatticamente preferiamo specificare in modo esplicito un prior normale (anche molto ampio). In questo modo le assunzioni sono sempre chiare e il modello rimane ben definito.\n\n\n### Intervalli di credibilità\n\nIn un’analisi bayesiana non otteniamo una singola stima del parametro, ma una distribuzione a posteriori che descrive tutta l’incertezza residua. Gli *intervalli di credibilità* servono a riassumere questa distribuzione in modo intuitivo:\n\n> *Dato il modello e i dati osservati, c’è una probabilità prefissata (ad esempio 94%) che il parametro cada all’interno dell’intervallo.*\n\nÈ un’interpretazione semplice e diretta, molto più naturale rispetto a quella degli intervalli di confidenza frequentisti.\n\n\n#### Due definizioni principali\n\n* **ETI (Equal-Tailed Interval)**: l’intervallo “a code uguali”.\n  Lascia la stessa probabilità nelle due code della distribuzione (es. 3% a sinistra e 3% a destra in un intervallo al 94%).\n\n  * Vantaggio: è *invariante* a trasformazioni monotone (se trasformo il parametro, trasformo anche i quantili).\n  * Svantaggio: se la posterior è molto asimmetrica, l’intervallo può risultare poco compatto.\n\n* **HDI (Highest Density Interval)**: l’intervallo “a massima densità”.\n  È il più stretto possibile che contiene la probabilità fissata.\n\n  * Vantaggio: rimane compatto anche con distribuzioni asimmetriche.\n  * Svantaggio: non è invariante a trasformazioni monotone; in presenza di distribuzioni multimodali può perfino risultare “spezzato” in più sotto-intervalli.\n\nSe la distribuzione a posteriori è simmetrica e unimodale (per esempio una Normale), *ETI e HDI coincidono*.\n\n\n#### Esempio con i campioni di $\\mu$\n\nPartiamo dai campioni MCMC della media $\\mu$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_draws <- as.numeric(fit2$draws(\"mu\"))\n```\n:::\n\n\nCalcoliamo sia ETI che HDI al 94% con il pacchetto **bayestestR**:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\neti94 <- bayestestR::eti(mu_draws, ci = 0.94)\nhdi94 <- bayestestR::hdi(mu_draws, ci = 0.94)\n\neti94\n#> 94% ETI: [99.12, 109.43]\nhdi94\n#> 94% HDI: [99.22, 109.48]\n```\n:::\n\n\n\n#### Visualizzazione\n\nCon `bayesplot::mcmc_areas()` possiamo visualizzare gli intervalli centrali (ETI) e aggiungere i limiti HDI come linee verticali:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- bayesplot::mcmc_areas(fit2$draws(\"mu\"), prob = 0.94) +\n  xlab(expression(mu)) + ylab(\"Densità\")\n\np + \n  geom_vline(xintercept = hdi94$CI_low,  linetype = \"dashed\") +\n  geom_vline(xintercept = hdi94$CI_high, linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n#### Posterior asimmetrica: esempio Beta\n\nPer apprezzare meglio la differenza, consideriamo una distribuzione asimmetrica, come una Beta(6,2):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\ntheta_draws <- rbeta(5000, shape1 = 6, shape2 = 2)\n\neti_beta <- bayestestR::eti(theta_draws, ci = 0.94)\nhdi_beta <- bayestestR::hdi(theta_draws, ci = 0.94)\n\neti_beta\n#> 94% ETI: [0.43, 0.96]\nhdi_beta\n#> 94% HDI: [0.49, 0.98]\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data.frame(theta = theta_draws), aes(x = theta)) +\n  geom_density() +\n  geom_vline(xintercept = c(eti_beta$CI_low, eti_beta$CI_high), linewidth = 0.7) +\n  geom_vline(xintercept = c(hdi_beta$CI_low, hdi_beta$CI_high), \n             linewidth = 1.2, linetype = \"dashed\") +\n  labs(x = expression(theta), y = \"Densità\")\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-33-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nQui si vede bene che l’*HDI è più corto*, perché concentra l’intervallo nelle zone di massima densità, evitando code poco informative.\n\n\n#### Quale livello usare? 89%, 94% o 95%?\n\n* **95%**: è la scelta più familiare (ereditata dal frequentismo).\n* **94%**: molto usata in manuali bayesiani (es. *McElreath, 2018*).\n* **89%**: proposta da Kruschke come compromesso più stabile con campioni limitati.\n\nLa regola pratica è semplice: scegli un livello *coerente con il resto dell’analisi* e specifica sempre il metodo (ETI o HDI). Se la posterior è asimmetrica, riportare entrambi può essere molto utile.\n\n\n#### Riassunto operativo\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Campioni da Stan\nmu_draws <- as.numeric(fit2$draws(\"mu\"))\n\n# Intervalli a 94%\neti94 <- bayestestR::eti(mu_draws, ci = 0.94)\nhdi94 <- bayestestR::hdi(mu_draws, ci = 0.94)\n\n# Riassunti\nlist(\n  mean   = mean(mu_draws),\n  median = median(mu_draws),\n  ETI94  = c(low = eti94$CI_low, high = eti94$CI_high),\n  HDI94  = c(low = hdi94$CI_low, high = hdi94$CI_high)\n)\n#> $mean\n#> [1] 104\n#> \n#> $median\n#> [1] 104\n#> \n#> $ETI94\n#>   low  high \n#>  99.1 109.4 \n#> \n#> $HDI94\n#>   low  high \n#>  99.2 109.5\n```\n:::\n\n\n\nIn sintesi:\n\n* con distribuzioni simmetriche ETI e HDI sono equivalenti,\n* con distribuzioni asimmetriche l’HDI è di solito più informativo perché si concentra nelle regioni più probabili,\n* riportare entrambi può aiutare a comunicare chiaramente le assunzioni e i risultati.\n\nUn esempio di frase per un report potrebbe essere:\n\n> “Con un intervallo di credibilità al 94%, possiamo dire che la media del QI nella popolazione ha il 94% di probabilità di trovarsi tra 102 e 107.”\n\n\n## Test di ipotesi bayesiane\n\nL’approccio bayesiano permette di formulare domande *dirette* del tipo:\n\n> “Qual è la probabilità che la media superi una certa soglia?”\n\nA differenza dei test frequentisti, non ci costringe a una risposta secca sì/no, ma restituisce una *misura graduata di plausibilità*. Inoltre, se abbiamo un margine di tolleranza *pratico* attorno alla soglia, possiamo definire una *ROPE* (*Region Of Practical Equivalence*) e distinguere tre scenari: il parametro stimato è *verosimilmente sotto*, *dentro* oppure *sopra* la zona di equivalenza.\n\n\n### Probabilità a posteriori sopra una soglia\n\nRiprendiamo l’esempio del QI con modello Normale e $\\sigma$ noto. Una domanda semplice è:\n\n$$\nP(\\mu > 110 \\mid y),\n$$\n\ncioè la probabilità che la media della popolazione superi il valore 110.\n\n**Calcolo dai campioni MCMC:**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu_draws <- as.numeric(fit2$draws(\"mu\"))\np_gt_110 <- mean(mu_draws > 110)\np_gt_110\n#> [1] 0.0193\n```\n:::\n\n\n> Interpretazione: dato *modello + dati*, la probabilità che $\\mu$ superi 110 è `p_gt_110`.\n> Non un “sì/no”, ma una misura graduata della plausibilità dell’affermazione.\n\n**Controllo analitico (conjugate Normal-Normal):**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np_gt_110_closed <- 1 - pnorm(110, mean = post_mean, sd = post_sd)\nc(MCMC = p_gt_110, ClosedForm = p_gt_110_closed)\n#>       MCMC ClosedForm \n#>     0.0193     0.0176\n```\n:::\n\n\nI due valori devono concordare (differenze minime = rumore Monte Carlo).\n\n\n### Decisione pratica\n\nSe occorre *prendere una decisione*, possiamo introdurre una regola:\n\n* *agire come se* $\\mu>110$ se $P(\\mu>110\\mid y)\\ge p^*$ (es. $p^*=0.9$),\n* altrimenti, non agire (o raccogliere più dati).\n\nIl valore soglia $p^\\*$ dipende dal rapporto *costi/benefici* degli errori (falsa allerta vs. falsa rassicurazione). L’approccio bayesiano rende esplicita questa scelta.\n\n\n### ROPE: equivalenza pratica\n\nNella realtà spesso non interessa se $\\mu$ sia esattamente 110, ma se sia *praticamente equivalente* a 110 entro una tolleranza accettabile. Definiamo allora una ROPE:\n\n$$\n\\text{ROPE} = [108,\\,112] .\n$$\n\nLe tre probabilità mutuamente esclusive sono:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrope <- c(108, 112)\n\nP_below <- mean(mu_draws <  rope[1])\nP_in    <- mean(mu_draws >= rope[1] & mu_draws <= rope[2])\nP_above <- mean(mu_draws >  rope[2])\n\nc(P_below = P_below, P_in = P_in, P_above = P_above)\n#> P_below    P_in P_above \n#> 0.91463 0.08250 0.00287\n```\n:::\n\n\nCon la posterior Normale si può calcolare anche in forma chiusa:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nP_below_cf <- pnorm(rope[1], mean = post_mean, sd = post_sd)\nP_in_cf    <- pnorm(rope[2], mean = post_mean, sd = post_sd) -\n              pnorm(rope[1], mean = post_mean, sd = post_sd)\nP_above_cf <- 1 - pnorm(rope[2], mean = post_mean, sd = post_sd)\n\nc(P_below_cf = P_below_cf, P_in_cf = P_in_cf, P_above_cf = P_above_cf)\n#> P_below_cf    P_in_cf P_above_cf \n#>    0.91498    0.08275    0.00226\n```\n:::\n\n\n\n### Interpretazione\n\n* Se $P(\\mu\\in \\text{ROPE})$ è alta → $\\mu$ è *praticamente equivalente* alla soglia.\n* Se $P(\\mu>\\text{ROPE})$ è alta → $\\mu$ è *sopra la soglia* in modo rilevante.\n* Se $P(\\mu<\\text{ROPE})$ è alta → $\\mu$ è *sotto la soglia* in modo rilevante.\n* Se le tre probabilità sono simili → il messaggio è *incertezza* → utile raccogliere più dati o rivedere la tolleranza.\n\n\n### Esempio di frase per un report\n\n> Con ROPE = $\\[108,112]$, la probabilità che $\\mu$ sia sotto-ROPE è $P(\\mu<108)=0.92$, dentro-ROPE $P(108\\le \\mu\\le112)=0.08$, sopra-ROPE $P(\\mu>112)=0.003$.\n> Questi risultati indicano che $\\mu$ è verosimilmente **inferiore** a 110 nel senso *pratico* definito dalla ROPE.\n\n\n### Diagnostiche di campionamento\n\nDopo aver ottenuto i campioni dalla distribuzione a posteriori, il passo successivo è verificare se questi siano di buona qualità. Un modello ben specificato e un campionamento MCMC efficace producono catene che esplorano lo spazio dei parametri in modo completo e bilanciato. Per questo motivo, nel riepilogo fornito da Stan e dai pacchetti associati compaiono alcune statistiche fondamentali, che meritano di essere interpretate con attenzione (la quantità `lp__` è un parametro speciale usato per le diagnostiche).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit2$summary(variables = c(\"mu\",\"lp__\"))\n#> # A tibble: 2 × 10\n#>   variable    mean  median    sd   mad      q5     q95  rhat ess_bulk ess_tail\n#>   <chr>      <dbl>   <dbl> <dbl> <dbl>   <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n#> 1 mu       104.240 104.236 2.721 2.740  99.751 108.710 1.001 5499.842 7540.041\n#> 2 lp__     -14.463 -14.195 0.705 0.313 -15.896 -13.967 1.001 7356.664 9175.822\n```\n:::\n\n\nUn primo indicatore è la statistica di convergenza $\\hat{R}$ (o R-hat). Quando le catene sono indipendenti e hanno raggiunto lo stesso equilibrio, i loro valori oscillano intorno alle stesse regioni della distribuzione. In questo caso $\\hat{R}$ assume valori molto vicini a 1. Nel nostro esempio, per i parametri `mu` e `lp__` si ottengono rispettivamente 1.002 e 1.001. Questi valori sono praticamente indistinguibili da 1 e segnalano che le catene hanno raggiunto una buona mescolanza. In termini pratici, valori inferiori a 1.01 sono generalmente considerati ottimali, mentre valori superiori a 1.05 suggeriscono possibili problemi di convergenza.\n\nUn secondo insieme di statistiche riguarda la dimensione del campione effettivo (ESS, *effective sample size*). A differenza di un campionamento indipendente, l’MCMC produce campioni correlati tra loro. L’ESS quantifica quanti campioni indipendenti “equivalenti” abbiamo realmente a disposizione. Nel nostro modello, i valori per `mu` sono di 2768 (bulk) e 3798 (tail), mentre per `lp__` superano i 3600. Si tratta di valori molto elevati, che garantiscono stime stabili anche per i quantili delle code della distribuzione. Più l’ESS è alto, più le nostre stime risultano precise. A titolo di esempio, l’errore Monte Carlo sulla media di `mu`, calcolato come $\\text{sd}/\\sqrt{\\text{ESS}}$, risulta circa 0.05: un valore trascurabile rispetto alla deviazione standard della distribuzione a posteriori, pari a 2.68. Questo significa che l’incertezza introdotta dal metodo numerico è minima.\n\nOltre agli indici di convergenza e di efficienza, vale la pena soffermarsi sulla forma della distribuzione stimata. Nel riepilogo vediamo che la media e la mediana di `mu` (104.22 e 104.24) coincidono quasi perfettamente, segnalando una distribuzione sostanzialmente simmetrica. La deviazione standard (2.68) e la MAD (2.65) confermano che la variabilità è ben catturata e non emergono anomalie nelle code. I quantili dal 5° al 95° indicano un intervallo credibile al 90% compreso tra circa 99.8 e 108.6. È interessante osservare che il valore 110 si colloca al di sopra di questo intervallo, fatto che si traduce in una probabilità molto bassa che $\\mu$ superi quella soglia.\n\nInfine, il parametro `lp__`, che rappresenta la log-densità a posteriori, non va interpretato come un parametro di interesse ma come strumento diagnostico. Anch’esso mostra valori di $\\hat{R}$ e di ESS ottimi, confermando che l’esplorazione dello spazio dei parametri è avvenuta senza difficoltà.\n\nIn sintesi, tutte le evidenze diagnostiche — $\\hat{R}$ prossimo a 1, valori elevati di ESS, concordanza tra media, mediana e misure di dispersione — ci permettono di concludere che il campionamento MCMC è stato stabile ed efficiente. Questo garantisce che i risultati ottenuti rappresentano fedelmente la distribuzione a posteriori specificata dal nostro modello.\n\n\n## Prior e Posterior Predictive Check\n\nPrima guardiamo se il prior che abbiamo scelto genera valori plausibili anche senza dati: questo è il prior predictive check. Se il prior produce punteggi di QI completamente inverosimili, dobbiamo correggerlo. Poi, dopo aver aggiornato il modello con i dati, passiamo al posterior predictive check. In questo caso le simulazioni devono assomigliare ai dati reali: se non ci riescono, significa che il modello non descrive bene il fenomeno.\n\n\n### Prior Predictive Check \n\n*Obiettivo.* Prima di guardare i dati, vogliamo chiederci: le nostre assunzioni a priori su $\\mu$ sono plausibili? In altre parole, il prior scelto produce valori di $y$ che hanno senso rispetto al dominio del problema (qui: punteggi QI)?\n\nIl nostro modello è:\n\n$$\ny_i \\mid \\mu \\sim \\mathcal{N}(\\mu,\\; \\sigma), \n\\qquad\n\\mu \\sim \\mathcal{N}(\\mu_0,\\; \\tau).\n$$\n\nCombinando likelihood e prior, la *distribuzione predittiva a priori* di una singola osservazione è:\n\n$$\ny_i \\sim \\mathcal{N}\\!\\Big(\\mu_0,\\; \\sqrt{\\sigma^2 + \\tau^2}\\Big).\n$$\n\nQuesta distribuzione descrive quali valori ci aspettiamo *prima di osservare alcun dato*.\n\n* Se produce valori estremi o inverosimili (ad es. QI < 40 o > 160), il prior è troppo *largo* o *spostato*.\n* Se invece produce valori troppo concentrati in un intervallo ristretto, il prior è *eccessivamente informativo*, lasciando poco spazio ai dati.\n\nPer implementare un *prior predictive check* possiamo usare lo stesso file Stan dell’inferenza, con una piccola modifica:\n\n1. aggiungiamo una variabile booleana `compute_likelihood`, che ci permette di decidere se includere o meno la riga `y ~ normal(mu, sigma);`,\n2. generiamo repliche $y_{\\text{rep}}$ in un blocco `generated quantities`.\n\nEcco il codice Stan:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstancode_norm_ppc <- \"\ndata {\n  int<lower=0> N;\n  vector[N] y;                 // usato solo se compute_likelihood=1\n  real<lower=0> sigma;         // sd nota\n  real mu0;                    // media del prior su mu\n  real<lower=0> mu_prior_sd;   // sd del prior\n  int<lower=0, upper=1> compute_likelihood; // 1 = usa y ~ normal(..), 0 = disattiva\n}\nparameters {\n  real mu;\n}\nmodel {\n  mu ~ normal(mu0, mu_prior_sd);\n  if (compute_likelihood == 1) {\n    y ~ normal(mu, sigma);\n  }\n}\ngenerated quantities {\n  vector[N] y_rep;\n  vector[N] log_lik; \n\n  for (n in 1:N) {\n    y_rep[n] = normal_rng(mu, sigma); // repliche prior/posterior predictive\n    log_lik[n] = normal_lpdf(y[n] | mu, sigma); \n  }\n}\n\"\nstanmod_ppc <- cmdstan_model(write_stan_file(stancode_norm_ppc), compile = TRUE)\n```\n:::\n\n\nPer un controllo *puro* del prior disattiviamo la likelihood (`compute_likelihood = 0`). In questo modo, Stan genera valori $y_{\\text{rep}}$ esclusivamente a partire dalle assunzioni a priori.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nN_ppc <- length(y_cont)\n\nstan_data_prior <- list(\n  N = N_ppc,\n  y = rep(0, N_ppc),   # placeholder, non usato quando compute_likelihood = 0\n  sigma = sigma,\n  mu0 = 100,\n  mu_prior_sd = 30,\n  compute_likelihood = 0\n)\n\nfit_prior <- stanmod_ppc$sample(\n  data = stan_data_prior,\n  iter_warmup = 500,\n  iter_sampling = 2000,\n  chains = 4,\n  parallel_chains = 4,\n  seed = 4790,\n  refresh = 500\n)\n```\n:::\n\n\nDopo il campionamento, estraiamo le repliche $y_{\\text{rep}}$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Estrazione dei dati simulati\nyrep_mat_prior <- posterior::as_draws_matrix(fit_prior$draws(\"y_rep\"))\n\n# Selezione solo delle colonne y_rep[1],...,y_rep[N]\nN_ppc <- length(y_cont)\nyrep_mat_prior <- as.matrix(yrep_mat_prior[, paste0(\"y_rep[\", 1:N_ppc, \"]\")])\n```\n:::\n\n\nConfrontiamo i dati osservati con alcune repliche generate dal prior:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nidx <- sample(seq_len(nrow(yrep_mat_prior)), 100)\n\nbayesplot::ppc_dens_overlay(\n  y = y_cont,\n  yrep = yrep_mat_prior[idx, , drop = FALSE]\n) \n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-43-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nInterpretazione:\n\n* Se le distribuzioni simulate coprono bene la variabilità dei dati reali, il prior è plausibile.\n* Se le simulazioni sono sistematicamente troppo larghe o troppo strette, il prior va ripensato (riducendo o ampliando `mu_prior_sd`).\n\n\n### Posterior predictive check \n\nDopo aver verificato che il *prior* sia ragionevole, possiamo passare alla fase successiva: confrontare il modello *dopo aver visto i dati*.\n\nPer farlo, riattiviamo la verosimiglianza (`compute_likelihood = 1`) e stimiamo la distribuzione a posteriori di $\\mu$. Nel blocco `generated quantities`, Stan genera anche delle *repliche posterior predictive* $y_{\\text{rep}}$, cioè nuovi dataset simulati sotto l’ipotesi che il modello e i parametri stimati siano corretti.\n\nIn altre parole:\n\n* il *prior predictive check* serve a testare le assunzioni *prima* dei dati,\n* il *posterior predictive check* serve a valutare se il modello *dopo i dati* è in grado di riprodurre l’evidenza osservata.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstan_data_post <- list(\n  N = length(y_cont),\n  y = y_cont,\n  sigma = sigma,\n  mu0 = 100,\n  mu_prior_sd = 30,\n  compute_likelihood = 1\n)\n```\n:::\n\n\nLancio del campionamento:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_post <- stanmod_ppc$sample(\n  data = stan_data_post,\n  iter_warmup = 1000,\n  iter_sampling = 10000,\n  chains = 4,\n  parallel_chains = 4,\n  seed = 4790,\n  refresh = 1000\n)\n```\n:::\n\n\nEstrazione e confronto con i dati reali:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ny_rep <- fit_post$draws(\"y_rep\", format = \"matrix\")\nppc_dens_overlay(y = stan_data_post$y, yrep = y_rep[1:100, ])\n```\n\n::: {.cell-output-display}\n![](03_stan_intro_files/figure-html/unnamed-chunk-46-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\nInterpretazione:\n\n* se la distribuzione delle repliche $y_{\\text{rep}}$ *copre bene* la distribuzione osservata $y$, significa che il modello è in grado di spiegare i dati;\n* se invece ci sono *scostamenti sistematici* (ad es. le repliche hanno media troppo bassa, o varianza troppo alta rispetto ai dati reali), il modello non descrive adeguatamente il fenomeno e potrebbe essere rivisto.\n\n\n**Collegamento con la verifica dei prior.**\n\n* Un *prior troppo largo* può portare a simulazioni estreme e poco plausibili *prima dei dati*.\n* Un *prior troppo stretto* rischia di imporre eccessiva rigidità al modello, lasciando poca flessibilità ai dati.\n* Nel *posterior predictive check*, quello che conta è verificare che, dopo aver aggiornato il modello con i dati, le simulazioni riflettano in modo realistico il comportamento osservato.\n\n**Metafora.** È come provare una ricetta: prima assaggiamo l’impasto crudo per capire se gli ingredienti sono dosati bene; poi, una volta cotto, assaggiamo il piatto finito per verificare che il risultato sia quello che ci aspettavamo.\n\n\n### Nota didattica\n\nCon $\\sigma$ noto e $\\mu \\sim \\mathcal{N}(\\mu_0, \\tau)$, la distribuzione predittiva *a priori* della media campionaria $\\bar{y}$ è:\n\n$$\n\\bar{y} \\sim \\mathcal{N}\\!\\big(\\mu_0,\\; \\sqrt{\\tau^2 + \\tfrac{\\sigma^2}{N}}\\big).\n$$\n\nQuesta formula fornisce un controllo rapido per tarare il prior rispetto alla precisione attesa del campione. Dopo l’aggiornamento con i dati, la distribuzione *a posteriori* restringe l’incertezza, e le repliche posterior predictive permettono di verificarne la coerenza empirica.\n\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nStan rappresenta il punto di arrivo naturale del percorso che abbiamo seguito fin qui. Dopo aver compreso la logica generale dell’inferenza bayesiana con Metropolis e aver visto come i linguaggi probabilistici abbiano reso praticabile questa logica, Stan ci offre ora uno strumento concreto per applicare questi principi a modelli reali e complessi.\n\nIl suo valore non sta soltanto nella potenza computazionale, ma soprattutto nella possibilità di *spostare l’attenzione dal calcolo all’idea scientifica*. Con Stan possiamo tradurre ipotesi psicologiche in modelli formali, esplicitarne le assunzioni e ottenere inferenze riproducibili senza perdere di vista la sostanza teorica.\n\nIn questo senso, Stan non è solo un software: è un ambiente che incoraggia la trasparenza, la chiarezza e la cumulatività della ricerca. Nei prossimi capitoli vedremo come utilizzarlo a partire da esempi semplici, per poi affrontare modelli più ricchi. L’obiettivo è acquisire familiarità non solo con la sintassi, ma soprattutto con il modo di pensare che rende la modellazione bayesiana uno strumento essenziale per la psicologia scientifica contemporanea.\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] insight_1.4.2         cmdstanr_0.9.0        pillar_1.11.0        \n#>  [4] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#>  [7] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#> [10] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#> [13] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#> [16] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#> [19] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#> [22] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#> [25] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      ggridges_0.5.7        compiler_4.5.1       \n#> [10] reshape2_1.4.4        systemfonts_1.2.3     vctrs_0.6.5          \n#> [13] stringr_1.5.1         pkgconfig_2.0.3       arrayhelpers_1.1-0   \n#> [16] fastmap_1.2.0         backports_1.5.0       labeling_0.4.3       \n#> [19] utf8_1.2.6            rmarkdown_2.29        ps_1.9.1             \n#> [22] ragg_1.5.0            purrr_1.1.0           xfun_0.53            \n#> [25] cachem_1.1.0          jsonlite_2.0.0        broom_1.0.9          \n#> [28] parallel_4.5.1        R6_2.6.1              stringi_1.8.7        \n#> [31] RColorBrewer_1.1-3    lubridate_1.9.4       estimability_1.5.1   \n#> [34] knitr_1.50            zoo_1.8-14            pacman_0.5.1         \n#> [37] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#> [40] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#> [43] codetools_0.2-20      curl_7.0.0            processx_3.8.6       \n#> [46] pkgbuild_1.4.8        plyr_1.8.9            lattice_0.22-7       \n#> [49] bayestestR_0.17.0     withr_3.0.2           bridgesampling_1.1-2 \n#> [52] coda_0.19-4.1         evaluate_1.0.5        survival_3.8-3       \n#> [55] RcppParallel_5.1.11-1 tensorA_0.36.2.1      checkmate_2.3.3      \n#> [58] stats4_4.5.1          distributional_0.5.0  generics_0.1.4       \n#> [61] rprojroot_2.1.1       rstantools_2.5.0      scales_1.4.0         \n#> [64] xtable_1.8-4          glue_1.8.0            emmeans_1.11.2-8     \n#> [67] tools_4.5.1           data.table_1.17.8     mvtnorm_1.3-3        \n#> [70] grid_4.5.1            QuickJSR_1.8.0        colorspace_2.1-1     \n#> [73] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#> [76] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#> [79] gtable_0.3.6          digest_0.6.37         TH.data_1.1-4        \n#> [82] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n#> [85] htmltools_0.5.8.1     lifecycle_1.0.4       MASS_7.3-65\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n",
    "supporting": [
      "03_stan_intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}