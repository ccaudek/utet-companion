{
  "hash": "b29509d0dc64f3a2b034a4764b0fc57d",
  "result": {
    "engine": "knitr",
    "markdown": "# Modelli Mistura Gaussiani {#sec-mcmc-mixture-models}\n\n\n## Introduzione {.unnumbered .unlisted}\n\nLa distribuzione gaussiana costituisce uno degli strumenti fondamentali della statistica, non solo perché molti fenomeni psicologici ed empirici possono approssimativamente seguirne l’andamento, ma anche perché essa funge da elemento costitutivo di modelli più complessi. Un esempio particolarmente rilevante è rappresentato dalle cosiddette *misture di distribuzioni*. Una mistura descrive una variabile casuale la cui distribuzione complessiva deriva dalla combinazione ponderata di più distribuzioni elementari, ciascuna delle quali rappresenta una possibile sottopopolazione dei dati osservati.\n\nNel caso delle variabili continue, una mistura di gaussiane è definita come la somma pesata di più densità normali. Ad esempio, una mistura di due distribuzioni normali può essere espressa come\n\n$$\nf(x; \\pi_1, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2) = \\pi_1 \\, \\phi(x; \\mu_1, \\sigma_1) + \\pi_2 \\, \\phi(x; \\mu_2, \\sigma_2),\n$$\ndove $\\pi_1$ e $\\pi_2 = 1 - \\pi_1$ rappresentano i pesi delle due componenti e $\\phi(x; \\mu, \\sigma)$ è la densità di una distribuzione normale con media $\\mu$ e deviazione standard $\\sigma$. In questo modo, la distribuzione osservata non coincide con una singola gaussiana, ma risulta dalla sovrapposizione di più curve normali che descrivono sottogruppi distinti della popolazione.\n\nUn’applicazione classica di questo approccio riguarda lo studio di popolazioni eterogenee. Se consideriamo, ad esempio, l’altezza di uomini e donne, possiamo assumere che ciascun gruppo segua una distribuzione normale con media e varianza proprie. La distribuzione complessiva dell’altezza nella popolazione generale sarà quindi una mistura di due gaussiane, che riflette la coesistenza di sottopopolazioni con caratteristiche differenti.\n\nL’idea alla base delle misture di gaussiane è dunque quella di fornire un modello flessibile capace di rappresentare situazioni in cui i dati provengono da più processi generativi sottostanti. Questo aspetto è particolarmente rilevante in psicologia, dove spesso i campioni includono soggetti appartenenti a sottogruppi differenti, non sempre immediatamente osservabili. L’utilizzo di modelli di mistura permette di identificare tali sottopopolazioni e di descriverne le proprietà con maggiore precisione, aprendo così la strada a interpretazioni più ricche e articolate dei fenomeni psicologici.\n\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n* Comprendere il ruolo dei modelli di mistura gaussiana nello studio di popolazioni eterogenee.\n* Simulare dati che rappresentano due sottogruppi distinti (mindfulness e controllo).\n* Stimare i parametri del modello tramite inferenza bayesiana in Stan.\n* Valutare l’incertezza delle stime e interpretarne i risultati.\n* Visualizzare la struttura latente dei dati per trarne implicazioni psicologiche.\n\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere [Mindfulness and affect-network density: Does mindfulness facilitate disengagement from affective experiences in daily life?](https://link.springer.com/content/pdf/10.1007/s12671-020-01335-4.pdf) di @rowland2020mindfulness.\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> source()\n\n# Carichiamo i pacchetti necessari\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(cmdstanr, posterior, insight, bayesplot, ggplot2)\n```\n:::\n\n:::\n\n\n## Simulazione dei Dati\n\nPer illustrare l’uso dei modelli di mistura, ci basiamo su uno studio condotto da @rowland2020mindfulness, che ha indagato gli effetti di un training di mindfulness molto breve sulla consapevolezza e sull’autocontrollo percepito nella vita quotidiana. In quell’indagine i partecipanti erano suddivisi in due gruppi: il primo riceveva un addestramento alla mindfulness, mentre il secondo fungeva da gruppo di controllo. Le valutazioni erano raccolte tramite un protocollo di monitoraggio giornaliero, avviato subito dopo la prima sessione di laboratorio e protratto per quaranta giorni consecutivi.\n\nPer ragioni didattiche, qui non utilizziamo i dati originali, ma costruiamo una simulazione ispirata allo studio. Ci concentriamo in particolare su una delle dimensioni prese in esame, quella relativa al vissuto di tristezza (“sad”). Immaginiamo che i due gruppi mostrino distribuzioni differenti: il gruppo sottoposto a training mindfulness con una media più bassa nei giudizi di tristezza, e il gruppo di controllo con una media più elevata. Generiamo quindi due sottopopolazioni gaussiane, con la stessa deviazione standard ma con valori medi distinti, e un numero uguale di osservazioni.\n\nIl codice seguente crea i dati simulati, li organizza in un data frame e ne fornisce una prima esplorazione grafica tramite un istogramma dei valori standardizzati:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri per le distribuzioni normali\n# Sad mindfulness\nmu_mindfulness <- 20   # giudizi sad\nsigma_mindfulness <- 10  # deviazione standard\n\n# Sad control\nmu_control <- 60   # giudizi sad\nsigma_control <- 10  # deviazione standard\n\n# Simulazione di 60 casi per ciascuna sottopopolazione\nset.seed(42)  # per la riproducibilità\nsad_mindfulness <- rnorm(60, mean = mu_mindfulness, sd = sigma_mindfulness)\nsad_control <- rnorm(60, mean = mu_control, sd = sigma_control)\n\n# Creazione del DataFrame per visualizzare i dati\ndati_sad <- data.frame(\n  Sad = c(sad_mindfulness, sad_control),\n  Group = c(rep(\"Mindfulness\", 60), rep(\"Control\", 60))\n)\n\n# Mostra i primi e gli ultimi dati simulati\nhead(dati_sad)\n#>    Sad       Group\n#> 1 33.7 Mindfulness\n#> 2 14.4 Mindfulness\n#> 3 23.6 Mindfulness\n#> 4 26.3 Mindfulness\n#> 5 24.0 Mindfulness\n#> 6 18.9 Mindfulness\ntail(dati_sad)\n#>      Sad   Group\n#> 115 43.4 Control\n#> 116 56.2 Control\n#> 117 54.9 Control\n#> 118 87.0 Control\n#> 119 46.4 Control\n#> 120 61.4 Control\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri delle distribuzioni\nparametri_distribuzioni <- data.frame(\n  Sottopopolazione = c(\"Mindfulness\", \"Control\"),\n  Media_mu = c(mu_mindfulness, mu_control),\n  Deviazione_Standard_sigma = c(sigma_mindfulness, sigma_control)\n)\n\nprint(parametri_distribuzioni)\n#>   Sottopopolazione Media_mu Deviazione_Standard_sigma\n#> 1      Mindfulness       20                        10\n#> 2          Control       60                        10\n```\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Standardizzazione dei dati\ndati_sad$Sad_z <- scale(dati_sad$Sad)\n\n# Creazione dell'istogramma per i dati standardizzati\nggplot(dati_sad, aes(x = Sad_z)) +\n  geom_histogram(\n    bins = 20\n  ) +\n  labs(\n    x = \"Giudizi 'Sad' Standardizzati\",\n    y = \"Frequenza\"\n  )\n```\n\n::: {.cell-output-display}\n![](09_stan_gaussian_mixture_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL’istogramma mostra come i dati simulati non seguano una singola distribuzione normale, ma rivelino piuttosto la presenza di due sottopopolazioni sovrapposte, coerenti con la distinzione tra gruppo mindfulness e gruppo di controllo.\n\n### Codice Stan\n\nIl seguente codice Stan è stato ripreso dallo studio di Michael Betancourt *Identifying Bayesian Mixture Models* e fornisce un’implementazione essenziale di un modello di mistura di due distribuzioni gaussiane:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Imposta il percorso al file Stan\nstan_file <- file.path(here(\"stan\", \"bimodal_model.stan\"))\n\n# Compila il modello\nmodel <- cmdstan_model(stan_file)\n```\n:::\n\nLa struttura del modello segue la logica tipica di Stan, articolandosi nei blocchi di definizione dei dati, dei parametri e del modello vero e proprio. Nel blocco `data` vengono introdotte le osservazioni: il numero totale dei dati, indicato con $N$, e il vettore $y$, che raccoglie i valori osservati. Si tratta quindi dell’informazione empirica che vogliamo interpretare attraverso la mistura.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel$print()\n#> data {\n#>   int<lower=0> N;\n#>   vector[N] y;\n#> }\n#> parameters {\n#>   ordered[2] mu;\n#>   array[2] real<lower=0> sigma;\n#>   real<lower=0, upper=1> theta;\n#> }\n#> model {\n#>   sigma ~ normal(0, 2);\n#>   mu ~ normal(0, 2);\n#>   theta ~ beta(5, 5);\n#>   for (n in 1 : N) \n#>     target += log_mix(theta, normal_lpdf(y[n] | mu[1], sigma[1]),\n#>                       normal_lpdf(y[n] | mu[2], sigma[2]));\n#> }\n```\n:::\n\n\nIl blocco `parameters` specifica le quantità ignote da stimare. In primo luogo compaiono le due medie $\\mu$, dichiarate come vettore ordinato: questa scelta tecnica, che impone la condizione $\\mu_1 \\leq \\mu_2$, evita problemi di identificabilità che potrebbero emergere se i due componenti della mistura fossero liberi di scambiarsi di posizione. Seguono le deviazioni standard $\\sigma$, che devono assumere valori positivi, e infine il parametro $\\theta$, vincolato all’intervallo compreso tra 0 e 1, che rappresenta la proporzione di osservazioni attribuibili al primo componente della mistura.\n\nIl cuore del modello si trova nel blocco `model`, dove vengono specificate le distribuzioni a priori e la funzione di verosimiglianza. Alle medie e alle deviazioni standard si assegna una distribuzione normale con media zero e deviazione standard pari a 2, che esprime aspettative flessibili ma non eccessivamente diffuse sui valori plausibili di questi parametri. Al parametro di mescolamento $\\theta$ si assegna invece una distribuzione Beta(5,5), centrata su 0.5, che riflette un’aspettativa iniziale di equilibrio tra le due componenti senza però escludere altre configurazioni.\n\nPer ciascuna osservazione $y[n]$, il modello calcola la log-verosimiglianza utilizzando la funzione `log_mix`. Questa funzione combina la probabilità che il dato provenga dalla prima distribuzione normale, pesata con $\\theta$, e quella che provenga dalla seconda distribuzione, pesata con $1 - \\theta$. In questo modo il modello traduce formalmente l’idea di mistura: ogni osservazione è considerata come proveniente da due sorgenti potenziali, con probabilità rispettivamente $\\theta$ e $1 - \\theta$.\n\nIn sintesi, il modello Stan implementa un meccanismo generativo in cui i dati osservati sono trattati come campioni provenienti da una distribuzione bimodale. L’obiettivo dell’inferenza bayesiana è stimare, a partire dai dati, le medie e le deviazioni standard delle due gaussiane che compongono la mistura, insieme alla proporzione $\\theta$ che determina il peso relativo dei due componenti. Questo approccio, pur nella sua semplicità, consente di catturare in modo rigoroso la presenza di sottopopolazioni latenti nei dati psicologici.\n\n\nCreiamo il dizionario con i dati nel formato richiesto da Stan.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione della lista di dati per Stan\nstan_data <- list(\n  N = length(dati_sad$Sad_z),\n  y = as.numeric(dati_sad$Sad_z)  # vettore numerico\n)\nstan_data\n#> $N\n#> [1] 120\n#> \n#> $y\n#>   [1] -0.28517 -1.12319 -0.72150 -0.60472 -0.70369 -0.92466 -0.22432 -0.91969\n#>   [9] -0.00486 -0.90586 -0.31379  0.11126 -1.48000 -0.99941 -0.93643 -0.60338\n#>  [17] -1.00177 -2.02879 -1.93528 -0.30719 -1.01147 -1.64991 -0.95314 -0.35283\n#>  [25] -0.05821 -1.06508 -0.99009 -1.64205 -0.67952 -1.15579 -0.68153 -0.57356\n#>  [33] -0.43058 -1.14234 -0.66010 -1.62207 -1.21833 -1.24710 -1.92391 -0.86307\n#>  [41] -0.78953 -1.03503 -0.55047 -1.19333 -1.47109 -0.69133 -1.22999 -0.25351\n#>  [49] -1.06550 -0.59486 -0.73934 -1.21806 -0.19652 -0.60038 -0.83985 -0.75898\n#>  [57] -0.58462 -0.83982 -2.17453 -0.75537  0.69405  0.93323  1.10493  1.45903\n#>  [65]  0.53816  1.41696  0.99844  1.30264  1.25165  1.16513  0.40143  0.81399\n#>  [73]  1.12298  0.44022  0.61803  1.10457  1.18561  1.05382  0.46955  0.37690\n#>  [81]  1.50794  0.96470  0.89133  0.80070  0.33597  1.11799  0.75903  0.77391\n#>  [89]  1.25712  1.20881  1.45573  0.64688  1.13460  1.45530  0.37213  0.48037\n#>  [97]  0.36306  0.22129  0.88766  1.13583  1.37298  1.30535  0.41871  1.65331\n#> [105]  0.56437  0.89872  0.67023  0.80007  0.93451  0.90463  0.84217  0.89983\n#> [113]  0.64287  0.63474  0.13389  0.68751  0.63109  2.02278  0.26333  0.91246\n```\n:::\n\n\n### Campionamento\n\nUna volta definito il modello, possiamo procedere all’inferenza bayesiana eseguendo il campionamento MCMC.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- model$sample(\n  data = stan_data,\n  iter_warmup = 2000,\n  iter_sampling = 2000,\n  seed = 123,\n  refresh = 0   \n)\n```\n:::\n\n\nIl campionamento produce migliaia di estrazioni dalla distribuzione a posteriori dei parametri. Un primo passo per la valutazione della bontà del modello consiste nell’ispezione delle catene MCMC, che consente di verificare la loro convergenza e la stabilità delle stime.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Traceplot per i parametri di interesse\nmcmc_trace(as_draws_array(fit$draws(c(\"mu\", \"sigma\", \"theta\"))))\n```\n\n::: {.cell-output-display}\n![](09_stan_gaussian_mixture_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n\n### Risultati\n\nPossiamo poi riassumere le distribuzioni a posteriori dei parametri principali del modello, ovvero le medie, le deviazioni standard e la proporzione di mescolamento.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit$summary(variables = c(\"mu\", \"sigma\", \"theta\"))\n#> # A tibble: 5 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 mu[1]    -0.858 -0.868 0.101 0.091 -1.001 -0.679 1.002 3141.500 2608.342\n#> 2 mu[2]     0.883  0.886 0.070 0.066  0.766  0.996 1.000 6778.401 5781.664\n#> 3 sigma[1]  0.550  0.539 0.082 0.074  0.439  0.699 1.001 3935.774 3249.503\n#> 4 sigma[2]  0.425  0.420 0.054 0.052  0.346  0.521 1.000 4739.147 4983.220\n#> 5 theta     0.507  0.505 0.054 0.052  0.422  0.599 1.000 4136.772 3356.066\n```\n:::\n\n\nPer visualizzare meglio i risultati, possiamo rappresentare la distribuzione bimodale ricostruita a partire dai valori medi posteriori.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Posterior estimates (da output Stan, qui inseriti manualmente come nell’esempio Python)\nmu_0 <- -0.859\nmu_1 <-  0.942\nsigma_0 <- 0.468\nsigma_1 <- 0.390\ntheta <- 0.520  # Probabilità del primo componente\n\n# Simulazione dati dalla distribuzione mista (per esempio 1000 campioni)\nset.seed(42)\ndata <- c(\n  rnorm(1000 * theta, mean = mu_0, sd = sigma_0),\n  rnorm(1000 * (1 - theta), mean = mu_1, sd = sigma_1)\n)\n\n# Densità teorica della mixture\nx <- seq(min(data), max(data), length.out = 1000)\nkde <- theta * dnorm(x, mean = mu_0, sd = sigma_0) +\n       (1 - theta) * dnorm(x, mean = mu_1, sd = sigma_1)\n\n# Scala la densità per l’istogramma (conteggi, non densità)\nbinwidth <- (max(data) - min(data)) / 50\nhist_area <- length(data) * binwidth\nkde_scaled <- kde * hist_area\n\n# Plot\nggplot(data.frame(x = data), aes(x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", alpha = 0.6, color = \"black\") +\n  geom_line(data = data.frame(x, y = kde_scaled), aes(x, y),\n            color = \"black\", size = 1) +\n  labs(\n    x = \"Sadness Score\", y = \"Counts\"\n  )\n```\n\n::: {.cell-output-display}\n![](09_stan_gaussian_mixture_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIl confronto tra l’istogramma dei dati simulati e la curva della distribuzione stimata mostra una sovrapposizione molto convincente, segno che il modello è riuscito a cogliere correttamente la struttura bimodale dei dati.\n\nPoiché i dati erano stati standardizzati prima dell’analisi, possiamo ricondurre i parametri stimati alla scala originale dei punteggi per ottenere una lettura più intuitiva.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Media e deviazione standard della distribuzione originale\nmean_original <- mean(dati_sad$Sad)\nstd_original  <- sd(dati_sad$Sad)\n\n# Parametri posteriori delle distribuzioni standardizzate\nmu_0_standardized <- -0.859\nmu_1_standardized <-  0.942\nsigma_0_standardized <- 0.468\nsigma_1_standardized <- 0.390\n\n# Conversione dei parametri alla scala originale\nmu_0_original <- mu_0_standardized * std_original + mean_original\nmu_1_original <- mu_1_standardized * std_original + mean_original\nsigma_0_original <- sigma_0_standardized * std_original\nsigma_1_original <- sigma_1_standardized * std_original\n\n# Stampa dei risultati\ncat(\n  sprintf(\"Media a posteriori della prima sottopopolazione: %.2f\\n\", mu_0_original),\n  sprintf(\"Media a posteriori della seconda sottopopolazione: %.2f\\n\", mu_1_original),\n  sprintf(\"Deviazione standard a posteriori della prima sottopopolazione: %.2f\\n\", sigma_0_original),\n  sprintf(\"Deviazione standard a posteriori della seconda sottopopolazione: %.2f\\n\", sigma_1_original)\n)\n#> Media a posteriori della prima sottopopolazione: 20.46\n#>  Media a posteriori della seconda sottopopolazione: 62.05\n#>  Deviazione standard a posteriori della prima sottopopolazione: 10.81\n#>  Deviazione standard a posteriori della seconda sottopopolazione: 9.01\n```\n:::\n\n\nIl confronto con i valori utilizzati per simulare i dati ($\\mu = 20$ e $\\mu = 60$, con $\\sigma = 10$ in entrambi i gruppi) evidenzia una corrispondenza molto buona. Questo risultato è particolarmente significativo se si considera che il modello non riceveva alcuna informazione esplicita sull’appartenenza di ciascun individuo a un gruppo. In altre parole, è stato in grado di individuare autonomamente le due sottopopolazioni e di stimarne i parametri con precisione.\n\nDal punto di vista sostantivo, questo esempio didattico ci permette di riflettere sulla questione posta nello studio di @rowland2020mindfulness. Se in una popolazione eterogenea esistono gruppi che reagiscono in modo diverso a un intervento psicologico, come nel caso del training di mindfulness, i modelli di mistura offrono uno strumento potente per identificare tali differenze senza dover disporre a priori di una classificazione. Nel nostro caso, il modello ha permesso di distinguere chiaramente il gruppo sottoposto a mindfulness da quello di controllo sulla base dei punteggi di tristezza. Ciò illustra in termini concreti come le tecniche bayesiane di mistura possano contribuire a svelare strutture latenti nei dati psicologici, rendendo visibili effetti che altrimenti rischierebbero di rimanere nascosti.\n\n\n## Riflessioni conlusive {.ununmbered .unlisted}\n\nIn questo capitolo abbiamo visto come i modelli di mistura gaussiana possano essere applicati con successo a dati psicologici che nascondono una struttura eterogenea. Partendo da una simulazione ispirata allo studio di @rowland2020mindfulness, abbiamo mostrato come l’approccio bayesiano consenta di stimare in modo accurato i parametri di due sottopopolazioni distinte sulla base dei giudizi di tristezza.\n\nIl risultato più rilevante è che il modello è stato in grado di distinguere i due gruppi senza alcuna informazione preventiva sulla loro appartenenza. Questo aspetto è cruciale in psicologia, dove spesso non si dispone di criteri netti per separare sottogruppi di individui, ma dove le differenze emergono come pattern latenti nei dati. L’inferenza bayesiana applicata ai modelli di mistura permette non solo di stimare questi pattern, ma anche di quantificare l’incertezza che li accompagna, fornendo quindi una base solida per le interpretazioni teoriche.\n\nDal punto di vista sostantivo, la lezione che possiamo trarre è che la variabilità osservata nei dati psicologici non deve essere considerata un semplice “rumore”, ma può riflettere la presenza di processi distinti che operano all’interno della popolazione. Proprio come nel nostro esempio, in cui il gruppo mindfulness mostrava livelli inferiori di tristezza rispetto al gruppo di controllo, i modelli di mistura offrono un mezzo per far emergere tali differenze anche in assenza di etichette esplicite.\n\nIn conclusione, i modelli di mistura gaussiana rappresentano uno strumento prezioso per lo psicologo che desidera andare oltre le analisi aggregate e indagare la complessità intrinseca dei fenomeni psicologici. Essi consentono di riconoscere la presenza di sottogruppi latenti, di stimarne le caratteristiche e di aprire così nuove prospettive interpretative sui dati, in linea con la crescente attenzione della ricerca contemporanea alla diversità interindividuale e alle dinamiche contestuali che influenzano l’esperienza umana.\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] insight_1.4.2         cmdstanr_0.9.0        pillar_1.11.0        \n#>  [4] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#>  [7] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#> [10] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#> [13] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#> [16] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#> [19] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#> [22] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#> [25] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        reshape2_1.4.4       \n#> [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        \n#> [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        \n#> [16] backports_1.5.0       labeling_0.4.3        utf8_1.2.6           \n#> [19] rmarkdown_2.29        ps_1.9.1              ragg_1.5.0           \n#> [22] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#> [25] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#> [28] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#> [31] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#> [34] zoo_1.8-14            pacman_0.5.1          Matrix_1.7-4         \n#> [37] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#> [40] abind_1.4-8           yaml_2.3.10           codetools_0.2-20     \n#> [43] curl_7.0.0            processx_3.8.6        pkgbuild_1.4.8       \n#> [46] plyr_1.8.9            lattice_0.22-7        withr_3.0.2          \n#> [49] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [52] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [55] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [58] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [61] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [64] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#> [67] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#> [70] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#> [73] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#> [76] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#> [79] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#> [82] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#> [85] MASS_7.3-65\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n\n\n",
    "supporting": [
      "09_stan_gaussian_mixture_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}