{
  "hash": "a8c194b21e959f5bee1859df60d0d778",
  "result": {
    "engine": "knitr",
    "markdown": "# Regressione logistica con Stan {#sec-reg-logistic-stan}\n\n\n## Introduzione {.unnumbered .unlisted}\n\nLa *regressione logistica* è un’estensione del modello lineare che consente di analizzare esiti dicotomici, cioè variabili che assumono soltanto due valori (ad esempio: successo/insuccesso, presente/assente, risposta corretta/errata). Situazioni di questo tipo sono molto frequenti nella ricerca psicologica: pensiamo alle risposte a item vero/falso, alla presenza o assenza di un sintomo clinico, oppure alla scelta tra due alternative in un compito sperimentale.\n\nA differenza della regressione lineare, non modelliamo direttamente la probabilità di successo, ma il suo *logit*, ossia il logaritmo del rapporto tra odds di successo e odds di insuccesso. Questo passaggio ha due vantaggi fondamentali: da un lato mantiene la struttura lineare del modello, dall’altro assicura che le probabilità restino confinate tra 0 e 1, rispettando così la natura del fenomeno da descrivere.\n\nIn questo capitolo ci concentreremo sulla *regressione logistica bivariata*, cioè con un solo predittore, continuo o categoriale. L’obiettivo è duplice: da un lato, imparare a stimare i coefficienti del modello in un’ottica *bayesiana* utilizzando *Stan*; dall’altro, capire come interpretare questi coefficienti su scale diverse: in termini di probabilità, di *odds ratio* e di *risk ratio*. Vedremo in particolare come un coefficiente unico possa assumere significati diversi a seconda della scala di lettura, e come il caso di un predittore dicotomico (dummy) sia in realtà una specializzazione del modello con variabile continua.\n\nLa regressione logistica rappresenta dunque un primo passo importante nel percorso dei *modelli lineari generalizzati (GLM)*, ampliando le possibilità dell’analisi statistica ben oltre il caso della regressione lineare e preparandoci ad affrontare dati e domande di ricerca più vari e realistici.\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- specificare una regressione logistica con un predittore continuo; \n- interpretare i coefficienti sulla scala dei logit, degli odds e delle probabilità, ricavando in modo chiaro le relazioni algebriche tra $RD$, $OR$ e $RR$; \n- stimare il modello con approccio frequentista (glm) e bayesiano (brms/Stan), comprendendo l’effetto dei priori e leggendo le distribuzioni a posteriori; \n- produrre predizioni posteriori su una griglia di valori di $x$ e rappresentare l’incertezza con curve e intervalli credibili; \n- valutare l’adeguatezza del modello attraverso i posterior predictive checks.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere il capitolo dedicato alla regressione statistica di *Applied regression analysis and generalized linear models* [@fox2015applied].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(brms, cmdstanr, posterior, brms, bayestestR, insight)\n```\n:::\n\n:::\n\n\n\n## Il modello di regressione logistica\n\nSupponiamo di osservare $n$ individui, ciascuno con un esito binario $y_i \\in \\{0,1\\}$ e un predittore continuo $x_i$. La regressione logistica specifica che\n\n$$\ny_i \\sim \\text{Bernoulli}(p_i), \\qquad\n\\text{logit}(p_i) = \\alpha + \\beta x_i .\n$$\nQui $p_i$ è la probabilità di successo per l’individuo $i$. L’intercetta $\\alpha$ è il log-odds di successo quando $x_i=0$, mentre il coefficiente $\\beta$ rappresenta il cambiamento nei log-odds per ogni unità di incremento in $x$.  \n\nSulla scala degli odds questo significa che\n\n$$\n\\text{odds}(x) = \\frac{p(x)}{1-p(x)} = \\exp(\\alpha + \\beta x).\n$$\nConfrontando due valori consecutivi, $x=a$ e $x=a+1$, otteniamo\n\n$$\n\\frac{\\text{odds}(a+1)}{\\text{odds}(a)} = e^{\\beta}.\n$$\nQuindi l’esponenziale di $\\beta$ è l’*odds ratio (OR)*: il fattore moltiplicativo con cui cambiano gli odds per un incremento unitario in $x$. Ad esempio, se $\\beta=1.0$, allora $OR \\approx 2.7$: ogni unità in più di $x$ rende gli odds di successo circa 2.7 volte maggiori.  \n\n\n::: {.callout-note collapse=\"true\"}\n\n## Perché $e^{\\beta}$ è l’odds ratio\n\nNel modello logistico $\\log\\!\\big(\\tfrac{p(x)}{1-p(x)}\\big)=\\alpha+\\beta x$, l’odds a livello $x$ è $\\tfrac{p(x)}{1-p(x)}=\\exp(\\alpha+\\beta x)$. Consideriamo due valori qualsiasi del predittore, $x=a$ e $x=b$. L’*odds ratio* che confronta $b$ con $a$ è definito come\n\n$$\nOR(b\\,\\text{vs}\\,a)\n=\\frac{\\tfrac{p(b)}{1-p(b)}}{\\tfrac{p(a)}{1-p(a)}}\n=\\frac{\\exp(\\alpha+\\beta b)}{\\exp(\\alpha+\\beta a)}\n=\\exp\\!\\big(\\beta(b-a)\\big).\n$$\n\nSe la variazione è unitaria ($b=a+1$), segue immediatamente che\n\n$$\nOR(a+1\\,\\text{vs}\\,a)=\\exp(\\beta).\n$$\n\nQuesta identità mostra due fatti chiave. Primo, l’odds ratio dipende solo dalla *differenza* $b-a$ e non dal livello di partenza $a$: in un modello logistico bivariato l’OR per un incremento fissato è *costante* lungo l’asse di $x$. Secondo, la scala di misura di $x$ determina l’interpretazione di $\\beta$: se $x$ è una *dummy* $\\{0,1\\}$, $\\beta$ è il log-odds ratio tra i due gruppi e $\\exp(\\beta)$ è l’OR gruppi; se $x$ aumenta di 10 unità, allora $\\exp(10\\beta)$ è l’OR per un incremento di dieci unità; se $x$ è standardizzato (ad es. z-score), $\\exp(\\beta)$ è l’OR per un incremento di *una* deviazione standard.\n:::\n\n\n\n## Risk difference, odds ratio e risk ratio\n\nConsideriamo due valori qualsiasi del predittore, $x=a$ e $x=b$. Indichiamo con\n\n$$\np_a = \\text{logit}^{-1}(\\alpha + \\beta a), \\qquad\np_b = \\text{logit}^{-1}(\\alpha + \\beta b)\n$$\nle probabilità corrispondenti. Possiamo descrivere la differenza fra i due livelli in tre modi:  \n\n- *Risk difference (RD):*  \n  $$\n  RD = p_b - p_a .\n  $$\n  È la differenza assoluta fra le due probabilità.  \n\n- *Odds ratio (OR):*  \n  $$\n  OR = \\frac{p_b/(1-p_b)}{p_a/(1-p_a)} = \\exp\\!\\bigl(\\beta(b-a)\\bigr).\n  $$\n  Mostra come gli odds cambiano passando da $a$ a $b$.  \n\n- *Risk ratio (RR):*  \n  $$\n  RR = \\frac{p_b}{p_a}.\n  $$\n  È il rapporto diretto fra probabilità, usato spesso in ambito epidemiologico.  \n\nLe tre misure sono modi diversi, ma coerenti, di esprimere l’effetto del predittore.\n\n\n### Scala delle probabilità e la regola del “dividere per 4”\n\nLa funzione logistica che lega $x$ a $p(x)$ è\n\n$$\np(x) = \\frac{e^{\\alpha + \\beta x}}{1 + e^{\\alpha + \\beta x}}.\n$$\nSe consideriamo la variazione di probabilità per un incremento unitario in $x$,\n\n$$\n\\Delta p = p(x+1) - p(x),\n$$\nvediamo che l’effetto non è costante ma dipende dal livello di $x$. Ai margini della curva, quando $p$ è vicino a 0 o 1, la variazione è minima; nella zona centrale, quando $p \\approx 0.5$, la curva è più ripida e l’effetto massimo.  \n\nLa derivata della funzione logistica è\n\n$$\n\\frac{dp}{dx} = \\beta \\, p(x)\\,[1 - p(x)].\n$$\nIl termine $p(x)(1-p(x))$ è massimo quando $p=0.5$, e in quel punto vale $0.25$. Quindi la massima variazione di probabilità per unità di $x$ è\n\n$$\n\\max \\frac{dp}{dx} = \\frac{\\beta}{4}.\n$$\nQuesta è la cosiddetta *regola del dividere per 4*: un metodo semplice per stimare, in prima approssimazione, l’effetto massimo di $\\beta$ sulla scala delle probabilità. Ad esempio, se $\\beta=1.0$, il massimo incremento di probabilità per unità di $x$ è circa 0.25, cioè 25 punti percentuali, quando $p=0.5$. Questa regola non è esatta in generale, ma fornisce un’intuizione immediata dell’ordine di grandezza dell’effetto di $\\beta$ sulla probabilità.\n\n\n### Sintesi\n\nIl coefficiente $\\beta$ della regressione logistica ha interpretazioni coerenti su scale diverse:  \n\n- sulla *scala logit* è la variazione lineare dei log-odds;  \n- sulla *scala odds* il suo esponenziale è l’odds ratio, il moltiplicatore degli odds;  \n- sulla *scala probabilità* descrive variazioni non costanti, con massimo effetto pari a circa $\\beta/4$ quando $p=0.5$.  \n\nQueste interpretazioni non sono alternative ma complementari: lo stesso coefficiente viene letto in tre linguaggi diversi, offrendo prospettive complementari sul legame tra $x$ e la probabilità di successo.\n\n\n### Visualizzazione delle tre scale\n\nPer fissare meglio le idee, possiamo rappresentare graficamente l’effetto del coefficiente $\\beta$ sulle tre scale: logit, odds e probabilità. Useremo valori simulati, così da confrontare direttamente i tre casi.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n#### Come leggere i tre grafici\n\n* *Scala logit*: la relazione con $x$ è lineare. Ogni unità in più di $x$ aumenta i log-odds di $\\beta$.\n* *Scala odds*: la crescita è esponenziale. Qui $\\exp(\\beta)$ indica di quanto si moltiplicano gli odds per ogni unità aggiuntiva di $x$.\n* *Scala probabilità*: la curva è sigmoide e rimane sempre tra 0 e 1. L’effetto di $\\beta$ non è costante: è massimo quando $p \\approx 0.5$, e minimo ai margini (quando la curva è piatta).\n\n\n### Interpretazione didattica\n\nLa regressione logistica descrive una relazione *non lineare* fra il predittore $x$ e la probabilità di successo $p(x)$. La curva che ne risulta è una sigmoide: per valori molto bassi di $x$ la probabilità si avvicina a 0, per valori molto alti tende a 1, e nella zona centrale varia rapidamente.\n\nIl segno del coefficiente $\\beta$ determina la direzione dell’effetto. Se $\\beta > 0$, all’aumentare di $x$ crescono log-odds, odds e probabilità: la curva ha pendenza positiva. Se $\\beta < 0$, accade il contrario e la curva decresce.  \n\nLo stesso effetto può essere letto su scale diverse, che offrono prospettive complementari:  \n\n- sulla *scala delle probabilità*, si osserva la variazione assoluta di $p(x)$;  \n- sulla *scala degli odds*, l’effetto è un moltiplicatore costante $OR = e^{\\beta}$ per ogni incremento unitario di $x$;  \n- sulla *scala logit*, l’effetto si traduce in un incremento lineare costante di $\\beta$ nei log-odds.  \n\nUn ulteriore strumento utile è la *risk difference* (RD), cioè la differenza di probabilità fra due livelli di $x$, e il *risk ratio* (RR), cioè il loro rapporto. Queste misure, pur non derivando direttamente dal coefficiente come l’odds ratio, offrono un linguaggio più immediato in molti contesti applicativi (per esempio in epidemiologia o psicologia clinica).  \n\nLa forza della regressione logistica sta proprio in questa unificazione: un unico modello produce tre chiavi di lettura — logit, odds e probabilità — che, se usate insieme, permettono di descrivere in modo chiaro e coerente l’impatto di una variabile indipendente su un esito binario.\n\n\n## Esempio numerico\n\nSimuliamo dati con un predittore discreto $X$ e una variabile dicotomica $Y$, in cui la probabilità di successo cresce con $X$. Applichiamo quindi la regressione logistica e tracciamo la curva stimata, confrontandola con le proporzioni empiriche osservate.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Fissiamo il seme per riproducibilità\nset.seed(42)\n\n# Numero di osservazioni\nn <- 1000\n\n# Predittore X: estraiamo numeri interi fra 0 e 9 con uguale probabilità\nX <- sample(0:9, n, replace = TRUE)\n\n# Definiamo una funzione logistica che restituisce plogis(alpha + beta * x),\n# cioè la trasformazione inversa del logit\nlogistic <- function(x, beta0, beta1) plogis(beta0 + beta1 * x)\n\n# Parametri veri del modello usato per simulare i dati\nbeta0 <- -2   # intercetta\nbeta1 <- 1    # coefficiente\n\n# Calcoliamo la probabilità di successo associata a ciascun valore di X\np <- logistic(X, beta0, beta1)\n\n# Generiamo i dati binari (0/1) da una distribuzione binomiale di Bernoulli\nY <- rbinom(n, size = 1, prob = p)\n\n# Creiamo un data frame con i dati simulati\ndf <- tibble::tibble(X = X, Y = Y)\nhead(df)\n#> # A tibble: 6 × 2\n#>       X     Y\n#>   <int> <int>\n#> 1     0     0\n#> 2     4     1\n#> 3     0     0\n#> 4     8     1\n#> 5     9     1\n#> 6     3     0\n```\n:::\n\n\nStima del modello di regressione logistica con funzione `glm()`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# family = binomial(link = \"logit\") specifica che usiamo una regressione logistica\nlogit_model <- glm(Y ~ X, data = df, family = binomial(link = \"logit\"))\n```\n:::\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creiamo una griglia di valori di X su cui calcolare le probabilità predette\nx_vals <- seq(min(df$X), max(df$X), length.out = 100)\n\n# Data frame per le predizioni\npred_df <- data.frame(X = x_vals)\n\n# Calcoliamo le probabilità predette dal modello (scala di risposta, cioè probabilità)\npred_df$pred <- predict(logit_model, newdata = pred_df, type = \"response\")\n```\n:::\n\n\nGrafico finale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(df, aes(x = X, y = Y)) +\n  # Per ogni valore di X mostriamo la proporzione empirica di successi (punti blu)\n  stat_summary(fun = mean, geom = \"point\", color = \"blue\") +\n  # Sovrapponiamo la curva logistica stimata dal modello (linea rossa)\n  geom_line(data = pred_df, aes(x = X, y = pred), color = \"red\") +\n  # Etichette degli assi\n  labs(x = \"X\", y = \"Probabilità stimata\")\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn questo esempio i punti blu rappresentano la proporzione empirica di successi per ciascun valore di $X$, mentre la linea rossa mostra la curva logistica stimata dal modello. La forma sigmoide emerge naturalmente e garantisce che le probabilità rimangano sempre comprese tra 0 e 1. \n\n\n## Stima bayesiana con Stan\n\nFinora abbiamo stimato i coefficienti $\\alpha$ e $\\beta$ con `glm()`, ottenendo valori puntuali secondo il metodo della massima verosimiglianza. Con Stan possiamo costruire lo stesso modello in chiave bayesiana, specificando priori debolmente informativi. Questo ci consente di ottenere l’intera distribuzione a posteriori dei parametri, invece di un singolo punto stima, e di quantificare direttamente l’incertezza.  \n\nUn dettaglio importante è che i risultati di Stan possono differire leggermente da quelli di `glm()`. La ragione è proprio la presenza dei priori: anche se scelti molto larghi (qui `normal(0, 2.5)`), essi esercitano un piccolo “effetto di contrazione” verso lo zero, soprattutto con campioni finiti. In assenza di priori (o con dati molto abbondanti), le due stime coincidono. Questa differenza è didatticamente preziosa, perché mostra come l’approccio frequentista si possa vedere come un caso limite del bayesiano.\n\n\n### Il modello in Stan\n\nIl modello di regressione logistica per un predittore continuo si scrive così:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstan_code <- '\ndata {\n  int<lower=0> N;           // numero di osservazioni\n  array[N] int<lower=0, upper=1> y;  // esiti (0/1)\n  vector[N] x;              // predittore\n}\nparameters {\n  real alpha;               // intercetta\n  real beta;                // coefficiente di regressione\n}\nmodel {\n  // prior deboli\n  alpha ~ normal(0, 2.5);\n  beta ~ normal(0, 2.5);\n  \n  // verosimiglianza\n  y ~ bernoulli_logit(alpha + beta * x);\n}\ngenerated quantities {\n  real OR = exp(beta);      // odds ratio\n}\n'\n```\n:::\n\nQuesto modello assume gli stessi dati simulati nell’esempio precedente. Prepariamo i dati in R:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstan_data <- list(\n  N = nrow(df),\n  y = df$Y,\n  x = df$X\n)\n```\n:::\n\nCompiliamo e stimiamo:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod <- cmdstan_model(write_stan_file(stan_code))\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- mod$sample(\n  data = stan_data,\n  seed = 123,\n  chains = 4, parallel_chains = 4,\n  iter_warmup = 1000, iter_sampling = 2000\n)\n```\n:::\n\n\n### Risultati\n\nEsaminiamo ora i coefficienti stimati dal modello bayesiano:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit$summary(variables = c(\"alpha\",\"beta\",\"OR\"))\n#> # A tibble: 3 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 alpha    -1.721 -1.716 0.171 0.170 -2.012 -1.447 1.003 2673.568 2761.536\n#> 2 beta      0.901  0.899 0.062 0.062  0.802  1.006 1.004 2687.550 2860.012\n#> 3 OR        2.467  2.457 0.154 0.153  2.231  2.735 1.003 2687.550 2860.012\n```\n:::\n\nIn uscita otteniamo per ciascun parametro la media posteriore, la deviazione standard e un intervallo credibile. Ad esempio, il coefficiente $\\beta$ ha un intervallo interamente positivo, coerente con la costruzione dei dati, e l’odds ratio risulta ben al di sopra di 1: segno che all’aumentare di $X$ cresce la probabilità di successo.\n\n\n### Interpretazione sulle tre scale\n\nLe distribuzioni posteriori di $\\alpha$ e $\\beta$ possono essere tradotte nelle tre scale discusse in precedenza.\n\n*Scala dei logit.*\nL’intercetta $\\alpha$ rappresenta i log-odds quando $x=0$. Supponiamo, ad esempio, che la posteriore dia $\\alpha \\approx -2$ con CrI 95% \\[-2.2, -1.8]. Significa che al livello di riferimento la probabilità è bassa. Il coefficiente $\\beta$, centrato intorno a 1 con CrI 95% \\[0.9, 1.1], indica che ogni unità in più di $x$ aumenta i log-odds di circa un punto.\n\n*Scala degli odds.*\nEsponenziando $\\beta$ otteniamo l’odds ratio. Con $\\beta \\approx 1$, la distribuzione posteriore di $OR$ è centrata su 2.7, con CrI ad esempio \\[2.5, 3.0]. Questo significa che, con altissima probabilità, un incremento unitario di $x$ moltiplica gli odds di successo di circa 2.5–3 volte.\n\n*Scala delle probabilità.*\nApplicando la trasformazione logistica, otteniamo $p(x) = \\text{logit}^{-1}(\\alpha + \\beta x)$. Per ogni draw posteriore possiamo calcolare una curva sigmoide: ne risulta un ventaglio di curve plausibili che descrivono l’incertezza. Intorno a $p=0.5$, la pendenza è massima e vale circa $\\beta/4$. Con $\\beta \\approx 1$, ciò corrisponde a un incremento massimo di probabilità di circa 25 punti percentuali per unità di $x$.\n\n\n### Quantità derivate: RD, OR, RR\n\nPossiamo anche confrontare due valori specifici del predittore, ad esempio $x=0$ e $x=1$, e derivare da ciascun draw posteriore tre misure di interesse:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Estrazione dei campioni posteriori\npost <- as_draws_df(fit)\n\n# Confronto tra x=0 e x=1\nx_a <- 0\nx_b <- 1\n\npost <- post %>%\n  mutate(\n    p_a = plogis(alpha + beta * x_a),\n    p_b = plogis(alpha + beta * x_b),\n    RD  = p_b - p_a,            \n    OR  = exp(beta * (x_b - x_a)),\n    RR  = p_b / p_a\n  )\n\nposterior_summary <- tibble(\n  quantity = c(\"p_a (x=0)\", \"p_b (x=1)\", \"RD (p_b - p_a)\", \"OR\", \"RR\"),\n  mean     = c(mean(post$p_a), mean(post$p_b),\n               mean(post$RD), mean(post$OR), mean(post$RR)),\n  q2.5     = c(quantile(post$p_a, .025), quantile(post$p_b, .025),\n               quantile(post$RD, .025), quantile(post$OR, .025), quantile(post$RR, .025)),\n  q97.5    = c(quantile(post$p_a, .975), quantile(post$p_b, .975),\n               quantile(post$RD, .975), quantile(post$OR, .975), quantile(post$RR, .975))\n)\n\nposterior_summary\n#> # A tibble: 5 × 4\n#>   quantity        mean  q2.5 q97.5\n#>   <chr>          <dbl> <dbl> <dbl>\n#> 1 p_a (x=0)      0.153 0.112 0.198\n#> 2 p_b (x=1)      0.307 0.255 0.359\n#> 3 RD (p_b - p_a) 0.153 0.137 0.170\n#> 4 OR             2.47  2.20  2.80 \n#> 5 RR             2.02  1.79  2.32\n```\n:::\n\nQui $p_a$ e $p_b$ sono le probabilità predette per i due valori di $x$, la *risk difference* è la loro differenza assoluta, l’*odds ratio* corrisponde a $e^\\beta$, e il *risk ratio* è il rapporto fra probabilità. Tutte queste quantità hanno ora distribuzioni posteriori con i rispettivi intervalli credibili.\n\n\n### Visualizzazione dell’incertezza\n\nPer rendere più chiara la variabilità delle predizioni, possiamo tracciare alcune curve campionate dalla posteriore, insieme alla curva media:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx_grid <- seq(0, 9, length.out = 100)\n\npred_curves <- post %>%\n  slice_sample(n = 200) %>%\n  mutate(.draw = row_number()) %>%\n  expand_grid(x = x_grid) %>%\n  mutate(p = plogis(alpha + beta * x))\n\npred_mean <- post %>%\n  expand_grid(x = x_grid) %>%\n  group_by(x) %>%\n  summarise(p = mean(plogis(alpha + beta * x)), .groups = \"drop\")\n\nggplot() +\n  geom_line(data = pred_curves, aes(x = x, y = p, group = .draw),\n            alpha = 0.1, color = \"grey\") +\n  geom_line(data = pred_mean, aes(x = x, y = p),\n            color = \"black\", size = 1) +\n  labs(\n    x = \"X\", y = \"Probabilità stimata\"\n  )\n```\n\n::: {.cell-output-display}\n![](01_logistic_regr_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=85%}\n:::\n:::\n\nIl grafico mostra come i dati sostengano un’intera famiglia di curve logistiche compatibili: la linea nera è la media posteriore, mentre le linee grigie rendono visibile l’incertezza.\n\n\n### Sintesi\n\nRispetto a `glm()`, l’approccio bayesiano con Stan fornisce un quadro più ricco e trasparente. Non abbiamo solo un punto stima e un errore standard, ma distribuzioni posteriori per tutti i parametri e le quantità derivate. Ciò ci permette di dire, ad esempio, che *con il 95% di probabilità posteriore l’odds ratio si colloca tra 2.5 e 3.0*, o che *l’incremento massimo di probabilità per unità di $x$ è di circa 25 punti percentuali*.\n\nIn sintesi, la regressione logistica bayesiana non solo replica quanto già visto con l’approccio frequentista, ma lo arricchisce con una rappresentazione completa dell’incertezza e con inferenze direttamente interpretabili in termini probabilistici.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n\n\nTable: Riassunto delle stime posteriori sulle tre scale: logit (α, β), odds (OR), probabilità (p ai due livelli scelti, RD e RR), con intervalli credibili al 95%.\n\n|Scala       |Quantità                               |     Media [CrI 95%]     |\n|:-----------|:--------------------------------------|:-----------------------:|\n|Logit       |α (log-odds a x=0)                     | -1.721 [-2.068, -1.399] |\n|Logit       |β (incremento di log-odds per +1 in X) |  0.901 [0.787, 1.031]   |\n|Odds        |OR = exp(β) per +1 in X                |  2.467 [2.196, 2.804]   |\n|Probabilità |p(x=0)                                 |  0.153 [0.112, 0.198]   |\n|Probabilità |p(x=1)                                 |  0.307 [0.255, 0.359]   |\n|Probabilità |RD = p(x=1) - p(x=0)                   |  0.153 [0.137, 0.170]   |\n|Probabilità |RR = p(x=1) / p(x=0)                   |  2.020 [1.787, 2.320]   |\n|Probabilità |Pendenza massima ≈ β/4 (a p≈0.5)       |  0.225 [0.197, 0.258]   |\n\n\n:::\n:::\n\n\n\n\n## Collegamento con il caso a due gruppi\n\nSe il predittore $x$ è una variabile dummy, che assume valore 0 in un gruppo e 1 nell’altro, il modello con predittore continuo si riduce esattamente al caso del confronto tra due proporzioni discusso nel capitolo successivo. In quel contesto avevamo:\n\n$$\np_{\\text{ref}} = \\text{logit}^{-1}(\\alpha), \\qquad\np_{\\text{work}} = \\text{logit}^{-1}(\\alpha + \\gamma),\n$$\nda cui derivano naturalmente  \n\n$$\nRD = p_{\\text{work}} - p_{\\text{ref}}, \\qquad\nOR = \\exp(\\gamma), \\qquad\nRR = \\frac{p_{\\text{work}}}{p_{\\text{ref}}}.\n$$\nIl confronto fra due gruppi, dunque, non è un modello separato ma un’applicazione particolare della regressione logistica generale. Questo ponte concettuale è importante perché mostra come il caso elementare delle due proporzioni si inserisca nello stesso quadro teorico della regressione logistica con predittori continui. Questo collegamento prepara il terreno per il prossimo capitolo, dove il confronto fra due proporzioni sarà analizzato in dettaglio.\n\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nIn questo capitolo abbiamo introdotto la *regressione logistica* come estensione naturale del modello lineare ai casi in cui l’esito è dicotomico. Abbiamo visto come il passaggio dal parametro probabilistico al logit consenta di mantenere la struttura lineare del modello, pur rispettando i vincoli logici delle probabilità comprese tra 0 e 1.\n\nLa stima bayesiana dei coefficienti, implementata in *Stan*, ci ha permesso di ottenere una rappresentazione completa dell’incertezza sulle relazioni tra predittori ed esiti. Abbiamo imparato a interpretare i coefficienti su scale diverse — probabilità, odds ratio, risk ratio — cogliendo così la flessibilità del modello e la sua capacità di adattarsi a molteplici domande di ricerca.\n\nLa regressione logistica è particolarmente rilevante in psicologia, dove esiti binari o dicotomici sono frequenti: dalle risposte corrette/errate in compiti cognitivi, alla presenza o assenza di sintomi clinici, fino alle scelte tra due alternative in contesti decisionali. Comprendere questo modello significa quindi dotarsi di uno strumento fondamentale, capace di connettere le nostre ipotesi teoriche con la natura concreta dei dati raccolti.\n\nMa la regressione logistica rappresenta anche un punto di partenza. Essa appartiene alla più ampia famiglia dei *modelli lineari generalizzati (GLM)*, che consentono di modellare variabili di natura diversa attraverso la combinazione di una distribuzione della famiglia esponenziale e di una funzione di collegamento (*link*). Nei prossimi capitoli esploreremo altri casi di GLM, consolidando così l’idea che regressione lineare, regressione logistica e ANOVA non siano strumenti separati, ma variazioni di un unico quadro metodologico, flessibile e coerente.\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] knitr_1.50            glue_1.8.0            insight_1.4.2        \n#>  [4] bayestestR_0.17.0     cmdstanr_0.9.0        pillar_1.11.0        \n#>  [7] tinytable_0.13.0      patchwork_1.3.2       ggdist_3.3.3         \n#> [10] tidybayes_3.0.7       bayesplot_1.14.0      ggplot2_3.5.2        \n#> [13] reliabilitydiag_0.2.1 priorsense_1.1.1      posterior_1.6.1      \n#> [16] loo_2.8.0             rstan_2.32.7          StanHeaders_2.32.10  \n#> [19] brms_2.22.0           Rcpp_1.1.0            sessioninfo_1.2.3    \n#> [22] conflicted_1.2.0      janitor_2.2.1         matrixStats_1.5.0    \n#> [25] modelr_0.1.11         tibble_3.3.0          dplyr_1.1.4          \n#> [28] tidyr_1.3.1           rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#> [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#> [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#> [16] labeling_0.4.3        utf8_1.2.6            rmarkdown_2.29       \n#> [19] ps_1.9.1              ragg_1.5.0            purrr_1.1.0          \n#> [22] xfun_0.53             cachem_1.1.0          jsonlite_2.0.0       \n#> [25] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#> [28] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      \n#> [31] estimability_1.5.1    zoo_1.8-14            pacman_0.5.1         \n#> [34] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#> [37] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#> [40] codetools_0.2-20      curl_7.0.0            processx_3.8.6       \n#> [43] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#> [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [49] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [52] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [55] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [58] scales_1.4.0          xtable_1.8-4          emmeans_1.11.2-8     \n#> [61] tools_4.5.1           data.table_1.17.8     mvtnorm_1.3-3        \n#> [64] grid_4.5.1            QuickJSR_1.8.0        colorspace_2.1-1     \n#> [67] nlme_3.1-168          cli_3.6.5             textshaping_1.0.3    \n#> [70] svUnit_1.0.8          Brobdingnag_1.2-9     V8_7.0.0             \n#> [73] gtable_0.3.6          digest_0.6.37         TH.data_1.1-4        \n#> [76] htmlwidgets_1.6.4     farver_2.1.2          memoise_2.0.1        \n#> [79] htmltools_0.5.8.1     lifecycle_1.0.4       MASS_7.3-65\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n",
    "supporting": [
      "01_logistic_regr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}