{
  "hash": "a771390fd548883b80afb4388c38a96c",
  "result": {
    "engine": "knitr",
    "markdown": "# La divergenza di Kullback-Leibler {#sec-kullback-leibler-divergence}\n\n## Introduzione {.unnumbered .unlisted}\n\nNel capitolo precedente abbiamo introdotto l’*entropia* come misura\ndell’incertezza di una distribuzione di probabilità. Ora facciamo un passo avanti: invece di misurare l’incertezza *di una sola distribuzione*, vogliamo misurare *quanto una distribuzione differisce da un’altra*. Uno strumento cruciale per rispondere a questa domanda è la divergenza di Kullback-Leibler [@kullback1951information], spesso abbreviata come divergenza KL ($D_{\\text{KL}}$). Essa misura quanto si perde in precisione o efficienza se si utilizza un modello errato per descrivere la realtà.\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Cos’è la divergenza KL e da dove nasce.\n- Come si collega al concetto di entropia.\n- Perché è utile nella scelta tra modelli statistici.\n- Come calcolarla e interpretarla, anche con esempi in R.\n\n::: {.callout-tip collapse=true title=\"Prerequisiti\"}\n- Per comprendere appieno questo capitolo, dovresti aver già appreso i concetti di *entropia e informazione di Shannon* ([@sec-entropy-shannon-information]).\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Funzione per il calcolo dei termini della divergenza KL\nkl_terms <- function(p, q) {\n  stopifnot(length(p) == length(q))\n  non_zero <- p > 0 & q > 0\n  p <- p[non_zero]\n  q <- q[non_zero]\n  term <- p * log2(p / q)\n  data.frame(x = seq_along(p), p = p, q = q, term = term)\n}\n\n# Funzione compatta per il valore totale\nkl_divergence <- function(p, q) {\n  sum(kl_terms(p, q)$term)\n}\n\n# Entropia vera (in bit)\nentropy <- function(p) {\n  p <- p[p > 0]\n  -sum(p * log2(p))\n}\n\n# Entropia incrociata (in bit)\ncross_entropy <- function(p, q) {\n  non_zero <- p > 0 & q > 0\n  p <- p[non_zero]\n  q <- q[non_zero]\n  -sum(p * log2(q))\n}\n```\n:::\n\n:::\n\n\n## La generalizzabilità dei modelli e il metodo scientifico\n\nUno degli obiettivi fondamentali della scienza è la *generalizzabilità*: un buon modello non deve spiegare solo i dati che abbiamo già, ma anche prevedere correttamente nuovi dati che potremmo raccogliere in futuro. Un modello troppo semplice rischia di *sotto-adattarsi* ai dati (*underfitting*), perdendo informazioni importanti; uno troppo complesso rischia di *sovra-adattarsi* (*overfitting*), confondendo il rumore casuale con segnali reali. Il problema della generalizzabilità è quindi centrale nel metodo scientifico: vogliamo modelli abbastanza flessibili da catturare i pattern reali, ma non così flessibili da adattarsi anche a variazioni casuali.\n\nNell’approccio bayesiano, come osserva @McElreath_rethinking, la scelta di un modello implica trovare un equilibrio tra due esigenze:\n\n1. *accuratezza predittiva* – il modello deve produrre previsioni affidabili sui dati futuri;\n2. *controllo della complessità* – il modello non deve introdurre più complessità di quanta ne richieda il fenomeno studiato.\n\nQuesto principio è vicino a quello noto come *rasoio di Occam*: tra due modelli che spiegano altrettanto bene i dati, preferiamo quello più semplice. La differenza è che, in ambito bayesiano, questa preferenza non è solo una regola intuitiva, ma può essere formalizzata in termini quantitativi, misurando quanta “informazione in più” dobbiamo spendere quando il nostro modello si discosta dalla realtà. Questa misura è data dalla *divergenza di Kullback–Leibler*, che vedremo nel seguito.\n\n\n## L'entropia relativa\n\nNel @sec-entropy-shannon-information abbiamo visto che l’*entropia* $H(P)$ misura la lunghezza media del codice più efficiente per descrivere una distribuzione di probabilità $P$. Ora estendiamo il ragionamento al confronto tra due distribuzioni:\n\n* $P$ = distribuzione *vera* dei dati, cioè quella che genera realmente gli eventi;\n* $Q$ = distribuzione *approssimata*, cioè quella fornita dal modello.\n\nLa *divergenza di Kullback–Leibler*, $D_{\\text{KL}}(P \\parallel Q)$, risponde alla seguente domanda:\n\n> in media, quanta informazione in più dobbiamo spendere se usiamo $Q$ invece di $P$ per descrivere i dati?\n\nDal punto di vista della codifica, questa quantità rappresenta l’aumento medio della lunghezza del codice quando si usa un modello impreciso.\n\n\n### Definizione formale\n\nPer una variabile casuale discreta $X$:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = \\sum_x p(x) \\log_2 \\frac{p(x)}{q(x)}\n$$ {#eq-kl-div-def}\n\nche può essere riscritta come:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = \\sum_x p(x) \\left[ \\log_2 p(x) - \\log_2 q(x) \\right].\n$$ {#eq-kl-div-def2}\n\nQuesta forma mette in evidenza un’interpretazione intuitiva:\n\n* $\\log_2 p(x)$ è l’*informazione* (in bit) associata all’esito $x$ secondo la distribuzione vera $P$;\n* $\\log_2 q(x)$ è l’*informazione* associata allo stesso esito secondo il modello $Q$;\n* la differenza $\\log_2 p(x) - \\log_2 q(x)$ indica, per quell’esito, quanto il modello $Q$ sottostima o sovrastima la sorpresa rispetto a $P$;\n* moltiplicando per $p(x)$ e sommando su tutti gli esiti otteniamo *una media ponderata* (pesata in base a quanto l’esito è probabile nella realtà).\n\nIn sintesi, $D_{\\text{KL}}(P \\parallel Q)$ è *la perdita media di efficienza* quando descriviamo la variabile $X$ con la distribuzione approssimata $Q$ invece che con la distribuzione vera $P$.\n\nSe $P = Q$ la divergenza è 0, perché non vi è alcuna perdita. Quanto più $Q$ si discosta da $P$, tanto più grande sarà la divergenza, segnalando un “costo informativo” maggiore.\n\n\n::: {.callout-note collapse=\"true\" title=\"Esempio: Divergenza KL (1)\"}\nSupponiamo che la variabile casuale $X$ possa assumere tre valori: A, B e C.\n\nLa *distribuzione vera* ($P$) è:\n\n| x | $p(x)$ |\n| - | -------- |\n| A | 0.5      |\n| B | 0.3      |\n| C | 0.2      |\n\nIl *modello approssimante* ($Q$) è:\n\n| x | $q(x)$ |\n| - | -------- |\n| A | 0.4      |\n| B | 0.4      |\n| C | 0.2      |\n\nCalcoliamo la divergenza KL:\n\n$$\n\\begin{aligned}\nD_{\\text{KL}}(P \\parallel Q) &= 0.5 \\log_2\\!\\left(\\frac{0.5}{0.4}\\right) \n+ 0.3 \\log_2\\!\\left(\\frac{0.3}{0.4}\\right) \n+ 0.2 \\log_2\\!\\left(\\frac{0.2}{0.2}\\right) \\\\[4pt]\n&= 0.5 \\log_2(1.25) + 0.3 \\log_2(0.75) + 0.2 \\log_2(1) \\\\[4pt]\n&\\approx 0.160 - 0.125 + 0 \\\\[4pt]\n&= 0.035 \\ \\text{bit}.\n\\end{aligned}\n$$\n\n**Interpretazione**\n\n* Per *A*, il modello $Q$ sottostima la probabilità vera (0.4 invece di 0.5). Questo comporta un costo informativo positivo: il codice dovrà essere leggermente più lungo rispetto all’uso di $P$.\n* Per *B*, il modello $Q$ sovrastima la probabilità vera (0.4 invece di 0.3). Qui il costo informativo è negativo, ma va pesato dal fatto che nella divergenza KL la somma è *pesata secondo $P$*, e dunque conta di più la stima errata sugli eventi più probabili.\n* Per *C*, il modello è perfetto ($p(x) = q(x)$) e il contributo alla divergenza è nullo.\n\nIl risultato complessivo, *0.035 bit per evento*, è molto piccolo: significa che, in media, usando $Q$ al posto di $P$ spenderemmo appena 0.035 bit di informazione in più per descrivere ogni osservazione. Le due distribuzioni sono quindi molto simili, ma la divergenza KL rileva comunque la differenza residua.\n:::\n\n\n::: {.callout-note collapse=\"true\" title=\"Esempio: Divergenza KL (2)\"}\nSupponiamo che la variabile casuale $X$ possa assumere tre valori: `x = 1, 2, 3`.\n\n* **Distribuzione vera** ($P$): $[0.1, \\ 0.6, \\ 0.3]$\n* **Distribuzione approssimata** ($Q$): $[0.2, \\ 0.5, \\ 0.3]$\n\nCalcoliamo la divergenza KL secondo la formula @eq-kl-def:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Definizione delle distribuzioni\nP <- c(0.1, 0.6, 0.3)  # distribuzione vera\nQ <- c(0.2, 0.5, 0.3)  # distribuzione approssimata\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo dei contributi per ciascun esito\ndf_kl_terms <- kl_terms(P, Q)\nprint(df_kl_terms)\n#>   x   p   q   term\n#> 1 1 0.1 0.2 -0.100\n#> 2 2 0.6 0.5  0.158\n#> 3 3 0.3 0.3  0.000\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Visualizzazione dei contributi\nggplot(df_kl_terms, aes(x = factor(x), y = term)) +\n  geom_col(fill = \"steelblue\") +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.3) +\n  labs(\n    x = \"Valori possibili di X\",\n    y = \"Contributo alla Divergenza KL\",\n    title = \"Contributo di ciascun esito alla Divergenza KL\"\n  )\n```\n\n::: {.cell-output-display}\n![](02_kl_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nInfine, sommiamo i contributi per ottenere la divergenza totale:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nKL_total <- sum(df_kl_terms$term)\ncat(sprintf(\"Divergenza KL da P a Q: %.4f bit\\n\", KL_total))\n#> Divergenza KL da P a Q: 0.0578 bit\n```\n:::\n\n\n**Interpretazione**\n\n* **Esito 1** ($p=0.1$, $q=0.2$) – Il modello $Q$ *sovrastima* un evento raro. Il contributo alla divergenza è negativo, ma l’impatto è ridotto perché l’evento è poco probabile nella realtà ($p$ piccolo).\n* **Esito 2** ($p=0.6$, $q=0.5$) – Il modello *sottostima* l’evento più frequente. Poiché $p$ è alto, questa sottostima ha un peso maggiore nella media ponderata, generando il contributo positivo più grande.\n* **Esito 3** ($p=0.3$, $q=0.3$) – Qui il modello è perfetto: $p(x) = q(x)$, quindi il contributo alla divergenza è zero.\n\nIl valore complessivo di $D_{\\text{KL}}$ è la somma di questi contributi: rappresenta la *perdita media di efficienza* (in bit per evento) quando si usa $Q$ al posto di $P$.\n\nIn questo caso, il risultato indica che usare $Q$ comporta una leggera inefficienza: la codifica o le previsioni richiedono, in media, un po’ più informazione di quanto sarebbe necessario usando la distribuzione vera.\n:::\n\n\n### Legame con l’entropia e l’entropia incrociata\n\nLa divergenza di Kullback–Leibler può essere riscritta come differenza tra *entropia incrociata* e *entropia vera*:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = H(P, Q) - H(P), \n$$ {#eq-kl-difference}\n\ndove:\n\n- $H(P)$ è l’entropia della distribuzione vera $P$ (incertezza media/lunghezza media del codice ottimale quando conosciamo la distribuzione corretta);\n- $H(P, Q)$ è l’*entropia incrociata*, cioè l’incertezza media *se* codifichiamo dati generati da $P$ utilizzando un codice ottimizzato per $Q$:\n  \n$$\nH(P, Q) = -\\sum_x p(x)\\log_2 q(x).\n$$ {#eq-cross-entropy}\n\n**Intuizione.** Con questa forma, $D_{\\text{KL}}$ è la *sorpresa extra media* (o *costo informativo* in bit per evento) che paghiamo quando usiamo il modello approssimato $Q$ al posto della distribuzione vera $P$. Poiché $H(P)$ non dipende dal modello, *minimizzare $D_{\\text{KL}}$ equivale a minimizzare $H(P,Q)$*.\n\n::: callout-note\n#### Perché serve per ELPD e LOO\n\nCriteri predittivi come *ELPD* e *LOO* stimano, in media, la stessa quantità di cui vogliamo minimizzare il valore: l’*entropia incrociata* $H(P,Q)$. Per questo, massimizzare ELPD (o ridurre la perdita di log-verosimiglianza predittiva) è un modo pratico per *avvicinare $Q$ a $P$*, ossia per ridurre indirettamente $D_{\\text{KL}}(P\\parallel Q)$.\n:::\n\n\n::: {.callout-note collapse=\"true\" title=\"Esempio: Entropia incrociata (1)\"}\n\nUtilizziamo le funzioni definite sopra (`entropy()`, `cross_entropy()`, `kl_divergence()`) sullo stesso esempio discusso in precedenza:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Esempio: distribuzione vera P e modello Q\nP <- c(0.1, 0.6, 0.3)\nQ <- c(0.2, 0.5, 0.3)\n\nH_P   <- entropy(P)           # H(P)\nH_PQ  <- cross_entropy(P, Q)  # H(P,Q)\nDKL   <- kl_divergence(P, Q)  # D_KL(P||Q)\n\ncat(sprintf(\"H(P)    = %.4f bit\\n\", H_P))\n#> H(P)    = 1.2955 bit\ncat(sprintf(\"H(P,Q)  = %.4f bit\\n\", H_PQ))\n#> H(P,Q)  = 1.3533 bit\ncat(sprintf(\"H(P,Q)-H(P) = %.4f bit (D_KL)\\n\", H_PQ - H_P))\n#> H(P,Q)-H(P) = 0.0578 bit (D_KL)\ncat(sprintf(\"D_KL(P||Q)  = %.4f bit (controllo)\\n\", DKL))\n#> D_KL(P||Q)  = 0.0578 bit (controllo)\n```\n:::\n\n\n**Interpretazione**  \n\n- $H(P)$ è il limite inferiore: la miglior compressione ottenibile conoscendo la verità ($P$).  \n- $H(P,Q)$ è la compressione che otterremmo usando il modello ($Q$).  \n- La loro differenza è *esattamente* $D_{\\text{KL}}(P\\parallel Q)$: la quantità di informazione “sprecata” in media per evento usando $Q$ al posto di $P$.  \n:::\n\n::: {.callout-note collapse=\"true\" title=\"Esempio: Entropia incrociata (2)\"}\nIn due esempi successivi rendiamo $Q$ sempre più diverso da $P$ e osserviamo come cambiano *entropia incrociata* e *divergenza KL*.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Distribuzione vera fissata\nP  <- c(0.1, 0.6, 0.3)\nH_P <- entropy(P)  # costante rispetto al modello\n\n# Due modelli: uno moderatamente errato (Q1), uno molto errato (Q2)\nQ1 <- c(0.35, 0.30, 0.35)\nQ2 <- c(0.60, 0.30, 0.10)\n\n# Calcolo di entropia incrociata e divergenza KL\nH_PQ1 <- cross_entropy(P, Q1)\nH_PQ2 <- cross_entropy(P, Q2)\n\nKL1 <- kl_divergence(P, Q1)\nKL2 <- kl_divergence(P, Q2)\n\ncat(sprintf(\"H(P)     = %.4f bit (fissa)\\n\", H_P))\n#> H(P)     = 1.2955 bit (fissa)\ncat(sprintf(\"H(P,Q1)  = %.4f bit   -> D_KL(P||Q1) = %.4f bit\\n\", H_PQ1, KL1))\n#> H(P,Q1)  = 1.6480 bit   -> D_KL(P||Q1) = 0.3525 bit\ncat(sprintf(\"H(P,Q2)  = %.4f bit   -> D_KL(P||Q2) = %.4f bit\\n\", H_PQ2, KL2))\n#> H(P,Q2)  = 2.1125 bit   -> D_KL(P||Q2) = 0.8170 bit\n```\n:::\n\n\n**Interpretazione**\n\nPoiché $H(P)$ non cambia, quando $Q$ si allontana da $P$ cresce $H(P,Q)$ e, di conseguenza, aumenta\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = H(P, Q) - H(P) .\n$$\n\n* Q1: il modello redistribuisce massa probabilistica, sottostimando l’esito più probabile e sovrastimando gli altri. Gli errori sugli esiti che $P$ considera frequenti pesano di più nella media, aumentando $H(P,Q1)$ e quindi $D_{\\text{KL}}$.\n* Q2: l’errore è estremo: la probabilità più alta viene assegnata all’esito meno probabile secondo $P$. I contributi positivi (sottostima degli esiti comuni) dominano, facendo crescere molto $D_{\\text{KL}}$.\n\nQuesto esempio mostra che minimizzare $H(P,Q)$ (e quindi $D_{\\text{KL}}$) significa allineare il più possibile le probabilità del modello con quelle “vere”, soprattutto per gli esiti a cui $P$ assegna più massa.\n:::\n\n\n::: {.callout-note collapse=\"true\" title=\"Dimostrazione: dalla differenza di entropie alla formula della D-KL\"}\nPartiamo dalla definizione come differenza tra entropia incrociata ed entropia vera:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = H(P, Q) - H(P).\n$$\n\nSostituendo:\n$$\nH(P,Q) = -\\sum_x p(x) \\log_2 q(x), \\quad\nH(P)   = -\\sum_x p(x) \\log_2 p(x),\n$$\n\nottieni:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) =\n\\left[ - \\sum_x p(x) \\log_2 q(x) \\right]\n- \\left[ - \\sum_x p(x) \\log_2 p(x) \\right].\n$$\n\nEliminando i segni negativi:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) =\n\\sum_x p(x) \\log_2 p(x) - \\sum_x p(x) \\log_2 q(x).\n$$\n\nRaccogliendo in un’unica somma:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) =\n\\sum_x p(x) \\left[ \\log_2 p(x) - \\log_2 q(x) \\right].\n$$\n\nApplicando la proprietà dei logaritmi:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) =\n\\sum_x p(x) \\log_2 \\frac{p(x)}{q(x)}.\n$$\n\n**Interpretazione:** questa è la forma esplicita più usata della $D_{\\text{KL}}$. Mostra chiaramente che si tratta di *una media ponderata secondo $P$* della differenza di informazione tra $P$ e $Q$ per ciascun esito $x$.\n:::\n\n\n### Interpretazione della divergenza KL\n\nLa divergenza $D_{\\text{KL}}(P \\parallel Q)$ misura *l’inefficienza media* che si introduce quando si usa la distribuzione $Q$ per descrivere dati che in realtà seguono $P$. In termini informativi, rappresenta il *costo aggiuntivo di sorpresa*: quanti bit in più, in media, servono per codificare gli eventi generati da $P$ se utilizziamo un codice ottimizzato per $Q$ invece che per $P$.\n\nQuesta quantità:\n\n* è *sempre non negativa*: il modello vero ($P$) non può mai essere peggiore, in media, del modello approssimato ($Q$);\n* è *asimmetrica*: $D_{\\text{KL}}(P \\parallel Q) \\neq D\\_{\\text{KL}}(Q \\parallel P)$. L’ordine è importante: invertire $P$ e $Q$ cambia il significato della misura, perché cambia quale distribuzione stiamo trattando come “vera”.\n\nPer questo motivo, la divergenza KL non è una “distanza” in senso geometrico, ma *una misura direzionale di perdita di informazione* o di inefficienza di codifica.\n\n\n### Proprietà fondamentali della divergenza KL\n\n* **Non-negatività:** $D_{\\text{KL}}(P \\parallel Q) \\geq 0$ per ogni coppia di distribuzioni $P$ e $Q$. Il valore minimo (0) si ottiene se e solo se $P = Q$.\n* **Asimmetria:** $D_{\\text{KL}}(P \\parallel Q) \\neq D\\_{\\text{KL}}(Q \\parallel P)$ in generale. Non soddisfa quindi le proprietà di una distanza simmetrica.\n* **Unità di misura:** dipende dalla base del logaritmo:\n\n  * base 2 → misura in *bit*;\n  * base $e$ → misura in *nat* (unità naturale di informazione).\n\n\n## Uso della divergenza $D_{\\text{KL}}$ nella selezione di modelli\n\nIn teoria, la selezione del modello consiste nello scegliere il modello $Q$ che *minimizza la divergenza dalla distribuzione vera* $P$:\n\n$$\n\\text{Modello ottimale} = \\arg\\min_Q D_{\\text{KL}}(P \\parallel Q).\n$$\n\nIn altre parole, il modello ideale è quello che si avvicina di più a $P$ e quindi riduce al minimo la perdita media di informazione quando lo usiamo per descrivere i dati.\n\n**Problema:** nella pratica, *$P$ è sconosciuta* — non possiamo osservare direttamente la distribuzione vera che ha generato i dati. Di conseguenza, non possiamo calcolare $D_{\\text{KL}}$ in modo esatto.\n\n\n### Come procedere nella pratica\n\nAnche se $P$ è ignota, possiamo comunque *confrontare* modelli in termini di divergenza KL sfruttando il legame con l’entropia incrociata $H(P,Q)$. Infatti, ricordiamo che:\n\n$$\nD_{\\text{KL}}(P \\parallel Q) = H(P,Q) - H(P).\n$$\n\nL’entropia $H(P)$ non dipende dal modello $Q$: è una *costante* rispetto al confronto tra modelli. Se prendiamo la differenza di divergenza KL tra due modelli $Q_1$ e $Q_2$, questa costante si *annulla*:\n\n$$\nD_{\\text{KL}}(P \\parallel Q_1) - D_{\\text{KL}}(P \\parallel Q_2) \n= H(P,Q_1) - H(P,Q_2).\n$$ {#eq-div-kl-models-comparison}\n\nQuindi, *per confrontare modelli non serve conoscere $H(P)$*: basta confrontare le loro entropie incrociate $H(P,Q)$, che dipendono solo da $Q$ e che possono essere *stimate dai dati*.\n\nNel prossimo capitolo vedremo due strumenti dell’approccio bayesiano che stimano proprio $H(P,Q)$ (o, più precisamente, il suo opposto $-H(P,Q)$):\n\n* **Leave-One-Out Cross-Validation (LOO-CV)** – valuta quanto bene il modello predice dati non usati nella stima;\n* **Expected Log Predictive Density (ELPD)** – fornisce la stima della qualità predittiva media del modello.\n\nQuesti metodi permettono di confrontare modelli in termini di *differenza di divergenza KL*, avvicinandoci così alla scelta del modello che, tra quelli considerati, è più vicino alla distribuzione vera $P$.\n\n\n::: {.callout-note collapse=\"true\" title=\"Esempio psicologico\"}\nImmaginiamo di voler prevedere il *punteggio di ansia settimanale* di uno studente.\n\n- **Modello A**: utilizza come predittore solo il punteggio di *coping* (capacità di fronteggiare lo stress).  \n- **Modello B**: utilizza *coping* + *supporto sociale*.\n\nSupponiamo che, valutando le loro prestazioni predittive, entrambi i modelli ottengano buoni risultati, ma il Modello B presenti una divergenza KL leggermente inferiore rispetto al Modello A.\n\n**Interpretazione:**\n\n- la divergenza KL più bassa del Modello B indica che, in media, le sue previsioni sono leggermente più vicine alla distribuzione “vera” dei dati (minore perdita di informazione); \n- tuttavia, se la differenza è piccola, potremmo preferire il Modello A per la sua *maggiore semplicità e interpretabilità*, applicando il *principio di parsimonia* (o *rasoio di Occam*).  \n\nQuesto esempio illustra che la selezione del modello non dipende solo dalla precisione predittiva, ma anche dal bilanciamento tra *accuratezza* e *complessità*.\n:::\n\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nIn questo capitolo abbiamo approfondito un concetto fondamentale della teoria dell’informazione: la *divergenza di Kullback–Leibler*. Nata in origine per valutare l’efficienza dei codici di trasmissione, la D-KL è oggi uno strumento essenziale anche nella statistica moderna, perché misura in modo preciso quanto una distribuzione di probabilità approssimata $Q$ (cioè un modello) si discosti dalla distribuzione vera $P$ che genera i dati.\n\nAbbiamo visto che la D-KL può essere interpretata come:\n\n* *perdita media di informazione* quando si usa $Q$ invece di $P$;\n* *eccesso di sorpresa* o inefficienza di codifica introdotta da un modello imperfetto;\n* differenza tra *entropia incrociata* e *entropia vera*, il che rende possibile stimarla indirettamente.\n\nQuesto legame con l’entropia incrociata è cruciale: sebbene $P$ non sia nota e la D-KL non possa essere calcolata in valore assoluto, possiamo confrontare modelli stimando le differenze di D-KL, perché la componente costante $H(P)$ si annulla nel confronto.\n\nNel prossimo capitolo ci concentreremo proprio su come effettuare questi confronti in pratica. Vedremo come strumenti come la *Leave-One-Out Cross-Validation (LOO-CV)* e l’*Expected Log Predictive Density (ELPD)* permettano di stimare la capacità predittiva dei modelli e di identificare quello che, tra le alternative considerate, è il più vicino alla distribuzione vera dei dati.\n\n\n::: {.callout-note}\n## Sintesi finale\n\n* La divergenza KL quantifica la perdita media di informazione usando $Q$ al posto di $P$.\n* Si può scrivere come $\\sum_x p(x) \\log \\frac{p(x)}{q(x)}$ o come $H(P,Q) - H(P)$.\n* È uno strumento chiave per valutare *quanto bene* un modello rappresenta la realtà.\n* In pratica, può essere confrontata tra modelli stimando $H(P,Q)$ con tecniche come *LOO-CV* ed *ELPD*.\n:::\n\n\n::: {.callout-important title=\"Problemi\" collapse=\"true\"}\n1. Cosideriamo due distribuzioni di probabilità discrete, $p$ e $q$:\n\n```\np <- c(0.2, 0.5, 0.3)\nq <- c(0.1, 0.2, 0.7)\n```\n\nSi calcoli l'entropia di $p$, l'entropia incrociata tra $p$ e $q$, la divergenza di Kullback-Leibler da $p$ a $q$.\n\nSi consideri `q = c(0.2, 0.55, 0.25)` e si calcoli di nuovo a divergenza di Kullback-Leibler da $p$ a $q$. Si confronti con il risultato precedente e si interpreti.\n\n2. Sia $p$ una distribuzione binomiale di parametri $\\theta = 0.2$ e $n = 5$. Sia $q_1$ una approssimazione a $p$: `q1 = c(0.46, 0.42, 0.10, 0.01, 0.01)`. Sia $q_2$ una distribuzione uniforme: `q2 <- rep(0.2, 5)`. Si calcoli la divergenza $\\mathbb{KL}$ di $q_1$ da $p$ e da $q_2$ da $p$ e si interpretino i risultati.\n:::\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] knitr_1.50            labeling_0.4.3        bridgesampling_1.1-2 \n#> [19] htmlwidgets_1.6.4     curl_7.0.0            pkgbuild_1.4.8       \n#> [22] RColorBrewer_1.1-3    abind_1.4-8           multcomp_1.4-28      \n#> [25] withr_3.0.2           purrr_1.1.0           grid_4.5.1           \n#> [28] stats4_4.5.1          colorspace_2.1-1      xtable_1.8-4         \n#> [31] inline_0.3.21         emmeans_1.11.2-8      scales_1.4.0         \n#> [34] MASS_7.3-65           cli_3.6.5             mvtnorm_1.3-3        \n#> [37] rmarkdown_2.29        ragg_1.5.0            generics_0.1.4       \n#> [40] RcppParallel_5.1.11-1 cachem_1.1.0          stringr_1.5.1        \n#> [43] splines_4.5.1         parallel_4.5.1        vctrs_0.6.5          \n#> [46] V8_7.0.0              Matrix_1.7-4          sandwich_3.1-1       \n#> [49] jsonlite_2.0.0        arrayhelpers_1.1-0    systemfonts_1.2.3    \n#> [52] glue_1.8.0            codetools_0.2-20      distributional_0.5.0 \n#> [55] lubridate_1.9.4       stringi_1.8.7         gtable_0.3.6         \n#> [58] QuickJSR_1.8.0        htmltools_0.5.8.1     Brobdingnag_1.2-9    \n#> [61] R6_2.6.1              textshaping_1.0.3     rprojroot_2.1.1      \n#> [64] evaluate_1.0.5        lattice_0.22-7        backports_1.5.0      \n#> [67] memoise_2.0.1         broom_1.0.9           snakecase_0.11.1     \n#> [70] rstantools_2.5.0      coda_0.19-4.1         gridExtra_2.3        \n#> [73] nlme_3.1-168          checkmate_2.3.3       xfun_0.53            \n#> [76] zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted} \n\n",
    "supporting": [
      "02_kl_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}