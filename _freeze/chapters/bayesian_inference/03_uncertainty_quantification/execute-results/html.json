{
  "hash": "7a740eba5ebc9d6adbf25b72358c984e",
  "result": {
    "engine": "knitr",
    "markdown": "# La quantificazione dell'incertezza {#sec-bayes-inference-intro}\n\n::: {.epigraph}\n\n> “The central problem of statistics is how to make decisions under uncertainty.”\n>\n> — **Leonard J. Savage**, The Foundations of Statistics\n:::\n\n## Introduzione {.unnumbered .unlisted}\n\nNel capitolo precedente abbiamo visto che l’incertezza è una componente inevitabile della ricerca psicologica. Abbiamo distinto diversi tipi di incertezza e abbiamo mostrato come l’approccio bayesiano offra un linguaggio per rappresentarla in modo esplicito.\n\nIn questo capitolo iniziamo a costruire gli strumenti tecnici di base del pensiero bayesiano. L’obiettivo non è ancora quello di entrare nei dettagli matematici, ma di sviluppare un’intuizione solida su alcuni concetti chiave:\n\n* *Credenze iniziali (priori)*: come rappresentare ciò che sappiamo o supponiamo prima di raccogliere i dati.\n* *Dati (likelihood)*: come descrivere formalmente l’informazione che proviene dall’osservazione empirica.\n* *Credenze aggiornate (posteriori)*: come combinare priori e dati per ottenere una descrizione più informata dell’incertezza.\n\nL’idea centrale è che il metodo bayesiano funziona come un processo di *aggiornamento coerente delle credenze*: partiamo da ciò che sappiamo, osserviamo i dati, e arriviamo a conclusioni più raffinate.\n\n::: {.callout-note}\n\n#### Intuizione\n\nIl pensiero bayesiano non elimina il dubbio, ma ci permette di trattarlo in modo sistematico: i dati non sostituiscono le nostre ipotesi, ma le trasformano.\n:::\n\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Come quantificare e rappresentare matematicamente l'incertezza attraverso le distribuzioni di densità.\n- Il processo di integrazione delle nuove evidenze con le conoscenze preesistenti.\n- Come i parametri sconosciuti determinano i dati osservati attraverso processi probabilistici. \n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere [Bayesian statistics for clinical research](https://www.sciencedirect.com/science/article/pii/S0140673624012959) di @Goligher2024.\n- Leggere [Dicing with the unknown](http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2004.00050.x/abstract) di Tony O'Hagan, per una descrizione chiara della distinzione tra incertezza aleatoria e incertezza epistemica.\n- Leggere il capitolo *Estimation* [@schervish2014probability].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n```\n:::\n\n:::\n\n\n## L’incertezza come distribuzione di probabilità\n\nFinora abbiamo parlato di incertezza in modo concettuale. Ora facciamo un passo in più: impariamo a *rappresentarla formalmente*.L’idea di base è che l’incertezza non si descrive con un singolo valore, ma con una *gamma di valori possibili*, a ciascuno dei quali attribuiamo un grado di plausibilità. Questo insieme di valori e plausibilità è ciò che chiamiamo *distribuzione di probabilità*.\n\n\n### Un esempio intuitivo\n\nSupponiamo di stimare il livello medio di ansia negli studenti prima di un esame.\n\n* Non sappiamo con certezza quale sia: potrebbe essere 2.8, oppure 3.1, o forse 3.5 su una scala da 1 a 5.\n* Alcuni valori ci sembrano più plausibili di altri. Per esempio, 3.0 è più verosimile di 1.0 o di 5.0.\n* Possiamo quindi rappresentare la nostra incertezza con una curva che assegna più “peso” ai valori plausibili e meno a quelli estremi.\n\n\n### Perché usare distribuzioni?\n\nUsare distribuzioni di probabilità per rappresentare l’incertezza ha due vantaggi fondamentali:\n\n1. *Trasparenza*: invece di fingere di conoscere un valore preciso, dichiariamo chiaramente *quanto* siamo incerti.\n2. *Flessibilità*: possiamo aggiornare la distribuzione quando raccogliamo nuovi dati, vedendo come la nostra rappresentazione dell’incertezza cambia.\n\n\n### Collegamento con la psicologia\n\nQuesta idea non vale solo per punteggi medi, ma per qualsiasi parametro di interesse psicologico:\n\n* la forza di un effetto terapeutico,\n* la relazione tra ansia e rendimento,\n* la probabilità che un soggetto scelga una certa opzione in un compito cognitivo.\n\nIn tutti questi casi, non possiamo mai dire “il valore vero è X”: possiamo solo attribuire una distribuzione di probabilità ai possibili valori.\n\n\n::: {.callout-note}\n\n#### Messaggio chiave\n\nRappresentare l’incertezza come una *distribuzione di probabilità* significa passare da una visione rigida (“il valore è uno solo”) a una visione più realistica e informativa (“alcuni valori sono più plausibili di altri”).\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Rappresentare l'incertezza sulla media dell'ansia come distribuzione di probabilità.](03_uncertainty_quantification_files/figure-html/fig-distribuzione-ansia-1.png){#fig-distribuzione-ansia fig-align='center' width=85%}\n:::\n:::\n\n\n### La natura dinamica dell’incertezza bayesiana\n\nUn aspetto fondamentale dell’approccio bayesiano è che l’incertezza non è statica. Non rimaniamo bloccati nella distribuzione iniziale che assegniamo ai valori possibili: ogni volta che osserviamo nuovi dati, *aggiorniamo la distribuzione*.\n\nIn questo modo, la rappresentazione dell’incertezza è *dinamica*: evolve con l’accumularsi delle informazioni.\n\n*Esempio psicologico*:\n\n* Prima di raccogliere i dati, possiamo pensare che la media dell’ansia degli studenti prima di un esame sia plausibilmente intorno a 3.\n* Dopo aver osservato le prime risposte, la distribuzione si sposta: se molti studenti riportano valori alti, la parte destra della curva diventa più plausibile.\n* Se in seguito raccogliamo ancora più dati, la curva si restringe, riflettendo una maggiore precisione nelle nostre stime.\n\n::: {.callout-note}\n\n#### Messaggio chiave\n\nNel pensiero bayesiano, l’incertezza è *viva*: si muove, si aggiorna e si affina ogni volta che impariamo qualcosa di nuovo dai dati.\n:::\n\n\n## Le fondamenta concettuali dell’inferenza bayesiana\n\nL’approccio bayesiano si fonda su un’idea semplice: ogni volta che raccogliamo dati, possiamo *aggiornare in modo coerente le nostre credenze*. Questa idea è riassunta in una formula molto celebre: il *teorema di Bayes*.\n\n$$\nP(\\text{Ipotesi} \\mid \\text{Dati}) \\;=\\; \n\\frac{P(\\text{Dati} \\mid \\text{Ipotesi}) \\; \\times \\; P(\\text{Ipotesi})}{P(\\text{Dati})}\n$$\n\n\n### Intuizione della formula\n\n* **$P(\\text{Ipotesi})$** → la nostra convinzione iniziale, detta *prior* (“quanto credevo plausibile questa ipotesi prima di raccogliere i dati”).\n* **$P(\\text{Dati} \\mid \\text{Ipotesi})$** → la compatibilità tra l’ipotesi e ciò che osserviamo, detta *verosimiglianza* o *likelihood*.\n* **$P(\\text{Ipotesi} \\mid \\text{Dati})$** → la convinzione aggiornata, detta *posterior* (“quanto credo plausibile questa ipotesi dopo aver visto i dati”).\n* **$P(\\text{Dati})$** → un fattore di normalizzazione: garantisce che tutte le ipotesi considerate abbiano probabilità che sommano a 1.\n\nIn sintesi:\n\n$$\n\\text{Posterior} \\;\\propto\\; \\text{Likelihood} \\times \\text{Prior}\n$$\n\n\n### Un esempio psicologico\n\nImmaginiamo di voler valutare se un *nuovo training cognitivo* riduce l’ansia negli studenti.\n\n* Prima di raccogliere i dati, abbiamo una convinzione iniziale: pensiamo che l’effetto sia possibile, ma non ne siamo sicuri (*prior*).\n* Dopo aver somministrato il training a un piccolo gruppo, osserviamo una riduzione dell’ansia (*dati*). Questi dati sono più compatibili con l’ipotesi “il training funziona” che con “il training non funziona” (*likelihood*).\n* Combinando le due cose otteniamo una nuova convinzione più informata (*posterior*): l’ipotesi che il training riduca l’ansia diventa più plausibile, ma con un margine di incertezza che continueremo a considerare.\n\n\n### Perché è importante?\n\nl teorema di Bayes non è solo una formula: è un *principio di ragionamento*. Ci dice come passare in modo sistematico da ciò che sappiamo prima (priori) a ciò che sappiamo dopo aver osservato i dati (posteriori). Questa struttura ci permette di integrare teoria e dati in modo coerente, esplicitare le assunzioni iniziali e rappresentare sempre l’incertezza residua.\n\n::: {.callout-note}\n\n#### Messaggio chiave\n\nIl teorema di Bayes è la regola che collega *credenze iniziali*, *dati osservati* e *credenze aggiornate*. Non elimina l’incertezza, ma ci mostra come aggiornarla in modo coerente.\n:::\n\n\n## L’aggiornamento bayesiano in azione: l’esempio del globo terrestre\n\nPer capire come funziona concretamente il teorema di Bayes, immaginiamo un esperimento mentale proposto da McElreath nel suo testo *Statistical Rethinking* [@McElreath_rethinking]. Abbiamo davanti a noi un *globo terrestre* (una sfera blu e marrone) e ci poniamo una domanda:\n\n> *“Quale proporzione della superficie terrestre è coperta da acqua?”*\n\nSappiamo che gran parte del pianeta è mare, ma non conosciamo la percentuale precisa.\nVogliamo stimarla con un piccolo “esperimento casuale”.\n\n\n### Il setup sperimentale\n\nChiamiamo $p$ la vera proporzione di superficie coperta d’acqua. Questo è il nostro *parametro di interesse*: il numero che vogliamo stimare.\n\nOgni volta che facciamo girare il globo e puntiamo il dito otteniamo un’osservazione:\n\n* *W* se tocchiamo acqua,\n* *L* se tocchiamo terra.\n\nIl nostro modello dei dati assume che ogni lancio sia indipendente e che la probabilità di osservare acqua sia proprio $p$.\n\nAll’inizio non sappiamo nulla di preciso: potremmo allora assegnare a $p$ una *distribuzione a priori uniforme*, cioè ritenere ogni valore compreso tra 0 e 1 ugualmente plausibile. Questo rappresenta uno stato di “ignoranza informativa”: nessuna preferenza iniziale per alcuni valori rispetto ad altri.\n\n### La dinamica dell’apprendimento\n\n* **Primo lancio** → osserviamo “W” (acqua). Ora valori molto bassi di $p$ diventano poco plausibili (se $p$ fosse vicino a 0, sarebbe stato molto improbabile ottenere acqua al primo colpo). La distribuzione a posteriori si sposta, assegnando più probabilità a valori alti di $p$.\n\n* **Secondo lancio** → otteniamo “L” (terra). Questo dato porta nella direzione opposta: valori molto alti di $p$ diventano meno plausibili. La distribuzione a posteriori si “riequilibra”, privilegiando valori intermedi.\n\nCon ogni nuova osservazione, il quadro cambia: nessun dato singolo è definitivo, ma ciascuno contribuisce a modificare il profilo della nostra incertezza.\n\n\n### L’accumulo progressivo dell’evidenza\n\nImmaginiamo di osservare la sequenza: *W, L, W, W, L, W, L, W, W*.\n\n* Dopo ogni lancio, la distribuzione si aggiorna.\n* Ogni *posterior* diventa automaticamente il *prior* per il passo successivo.\n* In questo modo, l’apprendimento è *cumulativo*: non scartiamo mai le informazioni già raccolte, ma le integriamo con le nuove.\n\nQuesta è l’essenza dell’approccio bayesiano: una catena continua di aggiornamento delle credenze.\n\n\n### L’evoluzione dell’incertezza\n\nUn punto cruciale è che non si aggiorna solo la stima più plausibile di $p$, ma anche la *larghezza della distribuzione*:\n\n* con pochi dati, la distribuzione è ampia → riflette grande incertezza;\n* con più osservazioni, la distribuzione diventa più stretta → la nostra conoscenza si affina;\n* la velocità e la forma del restringimento dipendono dai dati: sequenze molto coerenti riducono l’incertezza rapidamente, sequenze più variabili la riducono gradualmente.\n\n\n\n::: {.cell layout-align=\"center\" tags='hide-input'}\n::: {.cell-output-display}\n![](03_uncertainty_quantification_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=80%}\n:::\n:::\n\n\n\n### Come leggere il grafico\n\nIn ogni pannello, la *linea grigia* mostra il *prior* (prima dell’osservazione), e la *linea blu* il *posterior* (dopo l’osservazione). Si vede chiaramente come, passo dopo passo, la curva blu si restringa e si concentri intorno ai valori più compatibili con i dati. Ogni distribuzione aggiornata diventa la nuova base di partenza per il passo successivo.\n\nQuesto esempio mostra in modo concreto che l’inferenza bayesiana non cerca “una stima definitiva”, ma costruisce un *processo dinamico di apprendimento*, in cui l’incertezza si riduce e si adatta man mano che i dati accumulano evidenza.\n\n\n::: {.callout-note}\n## Posteriori diverse a confronto\n\nLo stesso insieme di dati può dare origine a posteriori molto diverse a seconda del prior.  \nQuesto non è un “difetto” del Bayes, ma un aspetto centrale: rende esplicita l’influenza delle ipotesi di partenza. Il vantaggio è che possiamo discutere apertamente *quanto* le conclusioni dipendano dal prior e, se necessario, confrontare più specificazioni (analisi di sensibilità).\n:::\n\n\n## Implicazioni per la ricerca psicologica\n\nL’esempio del globo terrestre è un gioco semplice, ma ci insegna alcune lezioni fondamentali che valgono pienamente per la psicologia.\n\n1. *Le nostre convinzioni cambiano con i dati*\n   La ricerca psicologica non si limita a raccogliere informazioni: è un processo di apprendimento. Ogni studio modifica il nostro quadro di riferimento, integrando la nuova evidenza con ciò che già sapevamo.\n\n2. *L’incertezza non scompare, si aggiorna*\n   Dopo pochi dati l’incertezza è ampia; con più osservazioni si restringe, ma non arriva mai a zero. In psicologia, dove i fenomeni sono complessi e variabili, è essenziale rappresentare non solo *quanto* un effetto è plausibile, ma anche *quanto* siamo incerti su di esso.\n\n3. *L’informazione è cumulativa*\n   Proprio come ogni *posterior* diventa il nuovo *prior*, la scienza psicologica avanza grazie all’accumulo coerente di conoscenze. Ogni studio dovrebbe essere visto come un passo in una catena di aggiornamenti, non come un verdetto finale.\n\n\n### Un esempio psicologico\n\nImmaginiamo di valutare l’efficacia di una nuova terapia per l’ansia. Prima dei dati, abbiamo solo intuizioni e risultati preliminari, che costituiscono un prior molto incerto. Dopo il primo studio, otteniamo indicazioni che la terapia potrebbe essere utile, ma con un ampio margine di dubbio, risultando in un posterior ancora largo. Con più studi, le distribuzioni si concentrano progressivamente. Se l’effetto è reale, la curva si restringe intorno a un valore positivo; se l’effetto è debole o assente, la distribuzione si sposterà di conseguenza.\n\nQuesto approccio rende più chiaro perché una singola ricerca non basta a stabilire verità definitive, e perché è cruciale replicare e accumulare evidenza.\n\n::: {.callout-note}\n\n#### Messaggio chiave\n\nIl modello del globo mostra in piccolo quello che accade in psicologia:\n\n* raccogliamo dati,\n* aggiorniamo le nostre credenze,\n* manteniamo l’incertezza,\n* costruiamo conoscenza in modo cumulativo.\n\nL’approccio bayesiano fornisce un linguaggio preciso per descrivere questo processo.\n:::\n\n\n## Considerazioni pratiche e limitazioni\n\nL’approccio bayesiano offre una cornice concettuale elegante e intuitiva per rappresentare l’incertezza e aggiornare le nostre credenze. Tuttavia, quando passiamo dalla teoria alla pratica della ricerca psicologica, emergono alcune considerazioni importanti.\n\n### Scelta delle credenze iniziali (priori)\n\nUn aspetto caratteristico dell’approccio bayesiano è che richiede di specificare sempre una distribuzione a priori. Questo è un vantaggio perché ci costringe a esplicitare le nostre assunzioni e ad ancorarle a conoscenze precedenti, ma è anche una responsabilità, poiché prior troppo forti o poco giustificate possono influenzare eccessivamente i risultati. In pratica, gli psicologi devono imparare a distinguere tra prior debolmente informativi, che lasciano spazio ai dati, e prior informativi, che incorporano evidenza accumulata o teoria.\n\n### Complessità computazionale\n\nMolti modelli psicologici sono complessi, con più parametri, strutture gerarchiche e dinamiche temporali. Le formule di Bayes, in questi casi, non si possono risolvere a mano. Si usano invece metodi numerici come le simulazioni Monte Carlo (MCMC), che richiedono tempo di calcolo e competenze specifiche. Oggi strumenti come Stan o brms rendono queste tecniche accessibili, ma serve comunque una formazione adeguata.\n\n### Interpretazione dei risultati\n\nI risultati bayesiani sono concettualmente più trasparenti, poiché parlano di plausibilità di valori invece di fornire un verdetto sì/no basato sulla \"significatività\". Tuttavia, richiedono un cambio di mentalità. Per chi è abituato a pensare in termini di p-value, questo può sembrare inizialmente meno rassicurante, ma in realtà è più realistico, in quanto fornisce una distribuzione di possibilità.\n\n### Limitazioni intrinseche\n\nInfine, va ricordato che anche l’approccio bayesiano ha limiti. Non elimina l’incertezza, ma la descrive soltanto meglio; dipende comunque dalla qualità dei dati, per cui dati rumorosi o campioni non rappresentativi rimangono problematici; e riflette sempre la struttura del modello scelto, per cui se il modello è inadeguato, anche la migliore inferenza bayesiana porterà a conclusioni distorte.\n\n::: {.callout-note}\n\n#### Messaggio chiave\n\nIl pensiero bayesiano non è una bacchetta magica: richiede scelte esplicite, strumenti adeguati e una lettura attenta dei risultati.\nLa sua forza non sta nell’eliminare le difficoltà della ricerca psicologica, ma nel renderle visibili e trattabili.\n:::\n\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nIn questo capitolo abbiamo fatto il nostro primo vero passo dentro il pensiero bayesiano. Abbiamo visto che l’incertezza può essere rappresentata come una distribuzione di probabilità e abbiamo introdotto il teorema di Bayes, la regola che collega credenze iniziali, dati osservati e credenze aggiornate. Attraverso l’esempio del globo terrestre, abbiamo compreso come l’aggiornamento bayesiano funzioni in pratica: ogni nuova osservazione restringe o sposta la distribuzione delle nostre convinzioni, riducendo progressivamente l’incertezza.\n\nAbbiamo discusso le implicazioni per la ricerca psicologica, riconoscendo che i dati non danno risposte definitive, ma guidano un processo di apprendimento cumulativo. Infine, abbiamo esaminato le considerazioni pratiche e le limitazioni dell’approccio, dalla scelta dei priori alla complessità computazionale, dall'interpretazione dei risultati alla dipendenza dalla qualità dei modelli.\n\nQuesta panoramica ci ha permesso di sviluppare un’intuizione solida: il bayesianesimo non elimina l’incertezza, ma ci fornisce un metodo coerente e trasparente per trattarla. Nei prossimi capitoli tradurremo queste idee in strumenti concreti, imparando a costruire e interpretare modelli bayesiani applicati alla ricerca psicologica.\n\nUn equivoco comune è pensare che l’approccio bayesiano fornisca “più certezze” rispetto a quello frequentista. In realtà, accade il contrario: i risultati bayesiani mettono in evidenza l’incertezza, anziché nasconderla dietro a un singolo numero o a un verdetto dicotomico. Questo può inizialmente sembrare meno rassicurante, soprattutto per chi è abituato a ragionare in termini di p-value e soglie di significatività. Tuttavia, proprio questa trasparenza costituisce la forza dell’approccio bayesiano: ci permette di comunicare in modo più onesto quali valori dei parametri sono plausibili e quanto spazio rimane per il dubbio. Adottare il pensiero bayesiano significa quindi cambiare prospettiva: non cercare una “certezza definitiva”, ma abituarsi a ragionare in termini di gradi di plausibilità e di apprendimento continuo dai dati.\n\n\n::: {.callout-note title=\"Approfondimento\" collapse=\"true\"}\n\nA una prima lettura, è sufficiente concentrarsi sul significato generale dell’aggiornamento bayesiano: ogni nuova osservazione modifica le nostre credenze sui parametri, restringendo progressivamente l’incertezza. Per il momento, gli studenti possono tralasciare i dettagli tecnici riportati qui sotto.\n\nDopo aver studiato le *famiglie coniugate* nei capitoli successivi, sarà utile tornare su questo esempio per comprenderne appieno il funzionamento. L’aggiornamento bayesiano diventerà allora più chiaro nei suoi aspetti formali, mostrando perché in alcuni casi (come questo) il calcolo può essere svolto in maniera particolarmente semplice ed elegante.\n\n### Il modello Beta-Binomiale\n\nL’esempio del globo è un caso classico di modello *Beta-Binomiale*:\n\n* la distribuzione *a priori* sulla proporzione $p$ è una Beta;\n* la verosimiglianza delle osservazioni (successi/insuccessi) segue una Binomiale;\n* per la proprietà di *coniugazione*, la distribuzione *a posteriori* risulta anch’essa una Beta.\n\nQuesta proprietà ci permette di aggiornare le credenze con una regola semplice:\n\n$$\n\\text{Posterior} \\sim \\text{Beta}(a + W, \\; b + L)\n$$\n\ndove $a$ e $b$ sono i parametri della prior (nel nostro caso iniziale $a=1, b=1$), mentre $W$ e $L$ sono il numero di osservazioni acqua (*Water*) e terra (*Land*).\n\n\n### Esempio passo per passo\n\n* **Primo pannello (1 osservazione: W)**\n  Prior: $\\text{Beta}(1,1)$ → Posterior: $\\text{Beta}(2,1)$.\n  La distribuzione si concentra su valori alti di $p$.\n\n* **Secondo pannello (2 osservazioni: W, L)**\n  Prior: $\\text{Beta}(2,1)$ → Posterior: $\\text{Beta}(3,2)$.\n  L’evidenza si riequilibra verso valori intermedi.\n\n* **Terzo pannello (3 osservazioni: W, L, W)**\n  Prior: $\\text{Beta}(3,2)$ → Posterior: $\\text{Beta}(4,2)$.\n  Cresce la plausibilità di valori intorno a $p \\approx 0.66$.\n\n* **Quarto pannello (4 osservazioni: W, L, W, W)**\n  Posterior: $\\text{Beta}(5,2)$.\n\n* **Quinto pannello (5 osservazioni: W, L, W, W, L)**\n  Posterior: $\\text{Beta}(5,3)$.\n\n* **Sesto pannello (6 osservazioni: W, L, W, W, L, W)**\n  Posterior: $\\text{Beta}(6,3)$.\n\n* **Settimo pannello (7 osservazioni: W, L, W, W, L, W, L)**\n  Posterior: $\\text{Beta}(6,4)$.\n\n* **Ottavo pannello (8 osservazioni: W, L, W, W, L, W, L, W)**\n  Posterior: $\\text{Beta}(7,4)$.\n\n* **Nono pannello (9 osservazioni: W, L, W, W, L, W, L, W, W)**\n  Posterior: $\\text{Beta}(8,4)$.\n\n\n### In sintesi\n\nL’aggiornamento sequenziale mostra come la distribuzione diventi progressivamente più concentrata intorno ai valori di $p$ compatibili con i dati osservati. Con l’accumulo di osservazioni, l’incertezza si riduce, la stima più plausibile di $p$ si stabilizza e l’inferenza diventa più precisa.\n\nQuesto è l’essenziale del processo bayesiano: un apprendimento cumulativo, coerente e formalmente elegante.\n:::\n\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] digest_0.6.37         timechange_0.3.0      estimability_1.5.1   \n#> [10] lifecycle_1.0.4       survival_3.8-3        magrittr_2.0.3       \n#> [13] compiler_4.5.1        rlang_1.1.6           tools_4.5.1          \n#> [16] yaml_2.3.10           knitr_1.50            labeling_0.4.3       \n#> [19] bridgesampling_1.1-2  htmlwidgets_1.6.4     curl_7.0.0           \n#> [22] pkgbuild_1.4.8        RColorBrewer_1.1-3    abind_1.4-8          \n#> [25] multcomp_1.4-28       withr_3.0.2           purrr_1.1.0          \n#> [28] grid_4.5.1            stats4_4.5.1          colorspace_2.1-1     \n#> [31] xtable_1.8-4          inline_0.3.21         emmeans_1.11.2-8     \n#> [34] scales_1.4.0          MASS_7.3-65           cli_3.6.5            \n#> [37] mvtnorm_1.3-3         rmarkdown_2.29        ragg_1.5.0           \n#> [40] generics_0.1.4        RcppParallel_5.1.11-1 cachem_1.1.0         \n#> [43] stringr_1.5.1         splines_4.5.1         parallel_4.5.1       \n#> [46] vctrs_0.6.5           V8_7.0.0              Matrix_1.7-4         \n#> [49] sandwich_3.1-1        jsonlite_2.0.0        arrayhelpers_1.1-0   \n#> [52] systemfonts_1.2.3     glue_1.8.0            codetools_0.2-20     \n#> [55] distributional_0.5.0  lubridate_1.9.4       stringi_1.8.7        \n#> [58] gtable_0.3.6          QuickJSR_1.8.0        htmltools_0.5.8.1    \n#> [61] Brobdingnag_1.2-9     R6_2.6.1              textshaping_1.0.3    \n#> [64] rprojroot_2.1.1       evaluate_1.0.5        lattice_0.22-7       \n#> [67] backports_1.5.0       memoise_2.0.1         broom_1.0.9          \n#> [70] snakecase_0.11.1      rstantools_2.5.0      coda_0.19-4.1        \n#> [73] gridExtra_2.3         nlme_3.1-168          checkmate_2.3.3      \n#> [76] xfun_0.53             zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n",
    "supporting": [
      "03_uncertainty_quantification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}