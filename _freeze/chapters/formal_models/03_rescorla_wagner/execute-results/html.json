{
  "hash": "f06a636068231e27c38beaa1d3acf946",
  "result": {
    "engine": "knitr",
    "markdown": "# Il modello di Rescorla–Wagner {#sec-dynamic-models-rescorla-wagner}\n\n## Introduzione {.unnumbered .unlisted}\n\nQuesto capitolo introduce il *modello di Rescorla–Wagner (RW)* con *regola di scelta Softmax* [@rescorla1972theory], come naturale estensione del modello di revisione degli obiettivi discusso nel capitolo precedente. Nel modello precedente l’aggiornamento era un *termine additivo* guidato da una discrepanza osservata; qui l’aggiornamento diventa esplicitamente *guidato dall’errore di predizione del rinforzo* (*reward prediction error*, RPE), ossia la differenza tra rinforzo ottenuto e rinforzo atteso. Questa formulazione è più *psicologicamente plausibile* e allineata all’evidenza in psicologia e neuroscienze: *si impara proporzionalmente a quanto l’esito sorprende le aspettative*.\n\nAccanto al livello di *apprendimento* (aggiornamento dei valori associativi $Q$), introduciamo il livello *decisionale*: le scelte non sono deterministiche, ma riflettono un compromesso fra *sfruttamento* dell’opzione migliore ed *esplorazione* di alternative. Con due opzioni, la Softmax si riduce a una *logistica della differenza* $Q_B-Q_A$, modulata dal parametro di *inverse temperature* $\\tau$: valori alti di $\\tau$ rendono le scelte più coerenti con $Q$, valori bassi più esplorative.\n\nQuesta distinzione *apprendimento–decisione* è cruciale: consente di *separare* il meccanismo che aggiorna le aspettative (parametro $lr$) dal meccanismo che le traduce in *probabilità di scelta* (parametro $\\tau$). Nei paragrafi successivi mostreremo come *simulare* dati (es. *probabilistic reversal learning*), *stimare* i parametri con *Stan* e *interpretare* i profili individuali e di gruppo. \n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n::: {.cell}\n\n:::\n\n:::\n\n\n## Il Modello di Rescorla–Wagner\n\nIl modello di *Rescorla–Wagner (RW)* è uno dei modelli più influenti nello studio dell’apprendimento associativo. Esso descrive come gli individui aggiornino le proprie aspettative in base all’esperienza, introducendo un meccanismo semplice ma potente che spiega fenomeni come acquisizione, estinzione e *blocking*.\n\nL’idea di fondo è che l’apprendimento si realizzi attraverso l’aggiornamento della *forza associativa* $Q_t(s)$ di uno stimolo $s$ al tempo $t$, in funzione della discrepanza tra ciò che ci si aspettava e ciò che si è effettivamente osservato.\n\n\n### Aggiornamento delle aspettative\n\nDopo ogni prova, la stima del valore viene modificata secondo la regola:\n\n$$\nQ_{t+1}(s) \\;=\\; Q_t(s) \\;+\\; \\alpha \\, \\delta_t ,\n$$\ndove\n\n$$\n\\delta_t \\;=\\; R_t - Q_t(s)\n$$\nè l’*errore di previsione* (*prediction error*), cioè la differenza tra la ricompensa ottenuta $R_t$ e l’aspettativa precedente $Q_t(s)$.\n\n* Se $\\delta_t > 0$: la ricompensa è stata migliore del previsto → l’associazione si rafforza.\n* Se $\\delta_t < 0$: la ricompensa è stata peggiore del previsto → l’associazione si indebolisce.\n* Se $\\delta_t = 0$: ciò che si è osservato coincide con l’attesa → nessun aggiornamento.\n\nIl parametro $ \\alpha \\in [0,1] $ rappresenta il tasso di apprendimento (qui lo indichiamo con `lr`). Con `lr` alto l’aggiornamento è rapido; con `lr` basso è lento e più “conservativo”.\n\n::: {.callout-note collapse=true title=\"Variante (facoltativa): tassi distinti per PE positivo/negativo\"}\nIn alcune applicazioni si usano due tassi distinti $\\alpha^+$ e $\\alpha^-$ per apprendere diversamente da buone e cattive notizie. Questa distinzione consente di modellare la diversa sensibilità di individui o gruppi alle ricompense inattese rispetto alle punizioni o ai premi. Nel presente tutorial adottiamo **un unico `lr`** per semplicità e coerenza con il codice Stan.\n:::\n\n\n### Dalla valutazione alla decisione\n\nAvere valori $Q_t(A)$ e $Q_t(B)$ diversi non implica che la scelta sia sempre deterministica. Gli individui possono alternare tra *sfruttamento* (scegliere l’opzione con valore atteso maggiore) ed *esplorazione* (provare opzioni alternative).\n\nCon due opzioni (A, B), la scelta è modellata come logistica della differenza di valore:\n\n$$\nP(\\text{scegliere B}) = \\text{inv\\_logit}\\!\\big(\\tau \\,[Q_t(B)-Q_t(A)]\\big).\n$$\nAll’aumentare di $\\tau$, anche piccole differenze $Q_t(B)-Q_t(A)$ producono scelte quasi deterministiche; con $\\tau$ bassa il comportamento è più esplorativo.\n\nIn sintesi, il modello di Rescorla–Wagner fornisce una descrizione formale e compatta di come gli individui apprendono in modo flessibile dalle proprie esperienze, adattando le aspettative e le decisioni in risposta ai cambiamenti dell’ambiente.\n\n\n::: {.callout-caution collapse=true title=\"Approfondimento: esplorazione vs. sfruttamento (logit a 2 opzioni)\"}\nCon due opzioni, la *Softmax* si riduce a una *logistica* sulla differenza di valore. Useremo quindi $\\Delta Q = Q_B - Q_A$ e modelleremo la probabilità di scegliere *B* come\n\n$$\nP(B) = \\text{inv\\_logit}\\big(\\tau \\,\\Delta Q\\big),\n$$\ndove $\\tau>0$ (*inverse temperature*) regola il compromesso esplorazione–sfruttamento:\n\n* *$\\tau$ basso* → comportamento *esplorativo*: anche differenze modeste non portano scelte deterministiche.\n* *$\\tau$ alto* → comportamento di *sfruttamento*: piccole differenze in $\\Delta Q$ bastano per preferenze quasi certe.\n\nNel grafico seguente mostriamo la relazione tra $\\Delta Q$ e $P(B)$ per due valori di $\\tau$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Differenze di valore (coerenti con Q in [0,1] → ΔQ in [-1, 1])\ndq <- seq(-1, 1, length.out = 201)\n\n# Logit a 2 opzioni (equivalente alla Softmax con K=2)\np_choose_B <- function(dq, tau) plogis(tau * dq)\n\ndf <- data.frame(\n  dq = rep(dq, 2),\n  prob_B = c(p_choose_B(dq, tau = 0.5),\n             p_choose_B(dq, tau = 5)),\n  tau = factor(rep(c(\"τ = 0.5 (alta esplorazione)\",\n                     \"τ = 5 (alto sfruttamento)\"),\n                   each = length(dq)))\n)\n\nggplot(df, aes(x = dq, y = prob_B, color = tau)) +\n  geom_line(size = 1.2) +\n  labs(\n    x = expression(Delta*Q[B-A]),\n    y = \"Probabilità di scegliere B\"\n  )\n```\n\n::: {.cell-output-display}\n![](03_rescorla_wagner_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n**Lettura del grafico**\n\n* Se $\\Delta Q = 0.5$:\n\n  * con $\\tau = 0.5$, $P(B) \\approx \\text{inv\\_logit}(0.25) \\approx 0.56$ → decisione ancora esplorativa;\n  * con $\\tau = 5$, $P(B) \\approx \\text{inv\\_logit}(2.5) \\approx 0.92$ → prevale lo sfruttamento.\n\n* Se $\\Delta Q = 1$:\n\n  * con $\\tau = 0.5$, $P(B) \\approx \\text{inv\\_logit}(0.5) \\approx 0.62$;\n  * con $\\tau = 5$, $P(B) \\approx \\text{inv\\_logit}(5) \\approx 0.993$, scelta quasi deterministica.\n\nQuesto esempio mostra come $\\tau$ controlli la transizione tra esplorazione e sfruttamento nel *caso binario* (logit), coerente con il modello Stan usato nel tutorial.\n:::\n\n\n### Identificabilità e scaling\n\nNella funzione Softmax conta solo la *differenza* tra i valori $Q$. Se aggiungiamo la stessa costante $c$ a entrambi (cioè $Q_t(s) + c$), le probabilità di scelta non cambiano.  \n\nPer questo motivo:  \n\n- si inizializzano di solito i valori in modo simmetrico (ad esempio $Q_0(A) = Q_0(B) = 0.5$);  \n- si mantengono i rinforzi nel range $\\{0,1\\}$;  \n- oppure si fissa un valore iniziale di riferimento, o si pongono vincoli su $\\beta$.  \n\nQueste scelte servono solo a rendere il modello *ben definito*, senza influenzare il comportamento osservato.\n\nNel presente tutorial il rinforzo è codificato come $R_t \\in \\{0,1\\}$. In questo caso i valori $Q$ convergono a stime di probabilità di ricompensa e risultano naturalmente in $[0,1]$. Con codifiche $\\{-1,+1\\}$, i $Q$ convergono a valori attesi in $[-1,+1]$ e occorre tenerne conto nell’interpretazione.\n\n\n\n## Simulazione\n\nSimuliamo i dati in un compito di *Probabilistic Reversal Learning* (PRL). In questo compito il partecipante deve scegliere ripetutamente tra due stimoli:\n\n* uno *ricco*, con probabilità di ricompensa $p=0.7$;\n* uno *povero*, con probabilità $1-p=0.3$.\n\nA metà esperimento le probabilità vengono invertite (*reversal*): lo stimolo che prima era ricco diventa povero e viceversa. Il partecipante deve quindi adattarsi al cambiamento per massimizzare le ricompense.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Simulatore PRL (RW + logit) allineato al modello Stan (choice in 0/1)\nsimulate_prl_rw_binary <- function(\n  n_trials        = 160,\n  p_reward_rich   = 0.7,\n  reversal_trial  = 80,            # se NULL, nessun reversal\n  lr              = 0.15,          # learning rate unico\n  tau             = 2,             # inverse temperature (decision noise)\n  Q0              = c(A = 0.0, B = 0.0),\n  seed            = 1234\n){\n  stopifnot(length(Q0) == 2, all(c(\"A\",\"B\") %in% names(Q0)))\n  set.seed(seed)\n  Q <- Q0\n\n  choice   <- integer(n_trials)   # 0 = A, 1 = B\n  reward   <- integer(n_trials)   # 0/1\n  rich_is_A <- rep.int(1L, n_trials) # 1 = A ricco, 0 = B ricco (inizio A ricco)\n  if (!is.null(reversal_trial)) {\n    rich_is_A[(reversal_trial + 1):n_trials] <- 0L\n  }\n\n  Q_A <- Q_B <- pB_seq <- pe_seq <- numeric(n_trials)\n\n  for (t in seq_len(n_trials)) {\n    # probabilità di scegliere B (softmax logit a due opzioni)\n    pB <- plogis(tau * (Q[\"B\"] - Q[\"A\"]))\n    choice[t] <- rbinom(1, 1, pB)   # 1=B, 0=A\n\n    a_idx <- if (choice[t] == 1L) 2L else 1L\n\n    # probabilità di ricompensa per l’opzione scelta\n    chosen_is_rich <- (choice[t] == 0L && rich_is_A[t] == 1L) ||  \n                      (choice[t] == 1L && rich_is_A[t] == 0L)\n    pr <- if (chosen_is_rich) p_reward_rich else (1 - p_reward_rich)\n\n    reward[t] <- rbinom(1, 1, pr)\n\n    # prediction error e aggiornamento RW\n    pe <- reward[t] - Q[a_idx]\n    Q[a_idx] <- Q[a_idx] + lr * pe\n\n    Q_A[t]   <- Q[\"A\"]\n    Q_B[t]   <- Q[\"B\"]\n    pB_seq[t] <- pB\n    pe_seq[t] <- pe\n  }\n\n  tibble::tibble(\n    trial     = seq_len(n_trials),\n    choice    = choice,    # 0=A, 1=B\n    reward    = reward,    # 0/1\n    rich_is_A = rich_is_A, # 0/1\n    Q_A = Q_A, Q_B = Q_B, pB = pB_seq, pe = pe_seq\n  )\n}\n\nsim <- simulate_prl_rw_binary()\n```\n:::\n\n\n*Nota.* La probabilità simulata coincide con quella dello Stan: $P(B)=\\text{inv\\_logit}\\!\\big(\\tau [Q(B)-Q(A)]\\big)$. Anche l’aggiornamento usa **lo stesso** `lr`.\n\n\nVisualizziamo l’evoluzione dei valori associativi $Q$:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](03_rescorla_wagner_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Interpretazione del grafico\n\nNella *prima fase* (trial 1–80), lo stimolo ricco (linea blu) riceve più ricompense e quindi accumula un valore $Q$ più alto rispetto allo stimolo povero (linea verde).\n\nAl momento del *reversal* (linea verticale tratteggiata), le probabilità si invertono. Lo stimolo blu smette di essere vantaggioso e il suo valore $Q$ scende, mentre lo stimolo verde cresce.\n\nQuesto andamento riflette il principio base del modello di *Rescorla–Wagner*:\n\n* i valori associativi $Q$ non sono fissi,\n* vengono aggiornati *trial per trial* con una regola di apprendimento molto semplice:\n\n$$\nQ_{\\text{nuovo}} = Q_{\\text{vecchio}} + lr \\times (reward - Q_{\\text{vecchio}}).\n$$\nIl termine $reward - Q_{\\text{vecchio}}$ è il *prediction error* (PE): la differenza tra ciò che si è osservato e ciò che ci si aspettava.\n\n* Se il feedback è migliore del previsto (PE>0), il valore $Q$ aumenta.\n* Se è peggiore del previsto (PE<0), il valore diminuisce.\n* Il *learning rate* `lr` regola di quanto il valore cambia a ogni prova.\n* Il parametro `tau` controlla invece quanto le scelte sono “guidate” dai valori Q:\n\n  * se `tau` è grande → scelte più deterministiche (si sceglie quasi sempre lo stimolo con Q maggiore);\n  * se `tau` è piccolo → scelte più esplorative o rumorose.\n\n\n## Stima Bayesiana con Stan\n\nNella simulazione conoscevamo i parametri generativi (`lr=0.15`, `tau=2`). Nella realtà, però, abbiamo *solo i dati osservati*:\n\n* per ogni trial sappiamo quale scelta è stata fatta (`choice=0` per A, `choice=1` per B),\n* e se il feedback è stato positivo o negativo (`reward=1` oppure `0`).\n\nI valori interni $Q$ e i parametri del modello non sono osservabili: dobbiamo *inferirli*.\n\nL’ipotesi è che i dati derivino da un processo di tipo *Rescorla–Wagner con regola di scelta logistica*. L’obiettivo è quindi stimare, a partire dai soli dati:\n\n* il *learning rate* `lr`, che regola la velocità di aggiornamento dei valori;\n* l’*inverse temperature* `tau`, che controlla quanto le scelte sono coerenti con i valori Q.\n\n\n### Codice Stan\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstancode_rw <- \"\ndata {\n  int<lower=1> nTrials;                   // numero di prove\n  array[nTrials] int<lower=0,upper=1> choice; // scelte osservate (0=A, 1=B)\n  array[nTrials] real<lower=0,upper=1> reward; // ricompense osservate (0/1)\n}\n\ntransformed data {\n  vector[2] initV = rep_vector(0.0, 2);   // valori Q iniziali\n}\n\nparameters {\n  real<lower=0,upper=1> lr;   // learning rate\n  real<lower=0,upper=3> tau;  // inverse temperature (softmax / decision noise)\n}\n\nmodel {\n  vector[2] v = initV;        // valori Q correnti\n  real pe;                    // prediction error\n  real p;                     // probabilità di scelta =1 (stimolo B)\n\n  // Priors deboli ma informative\n  lr ~ beta(2, 10);           // learning rate vicino a valori piccoli\n  tau ~ lognormal(log(2), 0.5); // inverse temperature positiva\n\n  for (t in 1:nTrials) {\n    // Probabilità di scegliere B: logit della differenza Q_B - Q_A\n    p = inv_logit(tau * (v[2] - v[1]));\n    choice[t] ~ bernoulli(p);\n\n    // Prediction error e aggiornamento\n    int a = choice[t] + 1;     // 0→1 (A), 1→2 (B)\n    pe = reward[t] - v[a];\n    v[a] += lr * pe;\n  }\n}\n\"\n```\n:::\n\n\nI prior scelti sono debolmente informativi ma coerenti con compiti PRL tipici (apprendimento moderato e scelte non eccessivamente rumorose). Possono essere resi più o meno conservativi in base al compito.\n\n\n### Commento al codice Stan\n\n1. **Inizializzazione.** All’inizio i due valori $Q[1]$ e $Q[2]$ (per A e B) sono fissati a 0.\n   Rappresentano l’aspettativa iniziale: “nessuna preferenza”.\n\n2. **Probabilità della scelta.**\n   Al trial $t$, confrontiamo i due valori:\n\n   $$\n   p(B) = \\text{inv\\_logit}\\big(\\tau \\cdot (Q_B - Q_A)\\big).\n   $$\n\n   Questo significa che se $Q_B > Q_A$, aumenta la probabilità di scegliere B.\n   Il parametro `tau` regola la “determinazione”:\n\n   * se `tau` è grande, basta una piccola differenza tra i due Q per portare a scelte quasi certe;\n   * se `tau` è piccolo, anche differenze grandi lasciano spazio all’esplorazione.\n\n3. **Valutazione della scelta osservata.**\n   La riga `choice[t] ~ bernoulli(p)` confronta la scelta osservata con la probabilità prevista.\n   Questa è la *verosimiglianza*: se la scelta osservata è coerente con i Q correnti, il modello “ottiene credito”; se è incoerente, viene “penalizzato”.\n\n4. **Osservazione dell’esito e prediction error.**\n   Dopo aver osservato il feedback, calcoliamo:\n\n   $$\n   PE = reward[t] - Q[\\text{scelta}],\n   $$\n\n   ossia la differenza tra il risultato ricevuto e quello atteso.\n\n5. **Aggiornamento dei valori.**\n   Solo il Q corrispondente all’opzione scelta viene aggiornato:\n\n   $$\n   Q_{\\text{nuovo}} = Q_{\\text{vecchio}} + lr \\cdot PE.\n   $$\n\n   * Se il feedback è migliore del previsto (PE>0), il valore cresce.\n   * Se è peggiore del previsto (PE<0), il valore diminuisce.\n   * Il learning rate `lr` determina di quanto cambia il valore a ogni trial.\n\n### Esempio intuitivo\n\nImmagina di avere $Q_A = 0.6$, $Q_B = 0.3$ e `tau=2`. La differenza $Q_B - Q_A = -0.3$. Il logit vale $-0.6$, e quindi:\n\n$$\np(B) = \\text{inv\\_logit}(-0.6) \\approx 0.35.\n$$\nQuindi, il modello si aspetta che A venga scelto nel 65% dei casi.\n\nSe il soggetto sceglie effettivamente A, la verosimiglianza è alta. Se sceglie B, è possibile ma meno probabile: il modello aggiorna i valori interni in base al feedback ricevuto.\n\nIn questo tutorial useremo i dati simulati in precedenza:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstan_data <- list(\n  nTrials = nrow(sim),\n  choice  = as.integer(sim$choice),\n  reward  = as.numeric(sim$reward)   # 0/1 come real per coerenza con <lower=0,upper=1>\n)\nglimpse(stan_data)\n#> List of 3\n#>  $ nTrials: int 160\n#>  $ choice : int [1:160] 0 1 1 0 1 1 0 0 0 0 ...\n#>  $ reward : num [1:160] 1 0 0 1 0 0 0 0 1 1 ...\n```\n:::\n\n\nCompiliamo il modello e eseguiamo il campionamento:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod_rw <- cmdstanr::cmdstan_model(write_stan_file(stancode_rw))\nfit_rw <- mod_rw$sample(\n  data = stan_data,\n  seed = 42,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 4000,\n  refresh = 200\n)\n```\n:::\n\n\nEseguiamo le diagnostiche di campionamento:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_rw$cmdstan_diagnose()  # controlli rapidi cmdstan\n#> Checking sampler transitions treedepth.\n#> Treedepth satisfactory for all transitions.\n#> \n#> Checking sampler transitions for divergences.\n#> No divergent transitions found.\n#> \n#> Checking E-BFMI - sampler transitions HMC potential energy.\n#> E-BFMI satisfactory.\n#> \n#> Rank-normalized split effective sample size satisfactory for all parameters.\n#> \n#> Rank-normalized split R-hat values satisfactory for all parameters.\n#> \n#> Processing complete, no problems detected.\n```\n:::\n\n\nEsaminiamo la distribuzione a posteriori dei parametri:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_rw$summary(c(\"lr\",\"tau\"))\n#> # A tibble: 2 × 10\n#>   variable  mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n#>   <chr>    <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n#> 1 lr       0.155  0.149 0.045 0.043 0.091 0.236 1.001 7735.557 7858.989\n#> 2 tau      2.407  2.435 0.350 0.382 1.790 2.927 1.000 6866.231 4320.660\n```\n:::\n\n\n\n## Interpretazione dei risultati\n\nLe distribuzioni a posteriori stimano quanto bene il modello è riuscito a recuperare i parametri generativi usati nella simulazione (`lr = 0.15`, `tau = 2`).\n\n* `lr` (learning rate) regola la rapidità con cui il soggetto aggiorna le proprie aspettative dopo ogni feedback.\n\n  * Valori alti → aggiornamenti rapidi: anche un singolo feedback può cambiare molto la stima del valore associato a uno stimolo.\n  * Valori bassi → aggiornamenti lenti: il soggetto “conserva” più a lungo le esperienze passate, adattandosi solo gradualmente ai cambiamenti.\n\n* `tau` (inverse temperature) controlla la coerenza delle scelte rispetto ai valori $Q$.\n\n  * Con `tau` alto → le scelte sono quasi deterministiche: basta una piccola differenza tra $Q_A$ e $Q_B$ per indurre una preferenza netta.\n  * Con `tau` basso → il comportamento appare più esplorativo o rumoroso: anche se uno stimolo ha un valore più alto, non sempre viene scelto.\n\nNei grafici delle distribuzioni posteriori:\n\n* le *linee tratteggiate* rappresentano i valori “veri” usati per simulare i dati,\n* le *distribuzioni stimate* mostrano l’incertezza del modello sulle possibili soluzioni.\n\nQuando la distribuzione è ben centrata sulla linea tratteggiata e piuttosto stretta, significa che il modello ha recuperato bene il parametro. Distribuzioni più larghe o spostate indicano maggiore incertezza o possibili *trade-off* tra parametri (ad esempio: un `lr` leggermente diverso può essere compensato da un `tau` più basso o più alto producendo previsioni simili).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraws_df <- fit_rw$draws(c(\"lr\",\"tau\")) |>\n  as_draws_df() |>\n  tibble::as_tibble()\n\ncols <- c(lr = \"#5d5349\", tau = \"#4682B4\")\n\nplot_post <- function(draws, param, col) {\n  ggplot(draws, aes(x = .data[[param]])) +\n    geom_histogram(aes(y = after_stat(density)), bins = 40,\n                   fill = col, color = \"black\", alpha = 0.6) +\n    geom_density(color = col, linewidth = 1) +\n    labs(x = param, y = \"Densità\", title = paste(\"Posterior di\", param))\n}\n\np_lr  <- plot_post(draws_df, \"lr\",  cols[\"lr\"])\np_tau <- plot_post(draws_df, \"tau\", cols[\"tau\"])\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrue_vals <- c(lr = 0.15, tau = 2)\np_lr  + geom_vline(xintercept = true_vals[\"lr\"],  linetype = 2)\n```\n\n::: {.cell-output-display}\n![](03_rescorla_wagner_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n\n```{.r .cell-code}\np_tau + geom_vline(xintercept = true_vals[\"tau\"], linetype = 2)\n```\n\n::: {.cell-output-display}\n![](03_rescorla_wagner_files/figure-html/unnamed-chunk-11-2.png){fig-align='center' width=85%}\n:::\n:::\n\n\nAttenzione ai possibili *trade-off*: in dataset brevi o con scarsa esplorazione, combinazioni diverse di `lr` e `tau` possono produrre predizioni simili (identificabilità parziale). Conviene sempre ispezionare tracce, R-hat, ESS e, se possibile, condurre *posterior predictive checks*.\n\n\n## Dal parametro allo stile cognitivo\n\nI parametri del modello non sono soltanto numeri: possono essere interpretati come *indicatori di stili di apprendimento e decisione*.\n\n* **Learning rate (`lr`)**\n\n  * Un soggetto con `lr` alto è molto sensibile ai singoli feedback: aggiorna le proprie aspettative in modo rapido e può adattarsi velocemente a un *reversal*.\n    *Esempio*: basta un paio di esiti negativi perché abbandoni l’opzione che prima sembrava migliore.\n  * Un soggetto con `lr` basso aggiorna più lentamente: dà più peso all’esperienza accumulata e tende a mantenere le proprie scelte anche di fronte a segnali contrari.\n    *Esempio*: continua a scegliere lo stimolo “vecchio ricco” anche dopo alcuni esiti negativi, adattandosi solo gradualmente.\n\n* **Inverse temperature (`tau`)**\n\n  * Un soggetto con `tau` alto si comporta in modo *deterministico*: sceglie quasi sempre l’opzione con il valore Q più alto. Questo corrisponde a uno stile “sfruttatore” (*exploiter*), focalizzato sul massimizzare subito i guadagni.\n  * Un soggetto con `tau` basso mostra un comportamento più *esplorativo o rumoroso*: anche se una delle due opzioni è chiaramente più vantaggiosa, di tanto in tanto sceglie l’altra. Questo stile può sembrare “incoerente”, ma in certi contesti favorisce l’esplorazione di alternative.\n\n### Messaggio chiave\n\nL’approccio bayesiano permette di stimare, per ogni individuo, un profilo fatto di *apprendimento* (quanto velocemente aggiorna le proprie aspettative) e *decisione* (quanto coerentemente agisce in base a quelle aspettative).\n\nDifferenze sistematiche nei parametri tra *gruppi sperimentali* (es. stimoli emozionanti vs. neutri) o tra *popolazioni cliniche e di controllo* possono rivelare *stili cognitivi distintivi*:\n\n* difficoltà nell’adattarsi a nuovi feedback (lr basso),\n* oppure scelte troppo rigide o troppo esplorative (tau troppo alto o troppo basso).\n\nIn questo modo, un modello computazionale semplice come il Rescorla–Wagner con regola logistica non solo descrive i dati, ma fornisce anche una *chiave di lettura psicologica* dei processi sottostanti.\n\n\n## Contextual Bandits e compiti *food* vs. *neutral*\n\nIl modello di Rescorla–Wagner con regola logistica, come visto nel tutorial, assume due parametri stabili per tutto il compito:\n\n* `lr`, che regola la velocità con cui i valori $Q$ vengono aggiornati,\n* `tau`, che governa la coerenza delle scelte rispetto a tali valori.\n\nLa famiglia dei *banditi contestuali* rappresenta un’estensione naturale: i parametri non sono più fissi, ma possono *variare in funzione del contesto*. In pratica, lo stesso schema RW + logit viene applicato separatamente a diverse condizioni sperimentali (es. stimoli *food* vs. stimoli *neutral*, per un campione di pazienti anoressiche) o a diversi gruppi di partecipanti (es. clinici vs. controlli).\n\nQuesta estensione è stata applicata con successo nello studio dei disturbi alimentari. Analisi recenti hanno mostrato che i *deficit di apprendimento* in anoressia nervosa non sono globali, ma *specifici del contesto alimentare*:\n\n* di fronte a stimoli legati al cibo, le persone con anoressia tendono a mostrare un *learning rate più basso*, cioè aggiornano le aspettative più lentamente;\n* negli stessi compiti con stimoli neutri, invece, i parametri risultano simili a quelli dei controlli [@colpizzi2025food].\n\nQuesta evidenza suggerisce che la vulnerabilità non consista in un deficit generale dei processi di apprendimento, ma in un’alterazione *selettiva e contestuale*, che contribuisce al mantenimento del disturbo.\n\nFormalmente, per il contesto $c \\in \\{\\text{food},\\text{neutral}\\}$ si stima:\n$$\nP_c(B_t)=\\text{inv\\_logit}\\!\\big(\\tau_c [Q_{c,t}(B)-Q_{c,t}(A)]\\big), \n$$\n$$\nQ_{c,t+1}(s)=Q_{c,t}(s)+lr_c \\,[R_{c,t}-Q_{c,t}(s)].\n$$\nIl confronto tra $lr_{\\text{food}}$ e $lr_{\\text{neutral}}$ (e analogamente per $\\tau$) quantifica le differenze contestuali.\n\n### Limiti e varianti minime\n\nIl modello di *Rescorla–Wagner* estende la semplice dinamica di aggiornamento introducendo un *livello decisionale*, che trasforma i valori appresi (*Q-values*) in probabilità di scelta. Questo ci permette di distinguere due componenti fondamentali del comportamento:\n\n1.  **Apprendimento dagli esiti** (*learning*), controllato dal parametro `lr` (*learning rate*), che determina quanto rapidamente il soggetto aggiorna le proprie aspettative in base all'errore di predizione (la differenza tra quanto atteso e quanto effettivamente ottenuto).\n\n    *  Un valore basso di `lr` corrisponde a un apprendimento più lento e stabile, ma meno reattivo ai cambiamenti.\n    *  Un valore alto di `lr` corrisponde a un adattamento veloce, ma a una maggiore sensibilità al rumore.\n\n2.  **Politica decisionale** (*decision policy*), governata dal parametro `tau` (*inverse temperature*), che regola la coerenza tra le scelte e i valori appresi.\n    *   `tau` alto: scelte deterministiche e coerenti con l'opzione migliore (alto sfruttamento).\n    *   `tau` basso: comportamento più esplorativo e meno prevedibile.\n\nQuesta distinzione è cruciale in ambito clinico. In uno studio sull'anoressia nervosa e l'apprendimento, ad esempio, abbiamo osservato che le difficoltà non sono generalizzate, ma *specifiche del contesto alimentare*: le pazienti presentano un tasso di apprendimento ridotto (`lr` più basso) quando gli stimoli sono legati al cibo, mentre il loro comportamento è simile a quello dei controlli in contesti neutri [@colpizzi2025food].\n\nIl modello RW offre quindi un'analisi più ricca della semplice prestazione media:\n\n*   consente di identificare differenze nei *meccanismi di apprendimento* (`lr`) tra diverse condizioni;\n*   permette di verificare se a queste diffeerenze si associano anche differenze nello *stile decisionale* (`tau`).\n\nPer uno psicologo, questo si traduce in due vantaggi concreti:\n\n*   **Chiarezza teorica:** permette di formulare ipotesi precise e separate sui processi di apprendimento e sulle strategie decisionali, andando oltre le semplici misure di accuratezza.\n*   **Rilevanza clinica:** distinguere un deficit nell'apprendimento da uno nella scelta ha implicazioni dirette per la progettazione di interventi mirati (es., potenziare la sensibilità al feedback o riequilibrare l'esplorazione e lo sfruttamento).\n\nIn sintesi, il modello Rescorla-Wagner non è solo uno strumento computazionale: è un *ponte fra comportamento osservato e processi mentali*, fondamentale per isolare deficit contestuali specifici e comprendere come le strategie decisionali varino tra individui e gruppi clinici.\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] cmdstanr_0.9.0        pillar_1.11.0         tinytable_0.13.0     \n#>  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#>  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [25] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        systemfonts_1.2.3    \n#> [10] vctrs_0.6.5           stringr_1.5.1         pkgconfig_2.0.3      \n#> [13] arrayhelpers_1.1-0    fastmap_1.2.0         backports_1.5.0      \n#> [16] labeling_0.4.3        utf8_1.2.6            rmarkdown_2.29       \n#> [19] ps_1.9.1              ragg_1.5.0            purrr_1.1.0          \n#> [22] xfun_0.53             cachem_1.1.0          jsonlite_2.0.0       \n#> [25] broom_1.0.9           parallel_4.5.1        R6_2.6.1             \n#> [28] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      \n#> [31] estimability_1.5.1    knitr_1.50            zoo_1.8-14           \n#> [34] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#> [37] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#> [40] codetools_0.2-20      curl_7.0.0            processx_3.8.6       \n#> [43] pkgbuild_1.4.8        lattice_0.22-7        withr_3.0.2          \n#> [46] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [49] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [52] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [55] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [58] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [61] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#> [64] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#> [67] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#> [70] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#> [73] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#> [76] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#> [79] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#> [82] MASS_7.3-65\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n",
    "supporting": [
      "03_rescorla_wagner_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}