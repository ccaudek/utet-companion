{
  "hash": "8c75d7721f02074d5f23b56c86d7c5ed",
  "result": {
    "engine": "knitr",
    "markdown": "# Errori di segno e errori di grandezza {#sec-crisis-s-m-errors}\n\n::: callout-important\n## In questo capitolo imparerai a\n\n- Relazione tra crisi della replicabilità e approccio frequentista.\n- Limiti della significatività statistica.\n- Errori di grandezza e errori di segno.\n- Utilizzo dell'approccio bayesiano per ottenere stime più precise e affidabili.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere [Horoscopes](../../figures/horoscopes.pdf). Alla luce degli approfondimenti forniti in questo corso, questo capitolo di @McElreath_rethinking dovrebbe assumere un significato diverso rispetto a quando è stato letto all'inizio del corso.\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyr)\n```\n:::\n\n:::\n\n## Introduzione\n\nIn questo capitolo analizzeremo la relazione tra la crisi della replicabilità e le procedure decisionali statistiche proprie dell'approccio frequentista. In particolare, approfondiremo gli errori di tipo *M* (*magnitude*) e di tipo *S* (*sign*), discussi da @loken2017measurement, e il loro impatto sulla validità dei risultati scientifici.\n\n::: callout-tip\n## Domande Iniziali\n\nPrima di esplorare le simulazioni e i risultati discussi in questo capitolo, prova a riflettere sulle seguenti domande. Ti invitiamo a formulare delle ipotesi sui risultati delle simulazioni prima di leggere le spiegazioni:\n\n1. Se si effettuano molteplici studi su un effetto molto piccolo utilizzando campioni di dimensioni ridotte, cosa pensi che accadrà ai risultati pubblicati che ottengono significatività statistica? Saranno accurati rispetto alla vera grandezza dell'effetto?\n2. In uno scenario in cui non esiste alcuna differenza tra due gruppi, quanto spesso credi che un test t fornisca un risultato statisticamente significativo che verrà pubblicato? Quali fattori potrebbero influenzare questa probabilità?\n3. Supponiamo che in una serie di esperimenti alcuni studi trovino effetti significativi e altri no. Quale pensi sia la tendenza degli studi pubblicati rispetto a quelli non pubblicati? Quale impatto può avere questa tendenza sulla percezione della realtà scientifica?\n4. Se dovessi valutare la replicabilità di uno studio basato sulla significatività statistica, quali problemi potresti incontrare se l’effetto sottostante è molto piccolo?\n\nTieni a mente le tue risposte mentre esplori le simulazioni presentate in questo capitolo. Alla fine del capitolo, confronteremo le previsioni con i risultati effettivi.\n:::\n\n## Il Filtro della Significatività Statistica\n\nNel @sec-crisis abbiamo esplorato come la pratica scientifica contemporanea sia spesso compromessa da casi di frode, principalmente a causa delle significative implicazioni economiche legate alla pubblicazione su riviste scientifiche di alto prestigio. Questo fenomeno è spesso sottovalutato, poiché le riviste tendono a essere riluttanti nel riconoscere la necessità di correzioni o ritrattazioni degli articoli già pubblicati.\n\nLa frode scientifica rappresenta una minaccia evidente alla riproducibilità dei risultati, un pilastro fondamentale del metodo scientifico. Tuttavia, le difficoltà nel replicare i risultati pubblicati non sono attribuibili esclusivamente a frodi o a \"pratiche di ricerca disoneste\" [@nelson2018psychology]. Un problema intrinseco risiede nel metodo statistico ampiamente adottato dai ricercatori: l'approccio del test di ipotesi nulla e della significatività statistica di stampo fisheriano. Secondo questo metodo, i risultati che non raggiungono la soglia di \"significatività statistica\" vengono scartati, mentre quelli che la superano sono considerati credibili, basandosi esclusivamente su questo criterio [@wagenmakers2008bayesian].\n\nTuttavia, l'idea che la significatività statistica sia un filtro affidabile per distinguere i risultati di ricerca \"validi\" da quelli \"non validi\" è fondamentalmente errata. Numerose evidenze dimostrano i limiti di questo approccio. Per approfondire questa problematica, esamineremo lo studio di @loken2017measurement, che mette in luce la relazione tra la crisi della replicabilità e le procedure decisionali statistiche dell'approccio frequentista.\n\nUno dei principali problemi evidenziati da @loken2017measurement è che, in contesti di ricerca complessi, la significatività statistica fornisce prove molto deboli riguardo al segno (*sign*) o all'entità (*magnitude*) degli effetti sottostanti. In altre parole, il raggiungimento della significatività statistica non garantisce né la rilevanza né la consistenza dei risultati ottenuti. Questo solleva seri dubbi sull'affidabilità di tale criterio come unico strumento per valutare la validità delle scoperte scientifiche.\n\n## Errori di tipo *M* e *S*\n\nPer illustrare le implicazioni del processo decisionale basato sulla significatività statistica, @loken2017measurement hanno condotto una simulazione. In questa simulazione, hanno considerato uno scenario di ricerca ipotetico in cui era presente un effetto reale, sebbene molto debole, difficilmente rilevabile senza un ampio volume di dati. Utilizzando l'approccio frequentista, hanno cercato di identificare questo effetto valutando la significatività statistica.\n\nI risultati della simulazione hanno mostrato che, anche in presenza di un effetto reale (seppur debole), l'approccio frequentista riusciva a rilevare un effetto statisticamente significativo solo in una piccola percentuale dei casi. Inoltre, quando un effetto significativo veniva individuato, la stima della sua grandezza risultava altamente imprecisa e instabile.\n\nIn sintesi, la significatività statistica fornisce un'indicazione generica sulla presenza o assenza di un effetto, ma non offre informazioni affidabili sulla sua entità o replicabilità. Questo problema è particolarmente rilevante in campi come la psicologia e le scienze sociali, dove gli studi spesso si basano su campioni di dimensioni ridotte e gli effetti osservati tendono a essere modesti. In tali contesti, l'approccio frequentista rischia di produrre prove deboli e instabili, compromettendo la replicabilità e l'affidabilità dei risultati.\n\n### Simulazione semplificata\n\nRiproduciamo qui, in forma semplificata, la simulazione condotta da @loken2017measurement. Iniziamo importando le librerie necessarie.\n\nConsideriamo due campioni casuali indipendenti di dimensioni $n_1 = 20$ e $n_2 = 25$, estratti rispettivamente dalle distribuzioni normali $\\mathcal{N}(102, 10)$ e $\\mathcal{N}(100, 10)$. La dimensione effettiva dell'effetto ($d$) per la differenza tra le medie dei due campioni è calcolata utilizzando la formula:\n\n$$\nd = \\frac{\\bar{y}_1 - \\bar{y}_2}{s_p},\n$$\n\ndove $\\bar{y}_1$ e $\\bar{y}_2$ rappresentano le medie campionarie dei due gruppi, e $s_p$ è la deviazione standard combinata, definita come:\n\n$$\ns_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}.\n$$\n\nIn questo caso specifico, la dimensione effettiva dell'effetto risulta molto piccola, indicando che la differenza osservata tra le medie dei due gruppi non ha una rilevanza pratica significativa. Ciò suggerisce che la distinzione tra i due gruppi, seppur statisticamente rilevabile, non ha un impatto sostanziale in contesti reali.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nmu_1 <- 102  # Media del primo gruppo\nmu_2 <- 100  # Media del secondo gruppo\nsigma <- 10  # Deviazione standard comune\nn1 <- 20     # Numero di osservazioni nel primo gruppo\nn2 <- 25     # Numero di osservazioni nel secondo gruppo\n\n# Calcolo della differenza media\nmean_difference <- abs(mu_1 - mu_2)\n\n# Calcolo della deviazione standard pooled\npooled_sd <- sqrt(((n1 - 1) * sigma^2 + (n2 - 1) * sigma^2) / (n1 + n2 - 2))\n\n# Calcolo di Cohen's d\ncohen_d <- mean_difference / pooled_sd\n\n# Output del risultato\ncat(\"Dimensione dell'effetto (Cohen's d):\", cohen_d, \"\\n\")\n#> Dimensione dell'effetto (Cohen's d): 0.2\n```\n:::\n\n\nEsaminiamo ora le conclusioni che emergerebbero applicando l'approccio frequentista e la sua procedura di decisione statistica in questo contesto. Supponiamo di condurre una simulazione in cui vengono estratti due campioni: il primo composto da 20 osservazioni provenienti dalla prima popolazione e il secondo da 25 osservazioni provenienti dalla seconda popolazione. Successivamente, applichiamo il test $t$ di Student per confrontare le medie dei due gruppi.\n\nNell'ambito dell'approccio frequentista, il valore-$p$ ottenuto dal test determina la decisione statistica. Se il valore-$p$ è superiore a 0.05, i risultati vengono considerati non significativi e, di conseguenza, scartati. Al contrario, se il valore-$p$ è inferiore a 0.05, il risultato è ritenuto \"pubblicabile\" e si conclude che esiste una differenza statisticamente significativa tra i due gruppi.\n\nPer valutare in modo approfondito le conclusioni derivate da questa procedura, è necessario ripetere l'intero processo per un numero elevato di iterazioni, ad esempio 50.000 volte. Ciò significa che, in ciascuna iterazione, vengono estratti nuovi campioni, viene calcolato il test $t$ di Student e viene determinato il corrispondente valore-$p$. Ripetendo questo processo su larga scala, è possibile ottenere una distribuzione completa dei risultati, che consente di analizzare la frequenza con cui si ottengono risultati significativi e la stabilità delle stime prodotte dall'approccio frequentista in questo contesto.\n\n\n::: {.cell layout-align=\"center\" tags='hide-output'}\n\n```{.r .cell-code}\n# Parametri\nn_samples <- 50000\nmu_1 <- 102\nmu_2 <- 100\nsigma <- 10\nn1 <- 20\nn2 <- 25\n\n# Inizializzazione del risultato\nres <- c()\n\n# Simulazioni\nset.seed(123)  # Per la riproducibilità\nfor (i in 1:n_samples) {\n  # Generazione dei campioni casuali\n  y1 <- rnorm(n1, mean = mu_1, sd = sigma)\n  y2 <- rnorm(n2, mean = mu_2, sd = sigma)\n  \n  # Calcolo della dimensione dell'effetto\n  y1bar <- mean(y1)\n  y2bar <- mean(y2)\n  v1 <- var(y1)\n  v2 <- var(y2)\n  s <- sqrt(((n1 - 1) * v1 + (n2 - 1) * v2) / (n1 + n2 - 2))\n  efsize <- (y1bar - y2bar) / s\n  \n  # Calcolo del valore p\n  t_test <- t.test(y1, y2, var.equal = TRUE)\n  \n  # Salvataggio della dimensione dell'effetto solo per risultati \"statisticamente significativi\"\n  if (t_test$p.value < 0.05) {\n    res <- c(res, efsize)\n  }\n}\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres_df <- data.frame(effect_size = res)\n\nggplot(res_df, aes(x = effect_size)) +\n  geom_histogram(bins = 20, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  geom_vline(\n    xintercept = 0.2, color = \"red\", linetype = \"dashed\", \n    size = 1.2, label = \"True Effect Size\") +\n  labs(\n    x = \"Effect Size\",\n    y = \"Frequency\",\n    title = \"Histogram of Effect Sizes for\\n'Statistically Significant' Results\"\n  ) \n```\n\n::: {.cell-output-display}\n![](04_s_m_errors_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nCome evidenziato da @loken2017measurement, l’applicazione dell’approccio frequentista nella procedura di decisione statistica può condurre a due tipi di errori rilevanti. Il primo, noto come errore di *magnitude* (grandezza), si manifesta quando i risultati pubblicati tendono a sovrastimare la reale entità dell’effetto. Nella simulazione condotta, nonostante la vera grandezza dell’effetto fosse modesta (0.2), la media della grandezza dell’effetto per i risultati classificati come \"statisticamente significativi\" era circa 0.8, suggerendo un effetto di entità \"ampia\". Questo indica una distorsione sistematica verso stime esagerate.\n\nIl secondo errore, chiamato errore di *segno*, si verifica quando, a causa della variabilità campionaria, la direzione dell’effetto viene stimata in modo errato. In tali casi, il ricercatore potrebbe erroneamente concludere che $\\mu_2 > \\mu_1$, quando in realtà non è così. È importante sottolineare che, anche in queste situazioni, la grandezza assoluta dell’effetto risulta sovrastimata.\n\nUn aspetto degno di nota è che queste conclusioni rimarrebbero valide anche se si considerasse l’intervallo di confidenza per la differenza tra le medie. In sintesi, l’approccio frequentista introduce un errore sistematico nella stima della grandezza dell’effetto, che rappresenta la quantità più rilevante per il ricercatore. In alcuni casi, può persino portare a errori nella determinazione della direzione dell’effetto, compromettendo ulteriormente l’affidabilità delle conclusioni scientifiche.\n\n## Riflessioni Conclusive\n\nIn conclusione, l’approccio frequentista non rappresenta un metodo affidabile per valutare i risultati della ricerca e determinarne l’attendibilità o la necessità di scartarli [@gelman2014beyond; @loken2017measurement]. Questa mancanza di affidabilità è dovuta all’introduzione di errori sistematici nella stima della grandezza degli effetti, che in alcuni casi possono persino portare a errori nella direzione dell’effetto stesso. Alla luce di queste criticità, non sembrano esserci motivi validi per continuare a fare affidamento su questo approccio.\n\nAl contrario, l’adozione dell’approccio bayesiano sembra offrire una soluzione più precisa e affidabile per l’analisi dei dati di ricerca. Questo metodo valuta la probabilità delle ipotesi alla luce dei dati osservati, evitando gli errori intrinseci dell’approccio frequentista e fornendo una base più solida per prendere decisioni informate sulla validità dei risultati. In questo modo, l’approccio bayesiano si presenta come un’alternativa più robusta e scientificamente rigorosa.\n\n::: {.callout-tip title=\"Risposte alle domande iniziali\" collapse=\"true\"}\nOra confrontiamo le previsioni con i risultati ottenuti dalle simulazioni:\n\n1. **Sovrastima della grandezza dell’effetto**: I risultati pubblicati tendono a essere selezionati sulla base della significatività statistica, il che porta a una sovrastima sistematica della grandezza dell’effetto rispetto alla realtà. Questo fenomeno, noto come errore di tipo *M* (magnitude), si verifica perché solo gli effetti con valori estremi (per caso) superano la soglia di significatività statistica e vengono pubblicati.\n   \n2. **Falsi positivi e loro frequenza**: In uno scenario in cui non esiste alcuna differenza tra i due gruppi (cioè la vera differenza è zero), il test t ha fornito risultati statisticamente significativi nel 5% dei casi, come previsto dalla soglia di α = 0.05. Tuttavia, la selezione dei risultati pubblicati amplifica questo problema, rendendo più probabile che i lettori incontrino falsi positivi nella letteratura scientifica.\n   \n3. **Bias nella pubblicazione**: Gli studi che riportano risultati significativi hanno maggiore probabilità di essere pubblicati rispetto a quelli che non trovano un effetto significativo. Questo porta a un effetto distorsivo nella letteratura scientifica, in cui i risultati pubblicati tendono a sovrastimare l’effetto reale. Il \"filtro della significatività statistica\" crea una percezione distorta della realtà scientifica, poiché gli effetti nulli o piccoli tendono a essere sottorappresentati nella letteratura.\n   \n4. **Replicabilità e significatività statistica**: Gli studi con effetti piccoli e campioni ridotti sono particolarmente vulnerabili al fallimento della replicazione. Anche quando un effetto reale esiste, la probabilità di ottenerne una stima precisa è bassa, e la replicazione potrebbe risultare in un valore non significativo, generando confusione nella comunità scientifica.\n\n**Considerazioni Finali**\n\nLe simulazioni evidenziano come la significatività statistica, utilizzata come criterio per la pubblicazione, contribuisca a un bias nella selezione dei risultati, distorcendo la percezione della realtà scientifica. Questo fenomeno, noto come \"filtro della significatività statistica\", è una delle cause principali della crisi della replicabilità, poiché induce i ricercatori e i lettori a sovrastimare la grandezza e la presenza di effetti studiati. \n\nPer affrontare queste problematiche, approcci alternativi, come quelli bayesiani, possono offrire soluzioni più robuste, permettendo una valutazione più affidabile delle ipotesi alla luce dei dati osservati.\n:::\n\n\n## Esercizi {.unnumbered}\n\n::: {.callout-important title=\"Problemi 1\" collapse=\"true\"}\n1. Perché la significatività statistica non è un criterio affidabile per valutare la validità dei risultati scientifici?\n2. Spiega il concetto di \"filtro della significatività statistica\" e il suo impatto sulla pubblicazione dei risultati.\n3. Qual è la differenza tra errore di tipo *M* e errore di tipo *S*? Come influenzano l’interpretazione dei risultati?\n4. Perché i risultati pubblicati tendono a sovrastimare la grandezza dell’effetto rispetto alla realtà?\n5. Quali sono le conseguenze della pubblicazione selettiva dei risultati per la replicabilità degli studi?\n6. Perché gli studi con campioni di piccole dimensioni sono più vulnerabili a errori nella stima della grandezza dell’effetto?\n7. In che modo la selezione dei risultati pubblicati altera la percezione della forza degli effetti studiati?\n8. Perché un test frequentista può portare a una falsa conclusione sulla direzione di un effetto?\n9. Quali sono le principali differenze tra l’approccio frequentista e quello bayesiano nella valutazione della significatività di un effetto?\n10. In che modo l’approccio bayesiano può ridurre il rischio di errori dovuti alla selezione dei risultati basata sulla significatività statistica?\n:::\n\n::: {.callout-tip title=\"Soluzioni 1\" collapse=\"true\"}\n1. **La significatività statistica non garantisce la validità di un risultato** perché dipende dalla dimensione del campione e da soglie arbitrarie (come p < 0.05). Inoltre, non misura la rilevanza pratica di un effetto, ma solo la probabilità che i dati osservati siano ottenuti sotto l’ipotesi nulla.\n   \n2. **Il \"filtro della significatività statistica\"** si riferisce alla tendenza a pubblicare solo risultati con p < 0.05, tralasciando studi con risultati non significativi. Questo porta a una distorsione nella letteratura scientifica e a una sovrastima della forza degli effetti riportati.\n   \n3. **Errore di tipo *M* (Magnitude)** indica la sovrastima della grandezza dell’effetto nei risultati pubblicati, mentre **errore di tipo *S* (Sign)** si riferisce all’errata determinazione della direzione dell’effetto. Questi errori si verificano perché solo gli effetti più estremi tendono a superare il filtro della significatività statistica.\n   \n4. **I risultati pubblicati tendono a sovrastimare la grandezza dell’effetto** perché solo gli effetti più grandi (anche per pura casualità) superano la soglia di significatività statistica e vengono pubblicati, mentre quelli più piccoli restano inediti.\n   \n5. **La pubblicazione selettiva riduce la replicabilità** perché introduce una distorsione sistematica nei risultati disponibili. Le repliche spesso non trovano effetti altrettanto grandi o significativi, creando instabilità nella conoscenza scientifica.\n   \n6. **I campioni piccoli aumentano la variabilità delle stime dell’effetto**, rendendo più probabile che un risultato significativo sia solo un’oscillazione casuale dei dati piuttosto che un vero effetto replicabile.\n   \n7. **La selezione dei risultati pubblicati altera la percezione della forza degli effetti** perché induce i lettori a credere che gli effetti siano più forti e consistenti di quanto non siano realmente.\n   \n8. **Un test frequentista può portare a una falsa conclusione sulla direzione dell’effetto** perché, in campioni piccoli, le stime dell’effetto possono essere fortemente influenzate dal rumore, portando a interpretazioni errate.\n   \n9. **L’approccio frequentista si basa sul valore-p e sulla soglia di significatività, mentre l’approccio bayesiano utilizza la probabilità a posteriori** per aggiornare la credibilità delle ipotesi alla luce dei dati osservati. Il metodo bayesiano permette inferenze più flessibili e robuste.\n   \n10. **L’approccio bayesiano riduce il rischio di errori dovuti alla selezione dei risultati** perché non si basa su una soglia arbitraria di significatività, ma fornisce un quadro probabilistico della forza dell’effetto, evitando distorsioni dovute alla pubblicazione selettiva.\n:::\n\n::: {.callout-important title=\"Problemi 2\" collapse=\"true\"}\n\nIn questo esercizio simuleremo più esperimenti, ognuno con **15 osservazioni**, per comprendere come il filtro della significatività statistica possa distorcere le nostre conclusioni sugli effetti osservati.\n\n**Obiettivo**\n\n- Comprendere come l'approccio frequentista possa portare a stime errate dell'effetto reale.\n- Esplorare gli errori di tipo *M* (magnitude) e *S* (sign), derivanti dal filtro della significatività statistica.\n\n**Struttura dell'esercizio**\n\n1. Simuliamo esperimenti in cui il **vero effetto** tra due gruppi è piccolo (Cohen’s *d* = 0.2). Consideriamo i dati SWLS e due popolazioni che differiscono nel modo indicato. Ipotizziamo che le due popolazioni SWLS siano normali. \n2. Estraiamo 15 osservazioni per gruppo.\n3. Usiamo un test *t* per verificare se la differenza tra i gruppi è significativa.\n4. Registriamo solo i risultati con *p* < 0.05, calcolando la distribuzione degli effetti significativi.\n5. Valutiamo se la stima dell'effetto nei risultati pubblicabili è gonfiata rispetto al vero effetto.\n6. Contiamo i casi in cui il segno dell'effetto è invertito.\n:::\n\n::: {.callout-tip title=\"Soluzioni 2\" collapse=\"true\"}\n\n```r\n# Librerie necessarie\nlibrary(ggplot2)\n\n# Impostazioni della simulazione\nset.seed(42)         # Per la riproducibilità\nn_sims <- 50000      # Numero di simulazioni\nn_per_group <- 15    # Numero di osservazioni per gruppo\ntrue_d <- 0.2        # Vero effetto (Cohen's d)\nswls_mean <- 25      # Media ipotizzata per il primo gruppo\nswls_sd <- 5         # Deviazione standard ipotizzata per la SWLS\n\n# Calcolo della media del secondo gruppo sulla base di Cohen's d\nswls_mean_2 <- swls_mean + true_d * swls_sd\n\n# Inizializziamo i vettori per registrare i risultati\neffect_sizes <- c()\nfalse_sign_count <- 0\n\n# Simulazioni\nfor (i in 1:n_sims) {\n  # Generazione dei due gruppi da distribuzioni normali\n  group1 <- rnorm(n_per_group, mean = swls_mean, sd = swls_sd)\n  group2 <- rnorm(n_per_group, mean = swls_mean_2, sd = swls_sd)\n  \n  # Calcolo della dimensione dell'effetto (Cohen's d)\n  mean_diff <- mean(group2) - mean(group1)\n  pooled_sd <- sqrt(((n_per_group - 1) * var(group1) + (n_per_group - 1) * var(group2)) / (2 * n_per_group - 2))\n  d_estimated <- mean_diff / pooled_sd\n  \n  # Test t per confrontare le medie\n  t_test <- t.test(group1, group2, var.equal = TRUE)\n  \n  # Consideriamo solo i risultati statisticamente significativi\n  if (t_test$p.value < 0.05) {\n    effect_sizes <- c(effect_sizes, d_estimated)\n    \n    # Conta i casi in cui il segno è invertito\n    if (d_estimated < 0) {\n      false_sign_count <- false_sign_count + 1\n    }\n  }\n}\n\n# Creazione di un dataframe per la visualizzazione\nres_df <- data.frame(effect_size = effect_sizes)\n\n# Istogramma della dimensione dell'effetto tra i risultati \"significativi\"\nggplot(res_df, aes(x = effect_size)) +\n  geom_histogram(bins = 30, fill = \"blue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = true_d, color = \"red\", linetype = \"dashed\", size = 1.2) +\n  labs(\n    x = \"Dimensione dell'effetto stimata\",\n    y = \"Frequenza\",\n    title = \"Distribuzione degli effetti significativi (SWLS)\"\n  ) +\n  theme_minimal()\n\n# Output di sintesi\ncat(\"Numero di risultati statisticamente significativi:\", length(effect_sizes), \"\\n\")\ncat(\"Media della dimensione dell'effetto tra i risultati pubblicati:\", mean(effect_sizes), \"\\n\")\ncat(\"Numero di risultati con segno invertito:\", false_sign_count, \"\\n\")\ncat(\"Proporzione di risultati con segno invertito:\", false_sign_count / length(effect_sizes), \"\\n\")\n```\n\n**Interpretazione dei Risultati**\n\n1. **Errore di tipo *M*** (Magnitude): La media degli effetti stimati nei risultati pubblicati sarà molto più grande di 0.2 (il vero effetto), dimostrando come il filtro della significatività tenda a sovrastimare gli effetti reali.\n2. **Errore di tipo *S*** (Sign): Una percentuale dei risultati pubblicabili mostrerà effetti nella direzione sbagliata (d < 0), dimostrando che il processo decisionale basato su *p* < 0.05 può portare a conclusioni errate.\n3. **Visualizzazione**: L'istogramma mostrerà che la distribuzione degli effetti significativi è spostata rispetto al vero effetto (linea rossa tratteggiata).\n\n**Domande di discussione**:\n\n  1. Perché la stima dell’effetto è gonfiata nei risultati pubblicabili?\n  2. Come cambia la situazione aumentando il numero di osservazioni per gruppo?\n  3. Quali strategie alternative potrebbero ridurre questi errori?\n\n**Approfondimento**:\n\n  - Ripetere l'esperimento con *n_per_group = 50* e osservare se l'errore di tipo *M* diminuisce.\n  - Confrontare questo approccio con un'analisi Bayesiana per evidenziare il ruolo dell'inferenza basata su probabilità posteriori.\n\n**Conclusione**\n\nQuesto esercizio mostra chiaramente i problemi dell’approccio frequentista basato su *p* < 0.05, evidenziando i limiti dell’uso della significatività statistica come filtro decisionale.\n:::\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] pillar_1.11.0         tinytable_0.13.0      patchwork_1.3.2      \n#>  [4] ggdist_3.3.3          tidybayes_3.0.7       bayesplot_1.14.0     \n#>  [7] ggplot2_3.5.2         reliabilitydiag_0.2.1 priorsense_1.1.1     \n#> [10] posterior_1.6.1       loo_2.8.0             rstan_2.32.7         \n#> [13] StanHeaders_2.32.10   brms_2.22.0           Rcpp_1.1.0           \n#> [16] sessioninfo_1.2.3     conflicted_1.2.0      janitor_2.2.1        \n#> [19] matrixStats_1.5.0     modelr_0.1.11         tibble_3.3.0         \n#> [22] dplyr_1.1.4           tidyr_1.3.1           rio_1.2.3            \n#> [25] here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#> [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#> [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#> [16] tools_4.5.1           yaml_2.3.10           knitr_1.50           \n#> [19] labeling_0.4.3        bridgesampling_1.1-2  htmlwidgets_1.6.4    \n#> [22] curl_7.0.0            pkgbuild_1.4.8        RColorBrewer_1.1-3   \n#> [25] abind_1.4-8           multcomp_1.4-28       withr_3.0.2          \n#> [28] purrr_1.1.0           grid_4.5.1            stats4_4.5.1         \n#> [31] colorspace_2.1-1      xtable_1.8-4          inline_0.3.21        \n#> [34] emmeans_1.11.2-8      scales_1.4.0          MASS_7.3-65          \n#> [37] cli_3.6.5             mvtnorm_1.3-3         rmarkdown_2.29       \n#> [40] ragg_1.5.0            generics_0.1.4        RcppParallel_5.1.11-1\n#> [43] cachem_1.1.0          stringr_1.5.1         splines_4.5.1        \n#> [46] parallel_4.5.1        vctrs_0.6.5           V8_7.0.0             \n#> [49] Matrix_1.7-4          sandwich_3.1-1        jsonlite_2.0.0       \n#> [52] arrayhelpers_1.1-0    systemfonts_1.2.3     glue_1.8.0           \n#> [55] codetools_0.2-20      distributional_0.5.0  lubridate_1.9.4      \n#> [58] stringi_1.8.7         gtable_0.3.6          QuickJSR_1.8.0       \n#> [61] htmltools_0.5.8.1     Brobdingnag_1.2-9     R6_2.6.1             \n#> [64] textshaping_1.0.3     rprojroot_2.1.1       evaluate_1.0.5       \n#> [67] lattice_0.22-7        backports_1.5.0       memoise_2.0.1        \n#> [70] broom_1.0.9           snakecase_0.11.1      rstantools_2.5.0     \n#> [73] coda_0.19-4.1         gridExtra_2.3         nlme_3.1-168         \n#> [76] checkmate_2.3.3       xfun_0.53             zoo_1.8-14           \n#> [79] pkgconfig_2.0.3\n```\n:::\n\n\n## Bibliografia {.unnumbered}\n\n",
    "supporting": [
      "04_s_m_errors_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}