{
  "hash": "9b09dbb67396eeac80c4ba9d0a241096",
  "result": {
    "engine": "knitr",
    "markdown": "# La regressione lineare bivariata {#sec-linear-models-biv-model-frequentist}\n\n## Introduzione {.unnumbered .unlisted}\n\nLa *regressione lineare* è uno degli strumenti più diffusi della statistica applicata. La sua funzione è descrivere come varia, *in media*, una variabile quantitativa — detta *dipendente* e indicata con $Y$ — al variare di un’altra variabile quantitativa — detta *indipendente* e indicata con $X$. Il modello esprime quindi una relazione media tra due grandezze, lasciando spazio a una quota di variabilità residua inevitabile nei dati psicologici e sociali.\n\nIn psicologia, la regressione lineare è stata adottata in modo estensivo per mettere in relazione costrutti teorici, per costruire previsioni empiricamente fondate, o per verificare ipotesi sulle dinamiche cognitive, emotive o sociali. Proprio per la sua pervasività, questo strumento rappresenta spesso la “lingua comune” della ricerca quantitativa.\n\nÈ importante, tuttavia, collocare la regressione nel giusto contesto epistemologico. Si tratta di un *modello fenomenologico*, che descrive associazioni tra variabili ma non fornisce di per sé spiegazioni sui meccanismi che le hanno generate. Inoltre, il modello si fonda su precise assunzioni matematiche e statistiche: comprenderle è fondamentale per utilizzare la regressione in modo consapevole e per riconoscerne i limiti. Solo così diventa possibile distinguere ciò che il modello ci dice davvero dai significati che rischiamo di attribuirgli indebitamente.\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Il modello di regressione lineare secondo l’approccio frequentista.\n- La stima dei coefficienti del modello utilizzando il metodo dei minimi quadrati.\n- L'interpretazione dei coefficienti dei minimi quadrati.\n- Il calcolo e l'interpretazione dell’indice di determinazione ($R^2$).\n- L'inferenza frequentista sui coefficienti dei minimi quadrati.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere il capitolo *Basic Regression* di [Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Second Edition)](https://moderndive.com/v2/).\n- Consulta l'appendice @sec-apx-lin-func per un'introduzione alle funzioni lineari.\n- Leggere *Navigating the Bayes maze: The psychologist's guide to Bayesian statistics, a hands-on tutorial with R code* [@alter2025navigating].\n- Leggere il capitolo *Linear Statistical Models* [@schervish2014probability].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(broom)\n```\n:::\n\n:::\n\n\n## La relazione tra $X$ e $Y$\n\nIl caso più semplice è il *modello di regressione lineare bivariato*, che descrive la relazione tra due variabili. L’idea di base è che, se $X$ e $Y$ sono associate, possiamo approssimare la loro relazione con una *retta* che indica come $Y$ tende a variare al variare di $X$.\n\nLa formulazione classica (frequentista) del modello è:\n\n$$\ny_i = a + b x_i + e_i, \\quad i = 1, \\dots, n,\n$$\n\\noindent\ndove:\n\n* $a$ (intercetta): valore atteso di $Y$ quando $X = 0$,\n* $b$ (pendenza): variazione attesa di $Y$ per ogni unità di aumento in $X$,\n* $e_i$ (errore residuo): differenza tra il valore osservato $y_i$ e il valore previsto dal modello.\n\nGraficamente, questa equazione corrisponde a una *retta di regressione* che rappresenta la miglior approssimazione lineare dei dati secondo il criterio dei minimi quadrati. Nella realtà, i punti raramente giacciono tutti sulla retta: le differenze sono catturate dagli errori residui.\n\nLa regressione, quindi, *non* predice esattamente ogni osservazione, ma descrive la *tendenza media* nella popolazione. Ad esempio, se $b = 2$, significa che — *in media* — un aumento di 1 unità in $X$ è associato a un aumento di 2 unità in $Y$. Ciò non implica che ogni caso segua la regola in modo perfetto, ma che l’andamento complessivo sia coerente con questa relazione.\n\nL’obiettivo dell’analisi è *stimare* i parametri $a$, $b$ e $\\sigma^2$ (varianza residua) dai dati, utilizzando il *metodo dei minimi quadrati (OLS)* o, più in generale, il *principio di massima verosimiglianza*. Nel paradigma frequentista, questi parametri sono considerati quantità fisse ma sconosciute, e l’incertezza riguarda esclusivamente gli errori di misura o variabilità non spiegata.\n\n\n## A cosa serve la regressione?\n\nSecondo @gelman2021regression, la regressione lineare può essere applicata in almeno quattro contesti principali:\n\n1. **Previsione** – Stimare valori futuri di una variabile di interesse o classificare casi in base a probabilità.\n\n   * *Esempi*: prevedere punteggi futuri a un test; monitorare il benessere psicologico in studi longitudinali; classificare individui in base alla probabilità di successo in un compito cognitivo.\n\n2. **Esplorazione delle associazioni** – Identificare e quantificare le relazioni tra predittori e risultato.\n\n   * *Esempi*: analizzare i tratti di personalità legati alla resilienza allo stress; studiare la relazione tra stili di attaccamento infantile e competenze relazionali adulte; valutare l’effetto di fattori socio-economici sullo sviluppo cognitivo.\n\n3. **Estrapolazione** – Estendere i risultati a contesti o popolazioni non direttamente osservati.\n\n   * *Esempi*: stimare l’efficacia di una terapia testata su studenti universitari nella popolazione generale; prevedere l’impatto di un intervento scolastico su un intero distretto a partire da dati di scuole pilota.\n\n4. **Inferenza causale** – Stimare effetti di trattamenti o interventi, *solo* se supportata da un disegno di ricerca adeguato (ad es., randomizzazione).\n\n   * *Esempi*: valutare l’efficacia di un programma di mindfulness sull’ansia; stimare l’impatto di una psicoterapia sul disturbo post-traumatico da stress; determinare l’effetto di un intervento educativo su una popolazione diversificata.\n\n**Nota importante:** in tutti i contesti, il modello deve includere *tutte le variabili rilevanti* per il fenomeno studiato. L’omissione di variabili confondenti può distorcere le stime — problema noto come *errore di specificazione del modello*. Ad esempio, in uno studio sull’efficacia di una psicoterapia per la depressione, fattori come età, condizioni di salute preesistenti e supporto sociale devono essere inclusi nell’analisi per evitare interpretazioni fuorvianti.\n\n\n### Tipologie di regressione\n\nL'analisi di regressione può essere classificata in base al numero di variabili coinvolte, distinguendosi in tre principali categorie che riflettono diversi livelli di complessità.\n\nLa *regressione bivariata* rappresenta la forma più elementare, in cui viene analizzata la relazione tra un unico predittore e una sola variabile esito. La sua semplicità la rende un punto di partenza ideale per comprendere la logica fondamentale della regressione, poiché consente di apprendere i concetti di base senza l’onere di complicazioni matematiche eccessive.\n\nUn livello di complessità superiore è dato dalla *regressione multipla*, nella quale un unico esito viene modellato utilizzando molteplici predittori. Questo approccio permette di esaminare l’effetto simultaneo di diverse variabili indipendenti, richiedendo un’attenta interpretazione dei parametri stimati.\n\nInfine, la *regressione multivariata* costituisce il caso più generale, in cui più variabili esito vengono analizzate simultaneamente in relazione a uno o più predittori. Questo tipo di modellazione richiede calcoli statistici più articolati e rappresenta lo strumento più potente e complesso tra le tre tipologie.\n\nIl percorso di apprendimento ideale inizia dalla regressione bivariata, poiché fornisce le basi concettuali e metodologiche che possono successivamente essere estese verso modelli più sofisticati. La comprensione dei principi fondamentali acquisiti nel caso semplice è infatti un prerequisito essenziale per affrontare le sfide interpretative e computazionali poste dalle regressioni multiple e multivariate.\n\n\n## La predizione dell'intelligenza\n\nPer illustrare il modello di regressione secondo l'approccio frequentista, utilizzeremo un dataset reale: i dati `kidiq` tratti dal *National Longitudinal Survey of Youth* [@gelman2021regression]. Questi dati riguardano un campione di donne americane e i loro figli, con particolare attenzione a due variabili fondamentali per la nostra analisi: il punteggio cognitivo del bambino (`kid_score`) e il quoziente intellettivo della madre (`mom_iq`). L'obiettivo principale consiste nell'esaminare se - e in che misura - l'intelligenza materna possa essere considerata un fattore predittivo delle capacità cognitive del bambino.  \n\n\n### Esplorazione dei dati\n\nImportiamo i dati in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq <- rio::import(here::here(\"data\", \"kidiq.dta\"))\n```\n:::\n\n\nEsaminiamo le prime righe del data frame:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhead(kidiq)\n#>   kid_score mom_hs mom_iq mom_work mom_age\n#> 1        65      1  121.1        4      27\n#> 2        98      1   89.4        4      25\n#> 3        85      1  115.4        4      27\n#> 4        83      1   99.4        3      25\n#> 5       115      1   92.7        4      27\n#> 6        98      0  107.9        1      18\n```\n:::\n\n\nUn diagramma a dispersione per i dati di questo campione suggerisce la presenza di un'associazione positiva tra l'intelligenza del bambino (`kid_score`) e l'intelligenza della madre (`mom_iq`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point(alpha = 0.6) +\n  labs(x = \"QI della madre\", y = \"QI del bambino\") +\n  ggtitle(\"Diagramma a dispersione\") \n```\n\n::: {.cell-output-display}\n![](01_reglin_frequentist_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n##  Il modello teorico\n\nIl modello di regressione lineare bivariata è:\n  \n$$\ny_i = a + b x_i + e_i, \\quad i = 1, \\dots, n\n$$\n\\noindent\ndove:\n  \n* $a$: intercetta (valore atteso di $y$ quando $x = 0$);\n* $b$: pendenza (variazione attesa di $y$ per +1 in $x$);\n* $e_i$: errore residuo (scarto tra osservato e predetto).\n\nNel nostro caso:\n  \n* $y = \\text{`kid\\_score`}$ (QI del bambino)\n* $x = \\text{`mom\\_iq`}$ (QI della madre)\n\nLa *componente deterministica* $\\hat{y}_i = a + b x_i$ rappresenta la parte prevedibile di $y$ in funzione di $x$.\nLa *componente aleatoria* $e_i = y_i - \\hat{y}_i$ cattura ciò che il modello non spiega.\n\n::: {.callout-note title=\"Illustrazione\" collapse=\"true\"}\nIl campione è costituito da $n$ coppie di osservazioni ($x, y$). \n\n$$\n\\begin{array}{cc}\n\\hline\nx_1 & y_1 \\\\\nx_2 & y_2 \\\\\nx_3 & y_3 \\\\\n\\vdots & \\vdots \\\\\nx_n & y_n \\\\\n\\hline\n\\end{array}\n$$\n\nPer ciascuna coppia di valori $x_i, y_i$, il modello di regressione si aspetta che il valore $y_i$ sia associato al corrispondente valore $x_i$ come indicato dalla seguente equazione\n\n$$\n\\mathbb{E}(y_i) = a + b x_i ,\n$$ {#eq-reglin-exp-val}\n\n\\noindent\novvero:\n\n$$\n\\begin{array}{ccc}\n\\hline\nx_i & y_i & \\mathbb{E}(y_i) = a + b x_i \\\\\n\\hline\nx_1 & y_1 & a + b x_1 \\\\\nx_2 & y_2 & a + b x_2 \\\\\nx_3 & y_3 & a + b x_3 \\\\\n\\vdots & \\vdots & \\vdots \\\\\nx_n & y_n & a + b x_n \\\\\n\\hline\n\\end{array}\n$$\n\nI valori $y_i$ corrispondono, nell'esempio che stiamo discutendo, alla variabile `kid_score`. I primi 10 valori della variabile $y$ sono i seguenti:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq$kid_score[0:10]\n#>  [1]  65  98  85  83 115  98  69 106 102  95\n```\n:::\n\n\nPer fare riferimento a ciascuna osservazione usiamo l'indice $i$. Quindi, ad esempio, $y_2$ è uguale a\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq$kid_score[2]\n#> [1] 98\n```\n:::\n\n:::\n\n\n### Stima del modello di regressione\n\nCalcoliamo i coefficienti della retta di regressione utilizzando la funzione `lm`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Modello di regressione lineare\nmod <- lm(kid_score ~ mom_iq, data = kidiq)\n```\n:::\n\n\nEsaminiamo i risultati:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Coefficienti stimati\ncoef(mod)\n#> (Intercept)      mom_iq \n#>       25.80        0.61\n```\n:::\n\n\nIn generale, molte rette possono approssimare la nube di punti, ma il modello di regressione impone vincoli: \n\n* la retta deve passare per il punto medio $(\\bar{x}, \\bar{y})$;\n* deve minimizzare la somma dei quadrati dei residui (SSE).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcola i valori medi\nmean_x <- mean(kidiq$mom_iq, na.rm = TRUE)\nmean_y <- mean(kidiq$kid_score, na.rm = TRUE)\n\n# Grafico nello stile \"manuscript\"\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point(alpha = 0.65, color = css_palette$text_primary) +\n  geom_smooth(\n    method = \"lm\", se = FALSE,\n    color = css_palette$accent_warm,\n    linewidth = 0.8\n  ) +\n  annotate(\n    \"point\", \n    x = mean_x, y = mean_y,\n    color = css_palette$illumination_blue,\n    size = 4.5, shape = 4, stroke = 2.2\n  ) +\n  labs(\n    x = \"QI della madre\",\n    y = \"QI del bambino\",\n    title = stringr::str_wrap(\"QI materno e QI del bambino\", width = 50),\n    subtitle = \"Con retta di regressione e punto medio\"\n  ) +\n  theme_manuscript()\n```\n\n::: {.cell-output-display}\n![](01_reglin_frequentist_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n\n### Interpretazione\n\nIl coefficiente $a$ indica l'intercetta della retta di regressione nel diagramma a dispersione. Questo valore rappresenta il punto in cui la retta di regressione interseca l'asse $y$ del sistema di assi cartesiani. Tuttavia, in questo caso specifico, il valore di $a$ non è di particolare interesse poiché corrisponde al valore della retta di regressione quando l'intelligenza della madre è pari a 0, il che non ha senso nella situazione reale. Successivamente, vedremo come è possibile trasformare i dati per fornire un'interpretazione utile del coefficiente $a$.\n\nInvece, il coefficiente $b$ indica la pendenza della retta di regressione, ovvero di quanto aumenta (se $b$ è positivo) o diminuisce (se $b$ è negativo) la retta di regressione in corrispondenza di un aumento di 1 punto della variabile $x$. Nel caso specifico del QI delle madri e dei loro figli, il coefficiente $b$ ci indica che un aumento di 1 punto del QI delle madri è associato, in media, a un aumento di 0.61 punti del QI dei loro figli.\n\nIn pratica, il modello di regressione lineare cerca di prevedere le medie dei punteggi del QI dei figli in base al QI delle madri. Ciò significa che non è in grado di prevedere esattamente il punteggio di ciascun bambino in funzione del QI della madre, ma solo una stima della media dei punteggi dei figli quando il QI delle madri aumenta o diminuisce di un punto.\n\nIl coefficiente $b$ ci dice di quanto aumenta (o diminuisce) in media il QI dei figli per ogni unità di aumento (o diminuzione) del QI della madre. Nel nostro caso, se il QI della madre aumenta di un punto, il QI dei figli aumenta in media di 0.61 punti.\n\nÈ importante comprendere che il modello statistico di regressione lineare non è in grado di prevedere il valore preciso di ogni singolo bambino, ma solo una stima della media dei punteggi del QI dei figli quando il QI delle madri aumenta o diminuisce. Questa stima è basata su una distribuzione di valori possibili che si chiama distribuzione condizionata $p(y \\mid x_i)$.\n\nUna rappresentazione grafica del valore predetto dal modello di regressione, $\\hat{y}_i = a + bx_i$ è stato fornito in precedenza. Il diagramma presenta ciascun valore $\\hat{y}_i = a + b x_i$ in funzione di $x_i$. I valori predetti dal modello di regressione sono i punti che stanno sulla retta di regressione.\n\n\n###  Centrare le variabili\n\nIn generale, per variabili a livello di scala ad intervalli, l'intercetta del modello di regressione lineare non ha un'interpretazione utile. Questo perché l'intercetta indica il valore atteso di $y$ quando $x = 0$, ma in caso di variabili a scala di intervalli, il valore \"0\" di $x$ è arbitrario e non corrisponde ad un \"assenza\" della variabile $x$. Ad esempio, un QI della madre pari a 0 non indica un'assenza di intelligenza, ma solo un valore arbitrario del test usato per misurare il QI. Quindi, sapere il valore medio del QI dei bambini quando il QI della madre è 0 non è di alcun interesse.\n  \nCentrando $x$ attorno alla sua media otteniamo un’intercetta interpretabile: il valore medio di `kid_score` quando `mom_iq` è nella media del campione.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq <- kidiq %>%\n  mutate(xd = mom_iq - mean(mom_iq))\n```\n:::\n\n\nSe ora usiamo le coppie di osservazioni $(xd_i, y_i)$, il diagramma a dispersione assume la forma seguente.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](01_reglin_frequentist_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nIn sostanza, abbiamo applicato una trasformazione ai dati, traslando tutti i punti del grafico lungo l'asse delle ascisse in modo che la media dei valori $x$ risulti pari a zero. Questa operazione non ha alterato la distribuzione o la forma complessiva della nuvola di punti, ma ha semplicemente modificato l'origine del sistema di riferimento sull'asse $x$.\n\nLa pendenza della retta di regressione che mette in relazione $x$ e $y$ rimane invariata, sia per i dati originali che per quelli trasformati. L'unico parametro che subisce una modifica è il valore dell'intercetta della retta di regressione, che acquisisce in questo modo un'interpretazione più intuitiva e significativa dal punto di vista sostanziale.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod1 <- lm(kid_score ~ xd, data = kidiq)\ncoef(mod1)\n#> (Intercept)          xd \n#>       86.80        0.61\n```\n:::\n\n\nNell'output del modello, l'intercetta rappresenta il valore predetto della variabile dipendente $y$ quando la variabile indipendente $x$ assume esattamente il suo valore medio campionario. Nel caso specifico dell'esempio, l'intercetta corrisponde al punteggio atteso del QI del bambino (`kid_score`) quando il quoziente intellettivo della madre (`mom_iq`) è pari alla media osservata nel campione di riferimento.\n\n\n## Metodo dei minimi quadrati\n  \nNel modello di regressione bivariata, i coefficienti $a$ (intercetta) e $b$ (pendenza) vengono stimati scegliendo la retta che *minimizza la somma dei quadrati dei residui* (*Sum of Squared Errors*, SSE):\n  \n$$\ne_i = y_i - (a + b x_i), \\quad SSE = \\sum_{i=1}^n e_i^2.\n$$\nQuesta scelta equivale, sotto l’assunzione di errori normali con media zero e varianza costante, alla stima di *massima verosimiglianza*.\n\nLa minimizzazione dell’SSE porta alle *equazioni normali*, la cui soluzione ha forma chiusa:\n  \n$$\nb = \\frac{\\mathrm{Cov}(x, y)}{\\mathrm{Var}(x)}, \\quad a = \\bar{y} - b \\,\\bar{x} ,\n$$\ndove:\n  \n* $\\bar{x}$ e $\\bar{y}$ sono le medie campionarie;\n* $\\mathrm{Cov}(x,y)$ è la covarianza tra $x$ e $y$;\n* $\\mathrm{Var}(x)$ è la varianza di $x$.\n\nQueste formule assicurano che:\n  \n1. La retta passa per il punto medio $(\\bar{x}, \\bar{y})$ della nube di punti.\n2. La pendenza $b$ quantifica la variazione media di $y$ per un’unità di incremento di $x$.\n3. L’intercetta $a$ è il valore previsto di $y$ quando $x=0$ (interpretazione utile solo se $x=0$ è significativo).\n\n<span class=\"sottopasso\">Esempio in R<span>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncov_xy <- cov(kidiq$kid_score, kidiq$xd)\nvar_x  <- var(kidiq$xd)\nb <- cov_xy / var_x\na <- mean(kidiq$kid_score) - b * mean(kidiq$xd)\nc(intercetta = a, pendenza = b)\n#> intercetta   pendenza \n#>      86.80       0.61\n```\n:::\n\n\nI risultati replicano quelli ottenuti in precedenza con `lm()`.\n\n\n### Residui \n\nIl residuo, ovvero la componente di ciascuna osservazione $y_i$ che non viene predetta dal modello di regressione, corrisponde alla *distanza verticale* tra il valore $y_i$ osservato e il valore $\\hat{y}_i$ predetto dal modello di regressione:\n\n$$\ne_i = y_i - (a + b x_i).\n$$\n\nPer fare un esempio numerico, consideriamo il punteggio osservato del QI del primo bambino.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq$kid_score[1]\n#> [1] 65\n```\n:::\n\n\nIl QI della madre è\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq$mom_iq[1]\n#> [1] 121\n```\n:::\n\n\nPer questo bambino, il valore predetto dal modello di regressione è\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\na + b * kidiq$mom_iq[1]\n#> [1] 161\n```\n:::\n\n\nL'errore che compiamo per predire il QI del bambino utilizzando il modello di regressione (ovvero, il residuo) è\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq$kid_score[1] - (a + b * kidiq$mom_iq[1])\n#> [1] -95.7\n```\n:::\n\n\nPer tutte le osservazioni abbiamo\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nres <- kidiq$kid_score - (a + b * kidiq$mom_iq)\n```\n:::\n\n\nÈ una proprietà del modello di regressione (calcolato con il metodo dei minimi quadrati) che la somma dei residui sia uguale a zero.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsum(res)\n#> [1] -26473\n```\n:::\n\n\nQuesto significa che ogni valore osservato $y_i$ viene scomposto dal modello di regressione in due componenti distinte. La componente deterministica $\\hat{y}_i$, che è predicibile da $x_i$, è data da $\\hat{y}_i = a + b x_i$. Il residuo, invece, è dato da $e_i = y_i - \\hat{y}_i$. La somma di queste due componenti, ovviamente, riproduce il valore osservato.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Creazione di un data frame con i valori calcolati\ndf <- data.frame(\n  kid_score = kidiq$kid_score,\n  mom_iq = kidiq$mom_iq,\n  y_hat = a + b * kidiq$mom_iq,\n  e = kidiq$kid_score - (a + b * kidiq$mom_iq),\n  y_hat_plus_e = (a + b * kidiq$mom_iq) + (kidiq$kid_score - (a + b * kidiq$mom_iq))\n)\n\n# Visualizzazione dei primi 6 valori\nhead(df)\n#>   kid_score mom_iq y_hat     e y_hat_plus_e\n#> 1        65  121.1   161 -95.7           65\n#> 2        98   89.4   141 -43.3           98\n#> 3        85  115.4   157 -72.2           85\n#> 4        83   99.4   147 -64.5           83\n#> 5       115   92.7   143 -28.4          115\n#> 6        98  107.9   153 -54.6           98\n```\n:::\n\n\n\n### Illustrazione del metodo dei minimi quadrati\n\nPer stimare i coefficienti $a$ e $b$, possiamo minimizzare la somma dei quadrati dei residui tra i valori osservati $y_i$ e quelli previsti $a + b x_i$.\n\nIniziamo con il creare una griglia per i valori di $b$. Supponiamo che il valore di $a$ sia noto ($a = 25.79978$). Usiamo R per creare una griglia di valori possibili per $b$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Griglia di valori per b\nb_grid <- seq(0, 1, length.out = 1001)\na <- 25.79978  # Intercetta nota\n```\n:::\n\n\nDefiniamo ora una funzione che calcola la somma dei quadrati dei residui ($SSE$) per ciascun valore di $b$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Funzione per la somma dei quadrati dei residui\nsse <- function(a, b, x, y) {\n  sum((y - (a + b * x))^2)\n}\n```\n:::\n\n\nApplichiamo la funzione `sse` alla griglia di valori $b$ per calcolare la somma dei quadrati dei residui per ogni valore di $b$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo di SSE per ciascun valore di b\nsse_vals <- sapply(\n  b_grid, \n  function(b) sse(a, b, kidiq$mom_iq, kidiq$kid_score)\n)\n```\n:::\n\n\n- `sapply`:\n  - È una funzione di R che applica una funzione ad ogni elemento di un vettore (o lista) e restituisce i risultati in un vettore.\n  - Qui, applica la funzione `sse` a ciascun valore di $b$ contenuto in `b_grid`.\n\n- `function(b)`:\n  - È una funzione anonima definita al volo per specificare come calcolare $SSE$ per ciascun valore di $b$.\n  - All'interno, viene chiamata la funzione `sse(a, b, x, y)` con i seguenti parametri:\n    - `a`: il valore dell'intercetta (fissato in precedenza o noto).\n    - `b`: il valore corrente nella griglia `b_grid`.\n    - `x`: la variabile indipendente del dataset (`kidiq$mom_iq`).\n    - `y`: la variabile dipendente del dataset (`kidiq$kid_score`).\n\n- Il risultato è un vettore, `sse_vals`, che contiene i valori di $SSE$ corrispondenti a ciascun valore di $b$ in `b_grid`.\n\nTracciamo un grafico che mostra la somma dei quadrati dei residui ($SSE$) in funzione dei valori di $b$, evidenziando il minimo.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Identificazione del valore di b che minimizza SSE\nb_min <- b_grid[which.min(sse_vals)]\n\n# Creazione del dataframe per ggplot\ndat <- data.frame(b_grid = b_grid, sse_vals = sse_vals)\n\n# Genera il grafico\nggplot(dat, aes(x = b_grid, y = sse_vals)) +\n  geom_line(linewidth = 1) +  \n  annotate(\n    \"point\", x = b_min, y = min(sse_vals),\n    color = \"red\", size = 3\n  ) +  # Punto minimo\n  labs(\n    x = expression(paste(\"Possibili valori di \", hat(beta))),\n    y = \"Somma dei quadrati\\ndei residui\",\n    title = \"Residui quadratici\"\n  ) +\n  annotate(\n    \"text\", x = b_min, y = min(sse_vals), \n    label = expression(hat(beta)), color = \"red\", vjust = -1, hjust = 0.5\n  )\n```\n\n::: {.cell-output-display}\n![](01_reglin_frequentist_files/figure-html/unnamed-chunk-24-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nInfine, identifichiamo il valore di $b$ che minimizza la somma dei quadrati dei residui.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb_min\n#> [1] 0.61\n```\n:::\n\n\nCon questa simulazione, abbiamo stimato il coefficiente $b$ minimizzando la somma dei quadrati dei residui. \n\nQuesto approccio può essere esteso per stimare simultaneamente entrambi i coefficienti ($a$ e $b$) utilizzando metodi di ottimizzazione più avanzati, come `optim` in R.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\noptim_result <- optim(\n  par = c(a = 25, b = 0.5),  # Valori iniziali\n  fn = function(params) {\n    a <- params[1]\n    b <- params[2]\n    sse(a, b, kidiq$mom_iq, kidiq$kid_score)\n  }\n)\n\n# Coefficienti stimati\noptim_result$par\n#>     a     b \n#> 25.79  0.61\n```\n:::\n\n\nQuesta simulazione illustra come, tramite il metodo dei minimi quadrati, sia possibile stimare i parametri di un modello bivariato di regressione.\n\n\n## L'errore standard della regressione\n\nL’*errore standard della stima* $s_e$ misura la deviazione media dei dati dalla retta:\n\n$$\n\\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n \\big(y_i - (\\hat{a} + \\hat{b}x_i)\\big)^2},\n$$ {#eq-reglin-err-standard-regr}\n\nL'indice $s_e$ possiede la stessa unità di misura di $y$ ed è una stima della deviazione standard dei residui nella popolazione.\n\nIn R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo dei residui\ne <- kidiq$kid_score - (a + b * kidiq$mom_iq)\n\n# Mostriamo i primi 10 residui\nhead(e, 10)\n#>  [1] -34.68  17.69 -11.22  -3.46  32.63   6.38 -41.52   3.86  26.41  11.21\n```\n:::\n\n\nCalcoliamo il valore medio assoluto dei residui per avere un'indicazione della deviazione media rispetto alla retta di regressione.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Media assoluta dei residui\nmean(abs(e))\n#> [1] 14.5\n```\n:::\n\n\nL'errore standard della stima $s_e$ si calcola come la radice quadrata della somma dei quadrati dei residui divisa per $n-2$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Calcolo di s_e\nse <- sqrt(sum(e^2) / (length(e) - 2))\nse\n#> [1] 18.3\n```\n:::\n\n\nNotiamo che il valore medio assoluto dei residui e l'errore standard $s_e$ non sono identici, ma hanno lo stesso ordine di grandezza. $s_e$ è una misura più rigorosa della deviazione standard dei residui. \n\nQuesta analisi dimostra come $s_e$ consenta di valutare quanto le previsioni del modello si discostino (in media) dai dati osservati.\n\n\n## L'indice di determinazione\n\nNell'approccio frequentista, la qualità dell’adattamento si apprezza osservando l’indice di determinazione $R^2$, che indica quanta parte della varianza di $y$ viene spiegata dal modello, e analizzando i residui: eventuali pattern sistematici nei residui possono segnalare che la struttura scelta non coglie tutte le caratteristiche dei dati o che esistono violazioni delle ipotesi (linearità, omoscedasticità, normalità degli errori). Un esame congiunto di $R^2$ e residui aiuta a diagnosticare e, se necessario, migliorare il modello.\n\nL'indice di determinazione viene calcolato utilizzando un'importante proprietà del modello di regressione, ovvero la scomposizione della varianza della variabile dipendente $y$ in due componenti: la varianza spiegata dal modello e la varianza residua.  \n\nPer una generica osservazione $x_i, y_i$, la deviazione di $y_i$ rispetto alla media $\\bar{y}$ può essere espressa come la somma di due componenti: il residuo $e_i=y_i- \\hat{y}_i$ e lo scarto di $\\hat{y}_i$ rispetto alla media $\\bar{y}$:\n\n$$\ny_i - \\bar{y} = (y_i- \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) = e_i + (\\hat{y}_i - \\bar{y}).\n$$ \n\nLa varianza totale di $y$ può quindi essere scritta come:\n\n$$\n\\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(e_i + (\\hat{y}_i - \\bar{y}))^2.\n$$\n\nSviluppando il quadrato e sommando, si ottiene:\n\n$$\n\\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2.\n$$ {#eq-reglin-dev}\n\nIl primo termine rappresenta la *varianza residua*, mentre il secondo termine rappresenta la *varianza spiegata* dal modello. Questa scomposizione della devianza va sotto il nome di *teorema della scomposizione della devianza*.\n\nQuesta scomposizione viene utilizzata per calcolare l'indice di determinazione $R^2$, che fornisce una misura della bontà di adattamento del modello ai dati del campione. L'indice di determinazione $R^2$ è definito come il rapporto tra la varianza spiegata e la varianza totale:\n\n$$\nR^2 = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}.\n$$ {#eq-reglin-r2}\n\nQuesto indice varia tra 0 e 1 e indica la frazione di varianza totale di $y$ spiegata dal modello di regressione lineare. Un valore alto di $R^2$ indica che il modello di regressione lineare si adatta bene ai dati, in quanto una grande parte della varianza di $y$ è spiegata dalla variabile indipendente $x$.\n\nPer l'esempio in discussione, possiamo calcolare la devianza totale, la devianza spiegata e l'indice di determinazione $R^2$ come segue:\n\nLa devianza totale misura la variabilità complessiva dei punteggi osservati $y$ rispetto alla loro media:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Devianza totale\ndev_t <- sum((kidiq$kid_score - mean(kidiq$kid_score))^2)\ndev_t\n#> [1] 180386\n```\n:::\n\n\nLa devianza spiegata misura la variabilità che il modello è in grado di spiegare, considerando i valori predetti $a + b x$:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Devianza spiegata\ndev_r <- sum(((a + b * kidiq$mom_iq) - mean(kidiq$kid_score))^2)\ndev_r\n#> [1] 36249\n```\n:::\n\n\nL'indice $R^2$ è il rapporto tra la devianza spiegata e la devianza totale, e indica la frazione della variabilità totale che è spiegata dal modello di regressione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Indice di determinazione\nR2 <- dev_r / dev_t\nround(R2, 3)\n#> [1] 0.201\n```\n:::\n\n\nPer verificare i calcoli, utilizziamo il modello di regressione lineare in R e leggiamo $R^2$ direttamente dal sommario del modello:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Modello di regressione lineare\nmod <- lm(kid_score ~ mom_iq, data = kidiq)\n\n# Sommario del modello per leggere R^2\nsummary(mod)$r.squared\n#> [1] 0.201\n```\n:::\n\n\nIl risultato mostra che circa il 20% della variabilità nei punteggi del QI dei bambini è spiegabile conoscendo il QI delle madri. Questo significa che il modello cattura una porzione rilevante della relazione, ma lascia anche spazio a fattori non inclusi nel modello che influenzano il QI dei bambini.\n\n\n## Inferenza sui coefficienti\n\nL'inferenza statistica sui coefficienti di regressione richiede la definizione della *distribuzione campionaria* dei coefficienti di regressione. Il modello di regressione bivariata (o lineare semplice) è:\n\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i,$$\ndove $Y_i$ è la variabile dipendente per l'osservazione $i$, $X_i$ è la variabile indipendente, $\\beta_0$ è l'intercetta (parametro ignoto), $\\beta_1$ è il coefficiente angolare (il parametro che ci interessa stimare), e $\\varepsilon_i$ è il termine di errore per l'osservazione $i$.\n\nNell'inferenza, il nostro obiettivo è stimare i parametri ignoti $\\beta_0$ e $\\beta_1$ usando i dati campionari disponibili. Il metodo più comune è quello dei Minimi Quadrati Ordinari (OLS), che ci fornisce gli stimatori $\\hat{\\beta}_0$ e $\\hat{\\beta}_1$ (spesso indicati semplicemente con $b_0$ e $b_1$). Lo stimatore per il coefficiente angolare è dato dalla formula:\n\n$$\\hat{\\beta}_1 = b_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}.$$\n\n\n### Cos'è la distribuzione campionaria di $b_1$?\n\nLo stimatore $b_1$ è una *variabile casuale*. Perché? Perché il suo valore dipende dal campione casuale di dati $(X_i, Y_i)$ che abbiamo estratto dalla popolazione.\n\nImmaginiamo di poter ripetere il processo di campionamento e stima del modello infinite volte:\n\n1.  Estraiamo un campione casuale di dimensione $n$.\n2.  Calcoliamo lo stimatore $b_1$ per quel campione.\n3.  Registriamo il valore di $b_1$.\n4.  Estraiamo un nuovo campione casuale (indipendentemente dal primo).\n5.  Calcoliamo il \"nuovo\" $b_1$.\n6.  Registriamo il valore.\n7.  ... e così via, per infinite volte.\n\nLa *distribuzione campionaria di $b_1$* è la distribuzione di probabilità di tutti i valori di $b_1$ che otterremmo da questi infiniti campioni casuali di dimensione $n$ estratti dalla stessa popolazione.\n\n\n### Assunzioni di Gauss–Markov e distribuzione campionaria della pendenza\n\nPer il modello di regressione bivariata\n\n$$\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i,\n$$\nlo stimatore OLS della pendenza è\n\n$$\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}.\n$$\nQuesto è una *variabile casuale*, perché il suo valore dipende dal campione estratto.\n\n\n#### 1. Assunzioni di Gauss–Markov\n\nAffinché $\\hat{\\beta}_1$ sia *non distorto* (*unbiased*) e *BLUE* (Best Linear Unbiased Estimator), devono valere:\n\n1. **Linearità nei parametri**\n   La relazione media tra $Y$ e $X$ è lineare:\n   $E(Y_i \\mid X_i) = \\beta_0 + \\beta_1 X_i$.\n\n2. **Campionamento casuale e indipendenza**\n   Le osservazioni $(X_i,Y_i)$ sono indipendenti e identicamente distribuite.\n\n3. **Esogeneità**\n   Gli errori hanno media nulla condizionata a $X$:\n   $E(\\varepsilon_i \\mid X_i) = 0$.\n   **Se violata, lo stimatore è distorto.**\n\n4. **Omoschedasticità**\n   Varianza costante degli errori:\n   $\\mathrm{Var}(\\varepsilon_i \\mid X_i) = \\sigma^2$.\n\n5. **Assenza di collinearità perfetta**\n   La variabilità di $X$ è positiva:\n   $\\sum_{i=1}^n (X_i - \\bar{X})^2 > 0$.\n\nPer inferenza esatta in piccoli campioni si aggiunge l’assunzione di *normalità*: $\\varepsilon_i \\sim N(0,\\sigma^2)$.\n\n\n#### 2. Proprietà della distribuzione campionaria di $\\hat{\\beta}_1$\n\nSotto le assunzioni di Gauss–Markov:\n\n* **Media (non distorsione)**\n  $E(\\hat{\\beta}_1) = \\beta_1$\n  → in media, il processo di stima restituisce il vero coefficiente.\n\n* **Varianza**\n\n  $$\n  \\mathrm{Var}(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n  $$\n\n  dove $\\sigma^2$ è la varianza degli errori.\n  In pratica si usa la stima:\n\n  $$\n  s^2_e = \\frac{\\sum e_i^2}{n-2}, \\quad\n  SE(\\hat{\\beta}_1) = \\sqrt{\\frac{s_e^2}{\\sum (X_i - \\bar{X})^2}}\n  $$\n\n* **Forma della distribuzione**\n\n  * Con normalità degli errori → distribuzione esatta normale per ogni $n$.\n  * Senza normalità, per grandi $n$ la distribuzione è approssimativamente normale (Teorema del Limite Centrale).\n\n\n#### 3. Uso della distribuzione campionaria\n\nConoscere la distribuzione campionaria di $\\hat{\\beta}_1$ serve per:\n\n1. **Test d’ipotesi**\n   Es.: $H_0: \\beta_1 = 0$, usando la statistica $t = \\hat{\\beta}_1 / SE(\\hat{\\beta}_1).$\n\n2. **Intervalli di confidenza**\n   Es.: $\\hat{\\beta}_1 \\pm t_{n-2,\\,0.975} \\times SE(\\hat{\\beta}_1)$,\n   interpretati in termini di proprietà a lungo termine della procedura di campionamento.\n\nIn sintesi, la distribuzione campionaria di $b_1$ descrive la variabilità attesa dello stimatore del coefficiente angolare attraverso diversi campioni casuali. Comprendere le sue proprietà (media, varianza, forma) è essenziale per interpretare correttamente i risultati di una regressione e trarre conclusioni affidabili sulla relazione nella popolazione.[^1]\n\n[^1]: Per un approfondimento sull'approccio frequentista alla regressione, si veda, per esempio, @caudek2001statistica.\n\n\n### Simulazione in R\n\nCalcoliamo la distribuzione campionaria di $\\hat{\\beta}_1$. Iniziamo simulando un modello lineare bivariato con variabili standardizzate:\n\n$$\nY_i = \\beta X_i + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma_\\varepsilon = 0.5),\n$$\ncon $\\beta = 1.5$ e $n = 30$ osservazioni.\n\nRipetiamo il campionamento $100{,}000$ volte per approssimare la distribuzione campionaria di $\\hat{\\beta}_1$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Parametri\nbeta     <- 1.5     # pendenza vera\nsigma_e  <- 0.5     # deviazione standard errori\nn        <- 30      # dimensione campione\nnrep     <- 1e5     # numero repliche\n\n# X fissata una volta (come nelle assunzioni di Gauss-Markov)\nx <- rnorm(n, mean = 0, sd = 1)\n\n# Stime memorizzate\nb_hat <- numeric(nrep)\n\n# Simulazione\nfor (i in 1:nrep) {\n  e <- rnorm(n, mean = 0, sd = sigma_e) # errori\n  y <- beta * x + e                     # risposta\n  b_hat[i] <- cov(x, y) / var(x)        # formula OLS\n}\n```\n:::\n\n\nEsaminiamo i risultati:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Statistiche empiriche\nmean_b_hat <- mean(b_hat)  # media stimata\nsd_b_hat   <- sd(b_hat)    # deviazione standard stimata\n\n# Errore standard teorico\nse_theo <- sqrt(sigma_e^2 / sum((x - mean(x))^2))\n\nc(media_empirica = mean_b_hat,\n  sd_empirica    = sd_b_hat,\n  SE_teorico     = se_theo)\n#> media_empirica    sd_empirica     SE_teorico \n#>          1.500          0.101          0.101\n```\n:::\n\n\nGeneriamo un grafico della distribuzione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data.frame(b_hat = b_hat), aes(x = b_hat)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 bins = 50) +\n  stat_function(fun = dnorm,\n                args = list(mean = mean_b_hat, sd = sd_b_hat),\n                linewidth = 1) +\n  labs(\n    title = expression(\"Distribuzione campionaria di\" ~ hat(beta)[1]),\n    x = expression(hat(beta)[1]),\n    y = \"Densità\"\n  )\n```\n\n::: {.cell-output-display}\n![](01_reglin_frequentist_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nL'analisi della distribuzione campionaria dello stimatore $\\hat{\\beta}_1$ conferma tre proprietà statistiche fondamentali. In primo luogo, emerge la *non distorsione* dello stimatore: la media empirica calcolata si attesta approssimativamente a 1.5, valore che coincide con il parametro teorico $\\beta_1$. Questo risultato fornisce una verifica empirica della proprietà per cui, sotto le assunzioni di Gauss-Markov, vale l'uguaglianza $E(\\hat{\\beta}_1) = \\beta_1$, indicando che lo stimatore è corretto e non sistematicamente distorto.\n\nPer quanto riguarda la *precisione* dello stimatore, si osserva che la deviazione standard empirica risulta molto vicina all'errore standard teorico, definito come $SE(\\hat{\\beta}_1) = \\sqrt{\\sigma^2 / \\sum (X_i - \\bar{X})^2}$. Questa corrispondenza suggerisce che la variabilità campionaria osservata è in linea con quanto previsto dal framework teorico, supportando l'affidabilità delle inferenze basate su questo stimatore.\n\nInfine, l'esame della *forma della distribuzione* rivela che l'istogramma dei valori stimati è ben approssimato da una curva normale. Questo risultato conferma la normalità asintotica dello stimatore anche per un campione di dimensione moderata ($n = 30$), avvallando l'utilizzo della distribuzione normale per la costruzione di intervalli di confidenza e test di ipotesi in questo contesto.\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nIn questo capitolo abbiamo introdotto la regressione lineare bivariata nell’ottica frequentista, mostrando come il metodo dei minimi quadrati consenta di stimare i parametri della retta di regressione e di valutarne l’affidabilità attraverso strumenti inferenziali. Abbiamo chiarito il significato di intercetta, pendenza e varianza residua, discutendo anche le condizioni che rendono il modello un buon riassunto dei dati.\n\nQuesta impostazione, pur molto diffusa, presenta però limiti evidenti. L’inferenza si appoggia quasi esclusivamente ai p-value e agli intervalli di confidenza, non permette di incorporare conoscenza pregressa e richiede assunzioni rigide sulla distribuzione degli errori. Il risultato è un approccio potente per descrivere associazioni, ma poco flessibile e spesso riduttivo rispetto alle domande scientifiche più sostanziali.\n\nPer superare questi vincoli, nei prossimi capitoli esploreremo la regressione lineare da una prospettiva *bayesiana*. Qui vedremo come sia possibile integrare informazioni a priori, rappresentare l’incertezza in termini probabilistici e ottenere stime più trasparenti e interpretabili. Sarà il primo passo per trasformare la regressione da semplice strumento descrittivo a un modello inserito in un quadro inferenziale più ampio, capace di adattarsi meglio alla complessità della ricerca psicologica.\n\n::: {.callout-important title=\"Problemi\" collapse=\"true\"}\n\n**1 – Definizione e scopi della regressione**    \nSecondo *Gelman et al.* (2021), quali sono i quattro principali utilizzi della regressione? Fornisci una breve descrizione di ciascuno.\n\n**2 – Errore di specificazione**  \nCos’è l’“errore di specificazione” in un modello di regressione? Quali effetti ha sulle stime dei parametri?\n\n**3 – Stima del modello con `lm()`**  \nUsando il data‑set `kidiq` (variabili `kid_score` e `mom_iq`):  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# carica i dati\nkidiq <- rio::import(here::here(\"data\", \"kidiq.dta\"))\n```\n:::\n\nAdatta il modello `kid_score ~ mom_iq` e riporta intercetta e pendenza.\n\n**4 – Interpretazione della pendenza**  \nInterpreta in parole la pendenza stimata al punto 3 nel contesto dei QI di madri e figli.\n\n**5 – Indice R²**  \nCalcola l’R² del modello di cui al punto 3. Cosa indica il suo valore?\n\n**6 – Centratura del predittore**  \nCrea la variabile centrata `mom_iq_c = mom_iq - mean(mom_iq)` e ri‑adatta il modello `kid_score ~ mom_iq_c`. Qual è la nuova intercetta e perché adesso è più interpretabile?\n\n**7 – Calcolo manuale di $b$**  \nCalcola manualmente la pendenza con la formula  \n$b = \\frac{\\text{Cov}(x,y)}{\\text{Var}(x)}$  \ne confrontala col risultato di `lm()`.\n\n**8 – Confronto tra σ̂ (tradizionale) e σ_CV**  \n* (a) Calcola l’errore standard della regressione (σ̂) usando il modello completo.  \n* (b) Esegui una validazione incrociata leave‑one‑out (LOOCV) e ottieni σ_CV.  \n* (c) Spiega perché, in genere, σ_CV è ≥ σ̂.\n\n**9 – Assunzioni di Gauss‑Markov**  \nElenca le cinque assunzioni di Gauss‑Markov per la regressione lineare semplice e indica quale, se violata, rende distorto lo stimatore OLS della pendenza.\n\n**10 – Simulazione della distribuzione campionaria di $b$**  \nSimula 100 000 campioni (n = 30) dal modello  \n$Y_i = 1.5\\,X_i + \\varepsilon_i,\\; X_i \\sim \\mathcal N(0,1),\\; \\varepsilon_i \\sim \\mathcal N(0,0.5^2)$.  \nPer ogni campione calcola $b̂$. Rappresenta l’istogramma dei 100 000 $b̂$, riporta media e deviazione standard empiriche e confrontale con l’errore standard teorico.\n:::\n\n::: {.callout-tip title=\"Soluzioni\" collapse=\"true\"}\n\n**1 – Definizione e scopi**    \n1. **Previsione** – modellare / predire nuove osservazioni.  \n2. **Esplorazione delle associazioni** – quantificare relazioni $X \\rightarrow Y$.  \n3. **Estrapolazione** – generalizzare dal campione alla popolazione.  \n4. **Inferenza causale** – stimare l’effetto di un intervento quando il design lo consente.\n\n**2 – Errore di specificazione**    \nOmettere un predittore rilevante ⇒ i residui assorbono la sua varianza ⇒ stime di $b$ distorte (bias) e varianze sottostimate → inferenze non valide.\n\n**3 – Stima del modello**  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rio); library(here)\nkidiq <- import(here(\"data\",\"kidiq.dta\"))\n\nmod <- lm(kid_score ~ mom_iq, data = kidiq)\ncoef(mod)\n#> (Intercept)      mom_iq \n#>       25.80        0.61\n```\n:::\n\nEsempio di output  \n```\n(Intercept)     mom_iq \n 25.800        0.610 \n```\n\n**4 – Interpretazione della pendenza**    \nUn punto in più di QI materno è associato, in media, a **0.61 punti** di QI del figlio.\n\n**5 – Indice R²**  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(mod)$r.squared\n#> [1] 0.201\n```\n:::\n\nOutput ≈ 0.20 ⇒ il 20 % della varianza di `kid_score` è spiegato da `mom_iq`.\n\n**6 – Centratura**  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkidiq$mom_iq_c <- kidiq$mom_iq - mean(kidiq$mom_iq)\nmod_c <- lm(kid_score ~ mom_iq_c, data = kidiq)\ncoef(mod_c)\n#> (Intercept)    mom_iq_c \n#>       86.80        0.61\n```\n:::\n\nL’intercetta ora ≈ **media del QI dei bambini** quando il QI materno è medio.  \nLa pendenza resta 0.61.\n\n**7 – Calcolo manuale di `b`**  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nb_manual <- cov(kidiq$mom_iq, kidiq$kid_score) / var(kidiq$mom_iq)\nall.equal(b_manual, coef(mod)[\"mom_iq\"])\n#> [1] \"names for current but not for target\"\n```\n:::\n\n`TRUE` → concordanza perfetta (salvo arrotondamenti).\n\n**8 – σ̂ vs σ_CV**\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# (a) σ̂\nsigma_hat <- summary(mod)$sigma\n\n# (b) LOOCV\nn <- nrow(kidiq)\nres_cv2 <- numeric(n)\nfor (i in seq_len(n)){\n  fit_i <- lm(kid_score ~ mom_iq, data = kidiq[-i,])\n  res_cv2[i] <- (kidiq$kid_score[i] - predict(fit_i, kidiq[i,]))^2\n}\nsigma_CV <- sqrt(mean(res_cv2))\n\nc(sigma_hat = sigma_hat, sigma_CV = sigma_CV)\n#> sigma_hat  sigma_CV \n#>      18.3      18.3\n```\n:::\n\n*σ_CV* tende a superare σ̂ perché ogni predizione è fatta su dati che **non** hanno “visto” quell’osservazione → niente sovradimensionamento.\n\n**9 – Assunzioni Gauss‑Markov**    \n1. Linearità nei parametri  \n2. Campionamento casuale IID  \n3. Esogeneità $E(\\varepsilon_i\\!\\mid X_i)=0$ ← **questa garantisce non‑distorsione**  \n4. Omoschedasticità  \n5. Assenza di collinearità perfetta  \n\n**10 – Simulazione**  \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nbeta  <- 1.5; sigma_e <- 0.5; n  <- 30; nrep <- 1e5\nx <- rnorm(n)\nb_hat <- replicate(nrep, {\n  y <- beta * x + rnorm(n, sd = sigma_e)\n  cov(x,y) / var(x)\n})\n\nmean_emp <- mean(b_hat)\nsd_emp   <- sd(b_hat)\nse_theo  <- sqrt(sigma_e^2 / sum((x - mean(x))^2))\nc(media_empirica = mean_emp, sd_empirica = sd_emp, SE_teorico = se_theo)\n#> media_empirica    sd_empirica     SE_teorico \n#>         1.5001         0.0948         0.0946\n```\n:::\n\nL’istogramma (codice sotto) mostra la forma quasi normale centrata su 1.5.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhist(b_hat, breaks = 60, freq = FALSE, main = \"Distribuzione campionaria di b̂\",\n     xlab = \"b̂\"); curve(dnorm(x, mean_emp, sd_emp), add = TRUE, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](01_reglin_frequentist_files/figure-html/unnamed-chunk-44-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n- La media empirica ≈ 1.5 ⇒ **unbiased**.   \n- La sd empirica ≈ SE teorico ⇒ formula varianza confermata.  \n- Distribuzione simmetrica ≈ Normale ⇒ normalità asintotica verificata.  \n:::\n\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] broom_1.0.9           pillar_1.11.0         tinytable_0.13.0     \n#>  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#>  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [25] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        mgcv_1.9-3           \n#> [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        \n#> [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        \n#> [16] backports_1.5.0       labeling_0.4.3        rmarkdown_2.29       \n#> [19] tzdb_0.5.0            haven_2.5.5           ragg_1.5.0           \n#> [22] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#> [25] jsonlite_2.0.0        parallel_4.5.1        R6_2.6.1             \n#> [28] stringi_1.8.7         RColorBrewer_1.1-3    lubridate_1.9.4      \n#> [31] estimability_1.5.1    knitr_1.50            zoo_1.8-14           \n#> [34] pacman_0.5.1          R.utils_2.13.0        readr_2.1.5          \n#> [37] Matrix_1.7-4          splines_4.5.1         timechange_0.3.0     \n#> [40] tidyselect_1.2.1      abind_1.4-8           yaml_2.3.10          \n#> [43] codetools_0.2-20      curl_7.0.0            pkgbuild_1.4.8       \n#> [46] lattice_0.22-7        withr_3.0.2           bridgesampling_1.1-2 \n#> [49] coda_0.19-4.1         evaluate_1.0.5        survival_3.8-3       \n#> [52] RcppParallel_5.1.11-1 tensorA_0.36.2.1      checkmate_2.3.3      \n#> [55] stats4_4.5.1          distributional_0.5.0  generics_0.1.4       \n#> [58] rprojroot_2.1.1       hms_1.1.3             rstantools_2.5.0     \n#> [61] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [64] emmeans_1.11.2-8      tools_4.5.1           forcats_1.0.0        \n#> [67] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#> [70] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#> [73] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#> [76] V8_7.0.0              gtable_0.3.6          R.methodsS3_1.8.2    \n#> [79] digest_0.6.37         TH.data_1.1-4         htmlwidgets_1.6.4    \n#> [82] farver_2.1.2          memoise_2.0.1         htmltools_0.5.8.1    \n#> [85] R.oo_1.27.1           lifecycle_1.0.4       MASS_7.3-65\n```\n:::\n\n:::\n\n## Bibliografia {.unnumbered .unlisted}\n\n",
    "supporting": [
      "01_reglin_frequentist_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}