{
  "hash": "0bfefd9bc1d7b75186ff4d4980913316",
  "result": {
    "engine": "knitr",
    "markdown": "# Regressione lineare in Stan {#sec-regression-stan}\n\n## Introduzione {.unnumbered .unlisted}\n\nNei capitoli precedenti abbiamo formulato la regressione lineare bivariata sia in chiave frequentista sia in chiave bayesiana, lavorando con modelli semplici e utilizzando notazioni compatte per facilitarne l’implementazione in R. Questi esercizi ci hanno permesso di cogliere la logica dell’approccio bayesiano, ma hanno anche mostrato i limiti delle soluzioni analitiche: sono praticabili solo in casi elementari e non si estendono facilmente a modelli con più predittori, variabili categoriali o strutture gerarchiche.\n\nPer affrontare problemi realistici è necessario un linguaggio che ci permetta di **specificare modelli generali** e di stimarli in modo efficiente con algoritmi di campionamento moderni. Questo linguaggio è **Stan**. Con Stan possiamo tradurre direttamente le nostre ipotesi in codice, lasciando al software il compito di eseguire il campionamento dalla distribuzione a posteriori.\n\nIn questo capitolo vedremo come implementare il modello di regressione lineare in Stan. Partiremo dalla traduzione della formulazione matematica in codice, per poi esaminare come i diversi blocchi del linguaggio (data, parameters, model, generated quantities) contribuiscano a definire e stimare il modello. Lo scopo non è solo imparare a scrivere un programma funzionante, ma soprattutto comprendere il legame stretto tra teoria statistica, specificazione del modello e calcolo computazionale.\n\n### Perché usare Stan per la regressione?\n\nScrivere la regressione in Stan significa specificare un *modello generativo* completo:\n\n$$\ny_n \\sim \\mathcal{N}(\\alpha + \\mathbf{x}_n^\\top \\boldsymbol{\\beta},\\, \\sigma),\n$$\ncon priors espliciti per $\\alpha$, $\\boldsymbol{\\beta}$ e $\\sigma$. Questo approccio offre tre vantaggi immediati:\n\n1. *Trasparenza* — la struttura del modello (likelihood e priors) è dichiarata in modo chiaro.\n2. *Coerenza* — l’inferenza è interamente bayesiana: non ci sono p-value né “soglie di significatività”.\n3. *Flessibilità* — lo stesso schema si estende facilmente a *GLM* (esiti Bernoulli, Poisson, ecc.) e a modelli multilivello, semplicemente cambiando la famiglia di distribuzione o la struttura gerarchica.\n\n### Idee guida del capitolo\n\n* *Notazione matriciale*: la scrittura compatta $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ si traduce in Stan in `matrix[N,K] X;` e `vector[K] beta;`, con previsioni calcolate come `X * beta`.\n* *Vettorializzazione*: scrivere `y ~ normal(X * beta + alpha, sigma);` è più elegante ed efficiente che iterare su ogni osservazione.\n* *Coefficiente parziale vs bivariato*: in presenza di correlazione tra predittori, il coefficiente bivariato può risultare distorto o addirittura invertire segno. I coefficienti del modello multiplo misurano invece l’associazione condizionata.\n* *Priors su scala naturale*: formulare i priors nelle unità originali (per esempio, punteggi da 0 a 10 o da 0 a 100) rende le ipotesi più interpretabili e comunicabili.\n\n### Una prima finestra sugli errori di specificazione\n\nIl capitolo affronterà anche alcuni casi frequenti di *specificazione errata*, con esempi concreti tratti dalla ricerca psicologica:\n\n1. *Variabili omesse*: trascurare un predittore rilevante e correlato introduce *bias* nei coefficienti.\n2. *Forma funzionale errata*: trattare relazioni non lineari come lineari semplici produce stime ingannevoli.\n3. *Varianza non costante e outlier*: assumere residui gaussiani omoschedastici può essere inadeguato; i *posterior predictive checks* aiutano a diagnosticarlo e a considerare likelihood più robuste.\n\n\nIn sintesi, la regressione lineare in Stan non è solo un esercizio di calcolo, ma un’occasione per apprendere un metodo: partire da un modello teorico, tradurlo in linguaggio probabilistico, implementarlo in codice, e valutarne la capacità di rappresentare i dati psicologici in modo critico e trasparente.\n\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Il modello di regressione multipla in *notazione matriciale*.  \n- Formulazione del modello in codice Stan.  \n- Stima e interpretazione dei *coefficienti parziali* e confronto con quelli di modelli bivariati.  \n- Effetto della *correlazione tra predittori* sulle stime e *errore di specificazione*.  \n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\nPer seguire al meglio questo capitolo è utile avere:\n\n- una conoscenza di base della *regressione lineare semplice* e del concetto di coefficiente di regressione;  \n- un’idea generale della *notazione vettoriale/matriciale*, anche solo a livello concettuale;\n- nozioni introduttive di *statistica bayesiana*: cosa sono i *prior*, la *likelihood* e il *posterior*;\n- familiarità minima con R per la simulazione di dati e con il flusso di lavoro di Stan.\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(brms, posterior, cmdstanr, tidybayes, loo, patchwork)\n```\n:::\n\n:::\n\n\n## Regressione lineare con un solo predittore \n\nConsideriamo una regressione lineare con *un solo predittore*, un’intercetta $\\alpha$, un coefficiente $\\beta$ e un errore gaussiano $\\sigma$:\n\n$$\ny_n = \\alpha + \\beta\\,x_n + \\varepsilon_n,\\qquad \\varepsilon_n \\sim \\mathcal{N}(0,\\sigma).\n$$\n\nIn Stan *con priors espliciti e debolmente informativi*:\n\n```stan\ndata {\n  int<lower=1> N;\n  vector[N] x;               // predittore (consigliato centrare in R)\n  vector[N] y;               // risposta continua\n}\nparameters {\n  real alpha;                // intercetta (valore di y al centro di x)\n  real beta;                 // coefficiente\n  real<lower=0> sigma;       // DS dell'errore\n}\nmodel {\n  // Priors debolmente informativi (propri)\n  alpha ~ normal(0, 10);\n  beta  ~ normal(0, 5);\n  sigma ~ student_t(4, 0, 10);  // half-Student-t implicita grazie al vincolo <lower=0>\n\n  // Likelihood (vettorializzata)\n  y ~ normal(alpha + beta * x, sigma);\n}\ngenerated quantities {\n  vector[N] mu = alpha + beta * x;\n  vector[N] y_rep;\n  for (n in 1:N) y_rep[n] = normal_rng(mu[n], sigma);\n\n  // R^2 \"bayesiano\" (Gelman et al.)\n  real R2 = variance(mu) / (variance(mu) + square(sigma));\n}\n```\nIn questo modello: \n\n* *N* è il numero di osservazioni; \n* per ogni osservazione abbiamo un valore di *x* (predittore) e un valore di *y* (variabile risposta);\n* `alpha ~ normal(0, 10)` e `beta ~ normal(0, 5)` sono *priors propri, debolmente informativi* sulla scala della variabile *y*;\n* `sigma ~ student_t(4, 0, 10)` è una *half-Student-t implicita* (perché `sigma` è vincolata $>0$); è un prior robusto e poco informativo sulla scala di $\\sigma$.\n\nSe venissero rimosse tutte le righe di prior, Stan *non* aggiungerebbe priors di default:\n\n* `alpha` e `beta` avrebbero di fatto un *prior piatto improprio* su $(-\\infty, +\\infty)$;\n* `sigma` avrebbe un *prior piatto improprio* su $(0, +\\infty)$.\n\nCon likelihood gaussiane questi priors possono talvolta produrre una posteriore propria, ma *non è una buona pratica*: meglio usare priors (anche deboli) e fare *prior predictive checks*.\n\n\n::: {.callout-tip}\n### Centrare i predittori\n\n*Consigliato* centrare `x` in R (`x_c <- x - mean(x)`) e passare `x_c` al modello: rende `alpha` interpretabile (valore atteso di $y$ al centro dei predittori) e migliora la geometria posteriore (minore correlazione tra $\\alpha$ e $\\beta$).\n:::\n\n\n### Notazione matriciale e vettorializzazione\n\nLa riga seguente è *vettorializzata*, cioè calcola la probabilità di tutte le osservazioni in un’unica istruzione:\n\n```stan\ny ~ normal(alpha + beta * x, sigma);\n```\nÈ equivalente a scrivere:\n\n```stan\nfor (n in 1:N) {\n  y[n] ~ normal(alpha + beta * x[n], sigma);\n}\n```\n\nLa forma vettorializzata è più compatta *e molto più veloce* da eseguire.\nIn Stan, quando un argomento di una distribuzione è un vettore, anche gli altri argomenti possono esserlo (purché abbiano la stessa dimensione) oppure possono essere scalari (in tal caso vengono “riciclati” per tutte le osservazioni).\n\n\n## Notazione matriciale: regressione multipla\n\nOra estendiamo il ragionamento al caso in cui i predittori siano più di uno, così da introdurre il concetto di *effetto parziale*.\n\nQuando abbiamo *più predittori* per ciascuna osservazione, possiamo scrivere il modello di regressione in *forma vettoriale/matriciale* [@caudek2001statistica]. Con più predittori per osservazione, in forma compatta il modello è:\n\n$$\n\\mathbf{y} = \\mathbf{X} \\, \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\ndove:\n\n* $\\mathbf{y}$ è un vettore colonna di dimensione $N \\times 1$, che contiene la variabile risposta per le $N$ osservazioni;\n* $\\mathbf{X}$ è una matrice $N \\times K$, dove ogni riga corrisponde a un’osservazione e ogni colonna a un predittore (la prima colonna, se presente, è di 1 e serve per l’intercetta);\n* $\\boldsymbol{\\beta}$ è un vettore colonna di dimensione $K \\times 1$, che contiene i coefficienti del modello (inclusa l’intercetta se la colonna di 1 è presente in $\\mathbf{X}$);\n* $\\boldsymbol{\\varepsilon}$ è un vettore $N \\times 1$ di errori casuali, che assumiamo distribuiti come $\\mathcal{N}(0, \\sigma^2)$.\n\n\n| Notazione matematica              | Significato                                                      | Oggetto in Stan                  | Dichiarazione Stan                           |\n|------------------------------------|-------------------------------------------------------------------|-----------------------------------|-----------------------------------------------|\n| $\\mathbf{y}$                     | Vettore colonna degli esiti (variabile risposta)                 | `y`                               | `vector[N] y;`                                |\n| $\\mathbf{X}$                     | Matrice dei predittori (N osservazioni × K predittori)            | `x`                               | `matrix[N, K] x;`                             |\n| $\\boldsymbol{\\beta}$             | Vettore colonna dei coefficienti di regressione                   | `beta`                            | `vector[K] beta;`                             |\n| $\\beta_0$                        | Intercetta                                                        | `alpha`                           | `real alpha;`                                 |\n| $\\sigma$                         | Deviazione standard dell’errore                                   | `sigma`                           | `real<lower=0> sigma;`                        |\n| $\\hat{\\mathbf{y}} = \\mathbf{X} \\boldsymbol{\\beta} + \\beta_0$ | Vettore delle predizioni lineari                                   | `x * beta + alpha`                | Espressione all’interno del modello Stan      |\n| $\\boldsymbol{\\varepsilon}$       | Vettore degli errori casuali                                      | —                                 | Implicito nella distribuzione `normal(..., sigma)` |\n\n\n### Sviluppo riga per riga\n\nScrivendo esplicitamente il contenuto della moltiplicazione $\\boldsymbol{X} \\, \\boldsymbol{\\beta}$, otteniamo:\n\n$$\n\\begin{bmatrix}\ny_{1} \\\\\ny_{2} \\\\\n\\vdots \\\\\ny_{N}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots & x_{1,K-1} \\\\\n1 & x_{21} & x_{22} & \\dots & x_{2,K-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{N1} & x_{N2} & \\dots & x_{N,K-1}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_{0} \\\\\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{K-1}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\varepsilon_{1} \\\\\n\\varepsilon_{2} \\\\\n\\vdots \\\\\n\\varepsilon_{N}\n\\end{bmatrix}\n$$\n\n\n### Interpretazione\n\n* *Ogni riga* della matrice $\\mathbf{X}$ contiene i valori dei predittori per una singola osservazione.\n* *La stessa* colonna di $\\boldsymbol{\\beta}$ (cioè lo stesso coefficiente) si applica a tutte le righe, moltiplicando il rispettivo valore del predittore.\n* Il termine $\\beta_0$ è l’intercetta: è costante e si applica a tutte le osservazioni.\n* La moltiplicazione $\\mathbf{X} \\, \\boldsymbol{\\beta}$ produce un vettore $N \\times 1$ di *valori previsti* ($\\hat{y}$), uno per ogni osservazione.\n* Gli *errori* $\\boldsymbol{\\varepsilon}$ rappresentano la differenza tra il valore osservato $y_i$ e il valore previsto $\\hat{y}_i$.\n\n\n### Esempio con due variabili indipendenti\n\nSe $N=3$ e $K=3$ (intercetta + 2 predittori), abbiamo:\n\n$$\n\\underbrace{\\begin{bmatrix}\ny_{1} \\\\ y_{2} \\\\ y_{3}\n\\end{bmatrix}}_{y}\n=\n\\underbrace{\\begin{bmatrix}\n1 & x_{11} & x_{12} \\\\\n1 & x_{21} & x_{22} \\\\\n1 & x_{31} & x_{32}\n\\end{bmatrix}}_{X}\n\\underbrace{\\begin{bmatrix}\n\\beta_{0} \\\\ \\beta_{1} \\\\ \\beta_{2}\n\\end{bmatrix}}_{\\beta}\n+\n\\underbrace{\\begin{bmatrix}\n\\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\varepsilon_{3}\n\\end{bmatrix}}_{\\varepsilon}\n$$\nIl che equivale a:\n\n$$\n\\begin{cases}\ny_{1} = \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + \\varepsilon_{1} \\\\\ny_{2} = \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + \\varepsilon_{2} \\\\\ny_{3} = \\beta_0 + \\beta_1 x_{31} + \\beta_2 x_{32} + \\varepsilon_{3}\n\\end{cases}\n$$\n\n\n### Interpretazione dei coefficienti *parziali* di regressione\n\nIn un modello di *regressione multipla* ogni coefficiente $\\beta_j$ rappresenta l’*effetto parziale* del predittore $x_j$ sulla variabile risposta $y$, *tenendo costanti* (cioè controllando per) gli altri predittori inclusi nel modello.\n\n* *Effetto parziale*: $\\beta_j$ indica di quanto ci si attende che cambi $y$ *in media* se $x_j$ aumenta di una unità, mentre tutti gli altri predittori del modello restano invariati.\n* *Unità di misura*: l’interpretazione è sempre nella scala originale di $y$ e $x_j$ (se non abbiamo standardizzato).\n* *Segno*: positivo se, a parità degli altri predittori, un aumento di $x_j$ è associato a un aumento di $y$; negativo se associato a una diminuzione.\n\n\n#### Differenza con la regressione bivariata\n\nSe stimiamo un modello *bivariato* (cioè con un solo predittore per volta), il coefficiente di regressione di $x_j$ rappresenta l’associazione *totale* tra $x_j$ e $y$, senza tenere conto di altri fattori. Questo può essere fuorviante quando  i predittori sono *correlati tra loro* e/o esiste un predittore $x_k$ che spiega parte della stessa varianza di $y$ che spiega $x_j$.\n\nIn questi casi:\n\n* *Modello bivariato*: il coefficiente di $x_j$ include anche l’effetto “indiretto” dovuto alla sua correlazione con altri predittori.\n* *Modello multiplo*: il coefficiente di $x_j$ è “depurato” dagli effetti degli altri predittori, cioè riflette l’associazione residua unica di $x_j$ con $y$.\n\n\n## Esempio numerico\n\nPer chiarire concretamente la differenza tra l'effetto totale (o grezzo) di un predittore e il suo effetto parziale (o netto), simuliamo uno scenario realistico ipotizzando che l’*esito* $y$ sia un punteggio di *stress* su scala 0–10. I due predittori sono:\n\n* $x_1$: *affetto negativo* istantaneo su scala 0–10 (più alto = più negativo);\n* $x_2$: *ore di sonno* nell’ultima notte su scala 0–10.\n\nSupponiamo che:\n\n* a parità di altre condizioni, *aumentare l’affetto negativo* di 1 punto (su 0–10) faccia crescere lo stress di qualche punto (effetto positivo);\n* *dormire di più* riduca lo stress (effetto negativo);\n* il *livello medio* di stress a affetto negativo medio e sonno medio sia moderato.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(42)\n\nN <- 2000\nrho <- 0.8\n\nx1 <- rnorm(N, 0, 1)\nz  <- rnorm(N, 0, 1)\nx2 <- rho * x1 + sqrt(1 - rho^2) * z  # cor(x1, x2) ~ 0.8\n\nbeta1_true <-  1\nbeta2_true <- -2\nsigma_true <-  0.5\n\ny <- beta1_true * x1 + beta2_true * x2 + rnorm(N, 0, sigma_true)\n\ndati <- tibble(y = y + abs(min(y)), x1 = x1 + abs(min(x1)), x2 = x2 + abs(min(x2)))\nsummary(dati)\n#>        y              x1             x2      \n#>  Min.   :0.00   Min.   :0.00   Min.   :0.00  \n#>  1st Qu.:3.26   1st Qu.:2.70   1st Qu.:2.73  \n#>  Median :4.23   Median :3.36   Median :3.38  \n#>  Mean   :4.23   Mean   :3.36   Mean   :3.38  \n#>  3rd Qu.:5.21   3rd Qu.:4.03   3rd Qu.:4.06  \n#>  Max.   :9.03   Max.   :6.96   Max.   :6.70\n```\n:::\n\n\nSe i due predittori $x_1$ e $x_2$ sono *correlati* (ad esempio: chi dorme poco tende anche a percepire più stress):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncor(dati$x1, dati$x2)\n#> [1] 0.8\n```\n:::\n\n\nallora i coefficienti stimati assumono significati diversi a seconda del modello:\n\n* Nel *modello bivariato* $y \\sim x_1$, il coefficiente di $x_1$ riflette non solo l’effetto diretto dello stress sull’ansia, ma anche l’effetto *indiretto* mediato dalla correlazione con il sonno (più stress → meno sonno → più ansia).\n* Nel *modello multiplo* $y \\sim x_1 + x_2$, il coefficiente di $x_1$ isola invece l’effetto *unico* dello stress, cioè la variazione di ansia associata a un incremento di stress *a parità di ore di sonno*.\n\nIn sintesi:\n\n* i coefficienti *bivariati* misurano l’*associazione totale* tra predittore e risposta,\n* i coefficienti *parziali* misurano l’*associazione unica*, controllando per gli altri predittori,\n* la differenza tra i due diventa rilevante quando i predittori sono *correlati*.\n\nPer rendere chiara la differenza, usiamo i dati simulati (`y`, `x1`, `x2`) e stimiamo i coefficienti di regressione in due modi diversi:\n\n– usando un predittore alla volta (`y ~ x1` e `y ~ x2`);\n– usando entrambi i predittori insieme (`y ~ x1 + x2`).\n\nPer semplicità, iniziamo a stimare i coefficienti con `lm()` (poi useremo Stan):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Modelli bivariati\nfit_biv_x1 <- lm(y ~ x1, data = dati)\nfit_biv_x2 <- lm(y ~ x2, data = dati)\n\n# Modello multiplo\nfit_mult <- lm(y ~ x1 + x2, data = dati)\n\n# Confronto dei coefficienti\ncoefs <- data.frame(\n  Modello = c(\"Bivariato x1\", \"Bivariato x2\", \"Multiplo\"),\n  beta_x1 = c(coef(fit_biv_x1)[\"x1\"], NA, coef(fit_mult)[\"x1\"]),\n  beta_x2 = c(NA, coef(fit_biv_x2)[\"x2\"], coef(fit_mult)[\"x2\"])\n)\ncoefs\n#>        Modello beta_x1 beta_x2\n#> 1 Bivariato x1  -0.618      NA\n#> 2 Bivariato x2      NA   -1.22\n#> 3     Multiplo   1.017   -2.02\n```\n:::\n\n\nI valori veri dei coefficienti erano:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta1_true <-  1\nbeta2_true <- -2\n```\n:::\n\n\n\nCosa osserviamo:\n\n* nel *modello bivariato* `y ~ x1`, il coefficiente di `x1` è *negativo* (≈ −0.6), anche se il vero effetto è *positivo* (+1);\n* nel *modello multiplo* `y ~ x1 + x2`, i coefficienti si avvicinano ai valori veri (≈ +1 per `x1`, ≈ −2 per `x2`): qui leggiamo gli *effetti parziali*, cioè ciascun predittore “a parità” dell’altro.\n\nIl cambio di segno nel bivariato è un chiaro esempio di *bias da variabile omessa*: se escludiamo `x2` (sonno), l’effetto stimato di `x1` (affetto negativo) assorbe anche parte dell’influenza del sonno, arrivando persino a invertirne il segno.\n\nMatematicamente, l’atteso del coefficiente bivariato è:\n\n$$\n\\mathbb{E}\\!\\left[\\hat\\beta^{(biv)}_{x1}\\right] \\;=\\; \n\\beta_1 \\;+\\; \\beta_2 \\frac{\\operatorname{Cov}(x_1, x_2)}{\\operatorname{Var}(x_1)} .\n$$\n\nQuando $\\beta_2$ e $\\operatorname{Cov}(x_1, x_2)$ hanno *segni opposti* e grande ampiezza, la correzione può superare $\\beta_1$ e invertire il segno.\n\n::: {.callout-note}\n**Messaggio didattico:**\n\n* I coefficienti *bivariati* misurano l’*associazione totale* tra un predittore e la risposta.\n* I coefficienti *parziali* del modello multiplo misurano l’*associazione unica*, condizionata agli altri predittori.\n* In psicologia (e non solo), questo è cruciale: includere o escludere variabili di controllo — ad esempio livello socioeconomico, sonno o supporto sociale — può cambiare radicalmente la stima di un effetto.\n:::\n\n\n### La regressione multipla in Stan\n\nOra ripetiamo l'analisi precedente usando Stan. Prima però ci rinfreschiamo la memoria relativamente alle operazioni di algebra matriciale necessarie per capire il codice Stan.\n\n::: {.callout-note collapse = \"true\"}\n## Come funziona la moltiplicazione tra matrici (ripasso essenziale)\n\n**Conformabilità.** Due matrici si possono moltiplicare solo se il *numero di colonne* della prima coincide con il *numero di righe* della seconda.  Se $A$ è $m \\times k$ e $B$ è $k \\times n$, allora il prodotto $C = A B$ *esiste* ed è una matrice di dimensione $m \\times n$.\n\n**Elemento $c_{ij}$.** L’elemento sulla *riga $i$* e *colonna $j$* di $C$ si ottiene come *prodotto scalare* tra:\n\n- la riga $i$-esima di $A$ e\n- la colonna $j$-esima di $B$.\n\nPer prodotto scalare intendiamo: *somma dei prodotti degli elementi corrispondenti*.  \nFormalmente,\n\n$$\nc_{ij} \\;=\\; \\sum_{t=1}^{k} a_{i t}\\, b_{t j}.\n$$\n\n\n**Esempio numerico.**\n\nSiano\n\n$$\nA=\\begin{bmatrix}\n1 & 2 & 3\\\\\n4 & 5 & 6\n\\end{bmatrix}\n\\quad (2\\times 3), \\qquad\nB=\\begin{bmatrix}\n1 & 0\\\\\n-1 & 2\\\\\n2 & 1\n\\end{bmatrix}\n\\quad (3\\times 2).\n$$\n\nSono *conformabili* (3 colonne di $A$ = 3 righe di $B$). Il prodotto $C=AB$ sarà $2\\times 2$.\n\nCalcoliamo i singoli elementi:\n\n- $c_{11} = [1,2,3]\\cdot[1,-1,2]^\\top = 1\\cdot 1 + 2\\cdot(-1) + 3\\cdot 2 = 1 - 2 + 6 = 5$\n- $c_{12} = [1,2,3]\\cdot[0,2,1]^\\top = 1\\cdot 0 + 2\\cdot 2 + 3\\cdot 1 = 0 + 4 + 3 = 7$\n- $c_{21} = [4,5,6]\\cdot[1,-1,2]^\\top = 4\\cdot 1 + 5\\cdot(-1) + 6\\cdot 2 = 4 - 5 + 12 = 11$\n- $c_{22} = [4,5,6]\\cdot[0,2,1]^\\top = 4\\cdot 0 + 5\\cdot 2 + 6\\cdot 1 = 0 + 10 + 6 = 16$\n\nQuindi\n\n$$\nC=AB=\\begin{bmatrix}\n5 & 7\\\\\n11 & 16\n\\end{bmatrix}.\n$$\n\n**Verifica in R.**\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nA <- matrix(c(1,2,3, 4,5,6), nrow = 2, byrow = TRUE)\nB <- matrix(c(1,0, -1,2, 2,1), nrow = 3, byrow = TRUE)\nA %*% B\n#>      [,1] [,2]\n#> [1,]    5    7\n#> [2,]   11   16\n```\n:::\n\n:::\n\n\nCon più predittori, anche in Stan possiamo usare la *notazione matriciale*:\n\n```stan\ndata {\n  int<lower=1> N;\n  int<lower=1> K;\n  matrix[N, K] X;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  alpha ~ normal(0, 10);\n  beta  ~ normal(0, 5);\n  sigma ~ student_t(4, 0, 10);\n\n  y ~ normal(X * beta + alpha, sigma);\n}\ngenerated quantities {\n  vector[N] mu = alpha + X * beta;\n  vector[N] y_rep;\n  for (n in 1:N) y_rep[n] = normal_rng(mu[n], sigma);\n  real R2 = variance(mu) / (variance(mu) + square(sigma));\n}\n```\nQui:\n\n* `x` è una matrice *N × K* di predittori;\n* `beta` è un vettore con *K* coefficienti;\n* `x * beta` produce un vettore di *N* valori predetti;\n* aggiungendo `alpha` otteniamo la previsione completa per ogni osservazione.\n\nAnche in questo caso la forma vettorializzata è equivalente a:\n\n```stan\nfor (n in 1:N) {\n  y[n] ~ normal(x[n] * beta + alpha, sigma);\n}\n```\n\n\n### Intercetta come colonna della matrice dei predittori\n\nSe preferiamo non dichiarare un parametro separato per l’intercetta (`alpha`), possiamo inserire una *colonna di 1* come prima colonna della matrice `x`. In questo caso il primo elemento di `beta` (`beta[1]`) fungerà da intercetta.\n\nSe però vogliamo assegnare un prior diverso all’intercetta rispetto agli altri coefficienti, è meglio dichiarare `alpha` come parametro separato. Questo è anche leggermente più efficiente, ma la differenza di velocità è trascurabile: la scelta va fatta per *chiarezza del codice*.\n\nScriviamo dunque il modello Stan per i dati dell'esempio in discussione.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstancode <- write_stan_file(\"\ndata {\n  int<lower=1> N;\n  int<lower=1> K;\n  matrix[N, K] X;\n  vector[N] y;\n}\nparameters {\n  real alpha;\n  vector[K] beta;\n  real<lower=0> sigma;\n}\nmodel {\n  alpha ~ normal(0, 10);\n  beta  ~ normal(0, 5);\n  sigma ~ student_t(4, 0, 10);\n\n  y ~ normal(X * beta + alpha, sigma);\n}\ngenerated quantities {\n  vector[N] mu = alpha + X * beta;\n  vector[N] y_rep;\n  for (n in 1:N) y_rep[n] = normal_rng(mu[n], sigma);\n  real R2 = variance(mu) / (variance(mu) + square(sigma));\n}\n\")\n```\n:::\n\n\nNotiamo la scelta dei *prior* nel modello Stan:\n\n```stan\nalpha ~ normal(0, 10);\nbeta  ~ normal(0, 5);\nsigma ~ student_t(4, 0, 10);\n```\n\n* **Intercetta `alpha`**: prior `normal(0,10)`.\n  È un prior *debole* e poco informativo: ammette con alta probabilità valori compresi tra circa −20 e +20. È adatto se i dati sono centrati (così l’intercetta rappresenta il valore medio di $y$). Se invece $y$ ha una scala diversa, conviene adeguare la deviazione standard del prior alla variabilità attesa dei dati.\n\n* **Coefficienti `beta`**: prior `normal(0,5)`.\n  Indica che, a priori, ci aspettiamo effetti di ampiezza moderata (nell’ordine di qualche unità di $y$ per uno scarto unitario di $x$), ma lasciamo aperta la possibilità a effetti anche più grandi. Se i predittori vengono *standardizzati*, un prior ancora più stretto come `normal(0,1)` diventa naturale, perché corrisponde all’ipotesi che la maggior parte degli effetti sia inferiore a ±2 deviazioni standard di $y$.\n\n* **Deviazione standard `sigma`**: prior `student_t(4, 0, 10)`.\n  È una distribuzione *half-Student-t* (positiva per vincolo `<lower=0>`), con code più pesanti della normale: questo rende il prior robusto, permettendo valori piccoli ma anche occasionalmente molto grandi per $\\sigma$. In pratica, evita che il modello “forzi” troppo la variabilità residua, ma previene anche valori assurdi.\n\n**Messaggio didattico:** questi prior sono *propri e debolmente informativi*: non costringono il modello a soluzioni arbitrarie, ma al tempo stesso evitano i problemi dei *prior impropri* (che Stan non assegna mai di default).\n\nIn generale, conviene:\n\n* centrare e, se possibile, standardizzare i predittori;\n* scegliere prior compatibili con la scala dei dati;\n* verificare le implicazioni con un *prior predictive check* (simulando $y$ dai soli prior).\n\nCompiliamo:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod <- cmdstan_model(stancode, compile = TRUE)\n```\n:::\n\n\n#### Un solo predittore\n\nIniziamo stimando il modello con *un solo predittore*. Costruiamo la matrice $X$ di dimensione $N \\times 1$ (una sola colonna) e centriamo i dati per rendere l’intercetta più interpretabile:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Scegliamo il predittore (qui x1; per usare x2 basta sostituire \"x1\")\nx1_c <- dati$x1 - mean(dati$x1)  # centratura rispetto alla media\n\n# Creiamo una matrice N×1 (attenzione: non usare as.numeric, che schiaccia la matrice)\nX <- matrix(x1_c, ncol = 1)\nK <- ncol(X)  # qui sarà 1\n```\n:::\n\n\nPrepariamo i dati per Stan:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstan_data <- list(\n  N = nrow(dati),\n  K = K,   # numero di predittori: 1\n  X = X,   # matrice N×1\n  y = dati$y\n)\nglimpse(stan_data)  # controllo rapido della lista\n#> List of 4\n#>  $ N: int 2000\n#>  $ K: int 1\n#>  $ X: num [1:2000, 1] 1.387 -0.549 0.379 0.648 0.42 ...\n#>  $ y: num [1:2000] 3.01 4.47 5.9 6.42 4.52 ...\n```\n:::\n\n\nEseguiamo il campionamento MCMC:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- mod$sample(\n  data = stan_data,\n  seed = 2025,\n  chains = 4,\n  parallel_chains = 4,\n  iter_warmup = 1000,\n  iter_sampling = 2000\n)\n```\n:::\n\n\nSintesi delle stime a posteriori:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsumm <- fit$summary(variables = c(\"alpha\", \"beta\", \"sigma\", \"R2\"))\nsumm\n#> # A tibble: 4 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 alpha     4.233  4.234 0.030 0.030  4.184  4.282 1.001 9428.401 6239.711\n#> 2 beta[1]  -0.618 -0.619 0.030 0.029 -0.667 -0.570 1.003 8381.228 6196.084\n#> 3 sigma     1.326  1.326 0.021 0.020  1.293  1.362 1.000 9120.684 5505.744\n#> 4 R2        0.177  0.177 0.015 0.015  0.153  0.202 1.003 8432.253 5653.883\n```\n:::\n\n\nCon un solo predittore, osserviamo lo stesso *bias da variabile omessa* che avevamo trovato con `lm()`: il coefficiente stimato per `x1` è distorto perché il modello non tiene conto dell’influenza di `x2`.\n\n\n#### Due predittori\n\nOra estendiamo il modello includendo *entrambi i predittori*. Creiamo una matrice $X$ di dimensione $N \\times 2$, centrando entrambe le colonne:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Matrice con due predittori centrati\nX <- cbind(dati$x1, dati$x2)\nX <- scale(X, center = TRUE, scale = FALSE)\nK <- ncol(X)  # ora K = 2\n```\n:::\n\n\nPrepariamo i dati per Stan:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstan2_data <- list(\n  N = nrow(dati), \n  K = K, \n  X = X, \n  y = dati$y\n)\nglimpse(stan2_data)\n#> List of 4\n#>  $ N: int 2000\n#>  $ K: int 2\n#>  $ X: num [1:2000, 1:2] 1.387 -0.549 0.379 0.648 0.42 ...\n#>   ..- attr(*, \"scaled:center\")= num [1:2] 3.36 3.38\n#>  $ y: num [1:2000] 3.01 4.47 5.9 6.42 4.52 ...\n```\n:::\n\n\nCampionamento:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit2 <- mod$sample(\n  data = stan2_data,\n  seed = 2025, \n  chains = 4, \n  parallel_chains = 4,\n  iter_warmup = 1000, \n  iter_sampling = 2000\n)\n```\n:::\n\n\nSintesi delle stime:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsumm <- fit2$summary(variables = c(\"alpha\", \"beta\", \"sigma\", \"R2\"))\nsumm\n#> # A tibble: 5 × 10\n#>   variable   mean median    sd   mad     q5    q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl>  <dbl>  <dbl> <dbl>    <dbl>    <dbl>\n#> 1 alpha     4.233  4.233 0.012 0.012  4.214  4.253 1.000 6567.012 5247.360\n#> 2 beta[1]   1.016  1.017 0.019 0.019  0.986  1.047 1.001 4515.829 4602.377\n#> 3 beta[2]  -2.017 -2.017 0.018 0.018 -2.047 -1.986 1.001 4308.570 4405.765\n#> 4 sigma     0.515  0.515 0.008 0.008  0.502  0.528 1.000 6678.973 5652.335\n#> 5 R2        0.876  0.876 0.004 0.004  0.869  0.882 1.001 6445.047 5823.024\n```\n:::\n\n\n**Interpretazione**\n\n* I *posterior mean* di `beta[1]` e `beta[2]` sono vicini ai valori veri della simulazione (+1 e −2), e gli intervalli di credibilità li comprendono.\n* Questo conferma che il modello multiplo riesce a recuperare gli effetti “puliti”, eliminando il bias da variabile omessa.\n\nLavorando su scala grezza, i coefficienti sono immediatamente leggibili:\n\n* `beta[1]`: *+1 punto di affetto negativo → +1 punto di stress*,\n* `beta[2]`: *+1 ora di sonno → −2 punti di stress*.\n\n\n### Diagnostica\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit2$cmdstan_diagnose()\n#> Checking sampler transitions treedepth.\n#> Treedepth satisfactory for all transitions.\n#> \n#> Checking sampler transitions for divergences.\n#> No divergent transitions found.\n#> \n#> Checking E-BFMI - sampler transitions HMC potential energy.\n#> E-BFMI satisfactory.\n#> \n#> Rank-normalized split effective sample size satisfactory for all parameters.\n#> \n#> Rank-normalized split R-hat values satisfactory for all parameters.\n#> \n#> Processing complete, no problems detected.\n```\n:::\n\n\n### Controllo predittivo posteriore\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndraws <- fit2$draws()\n# Estrazione sicura dei draw per mu e y_rep\n# (uso 'variables =' e imposto l'ordine esplicito 1..N per y_rep)\nmu_draws   <- posterior::as_draws_matrix(fit2$draws(variables = \"mu\"))\nyrep_draws <- posterior::as_draws_matrix(\n  fit2$draws(variables = paste0(\"y_rep[\", 1:N, \"]\"))\n)\n\n# Controllo di coerenza dimensioni (opzionale ma utile)\nstopifnot(ncol(yrep_draws) == length(dati$y))\n\n# PPC: uso 100 repliche (righe = draw, colonne = osservazioni)\nns <- min(100, nrow(yrep_draws))\nbayesplot::ppc_dens_overlay(dati$y, yrep_draws[1:ns, ])\n```\n\n::: {.cell-output-display}\n![](05_stan_regression_files/figure-html/unnamed-chunk-18-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nSe le curve replicate si sovrappongono bene alla distribuzione osservata di $y$, il modello ha una buona calibrazione predittiva.\n\n\n## Robustezza agli *outlier*: regressione t di Student\n\nLa regressione lineare classica assume che gli errori siano gaussiani, cioè distribuiti come $\\mathcal{N}(0,\\sigma)$. Questa ipotesi funziona bene quando la distribuzione dei residui è “ben comportata”. Tuttavia, in psicologia capita spesso di avere osservazioni *estreme* (outlier) che non si adattano a questa ipotesi: pochi valori anomali possono “tirare” la retta e distorcere molto le stime.\n\nUn rimedio è sostituire la normale con la *distribuzione t di Student*, che ha *code più pesanti*. Questo significa che gli outlier ricevono meno peso: la stima è più robusta, perché non si lascia influenzare eccessivamente da poche osservazioni estreme.\n\n### Il modello in Stan\n\nEcco la versione “robusta” della regressione multipla:\n\n```stan\n// file: lm_multiple_t.stan\ndata {\n  int<lower=1> N;          // numero osservazioni\n  int<lower=1> K;          // numero predittori\n  matrix[N, K] X;          // matrice dei predittori\n  vector[N] y;             // variabile risposta\n}\nparameters {\n  real alpha;              // intercetta\n  vector[K] beta;          // coefficienti di regressione\n  real<lower=0> sigma;     // scala residui\n  real<lower=2> nu;        // gradi di libertà (>=2 per varianza finita)\n}\nmodel {\n  // Priors\n  alpha ~ normal(0, 10);\n  beta  ~ normal(0, 5);\n  sigma ~ student_t(4, 0, 10);\n  nu    ~ exponential(0.1);   // media ≈ 10, lascia aperta la possibilità a valori piccoli\n\n  // Likelihood: regressione t (GLM)\n  target += student_t_id_glm_lpdf(y | nu, X, alpha, beta, sigma);\n}\ngenerated quantities {\n  vector[N] mu = alpha + X * beta;\n  vector[N] y_rep;\n  for (n in 1:N)\n    y_rep[n] = student_t_rng(nu, mu[n], sigma);\n\n  real R2 = variance(mu) / (variance(mu) + square(sigma));\n}\n```\n\n### Interpretazione dei parametri\n\n* `nu` controlla lo “spessore delle code”:\n\n  * valori grandi (es. $\\nu > 30$) rendono la t praticamente indistinguibile da una normale,\n  * valori piccoli (es. $\\nu \\approx 3$–$5$) producono code pesanti e quindi maggiore robustezza.\n* `sigma` misura la dispersione residua, ma ora è separata dalla robustezza delle code.\n\n**Messaggio didattico:** con la t di Student il modello “capisce” che ci sono dati un po’ anomali e li tratta con cautela, senza lasciarsi dominare da essi.\n\n\n## Non linearità (errore di specificazione) e PPC\n\nUn altro caso frequente di *errore di specificazione* si ha quando forziamo un modello *lineare* su dati che in realtà hanno una *curvatura* (per esempio quadratica). Anche se i residui sembrano gaussiani, il modello sbaglia sistematicamente le previsioni.\n\n### Simuliamo dati non lineari\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(11)\nN <- 300\nx <- rnorm(N)\ny_true <- 1 + 2*x + 1.5*x^2         # relazione quadratica\ny <- y_true + rnorm(N, 0, 1.5)\n\nX <- cbind(x)                       # modello lineare \"sbagliato\"\nK <- 1\n```\n:::\n\n\n### Fittiamo un modello lineare\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_lin <- mod$sample(\n  data = list(N = N, K = K, X = scale(X, center = TRUE, scale = FALSE), y = y),\n  seed = 2025, chains = 4, parallel_chains = 4,\n  iter_warmup = 1000, iter_sampling = 2000\n)\n```\n:::\n\n\n### Controllo predittivo posteriore\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nyrep_lin <- as_draws_matrix(fit_lin$draws(\"y_rep\"))\nbayesplot::ppc_scatter_avg(y = y, yrep = yrep_lin[1:200, ])\n```\n\n::: {.cell-output-display}\n![](05_stan_regression_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Cosa osserviamo?\n\nIl PPC mostra un *mismatch sistematico*: le previsioni lineari non seguono la curvatura dei dati veri. Il modello sta sbagliando in maniera *strutturale*, non solo per rumore.\n\n### Come rimediare?\n\n* Aggiungere un termine quadratico ($x^2$) come nuovo predittore;\n* Usare polinomi di grado superiore;\n* Oppure usare modelli flessibili (spline, funzioni base, Gaussian process).\n\n**Messaggio didattico:** i PPC non servono solo a “validare” un modello, ma soprattutto a diagnosticare quando la forma funzionale è troppo rigida. In questo caso, il PPC rivela l’esigenza di un modello più flessibile.\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nIn questo capitolo abbiamo tradotto il modello di regressione lineare nel linguaggio *Stan*, facendo un passo importante verso la pratica della modellazione bayesiana. Abbiamo visto come i diversi blocchi del programma permettano di dichiarare i dati, definire i parametri, specificare la struttura del modello e produrre quantità derivate utili all’analisi.\n\nIl passaggio a Stan non è solo tecnico. Significa collocarsi in un quadro in cui possiamo esprimere modelli molto più flessibili e realistici rispetto ai casi semplici affrontati in precedenza. La regressione lineare diventa così il punto di partenza per una modellazione che può includere predittori multipli, effetti gerarchici e specificazioni personalizzate, mantenendo intatta la logica dell’aggiornamento bayesiano.\n\nNaturalmente, questa maggiore potenza comporta anche nuove responsabilità. Scrivere un modello in Stan richiede di essere consapevoli delle assunzioni fatte, di controllare la qualità del campionamento e di interpretare i risultati alla luce di ciò che il modello può — e non può — dirci. In questo senso, Stan non sostituisce la comprensione statistica: la rende anzi ancora più necessaria.\n\nNei capitoli successivi discuteremo due aspetti cruciali che emergono quando si applica la regressione ai dati reali: l’*errore di specificazione del modello* e il *bias da variabile omessa*. Saranno esempi concreti per riflettere sui limiti intrinseci del modello lineare e sull’importanza di formulare ipotesi che siano non solo eleganti matematicamente, ma anche sostantivamente adeguate alla complessità dei fenomeni psicologici.\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] cmdstanr_0.9.0        pillar_1.11.0         tinytable_0.13.0     \n#>  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#>  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [25] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gridExtra_2.3         inline_0.3.21         sandwich_3.1-1       \n#>  [4] rlang_1.1.6           magrittr_2.0.3        multcomp_1.4-28      \n#>  [7] snakecase_0.11.1      compiler_4.5.1        reshape2_1.4.4       \n#> [10] systemfonts_1.2.3     vctrs_0.6.5           stringr_1.5.1        \n#> [13] pkgconfig_2.0.3       arrayhelpers_1.1-0    fastmap_1.2.0        \n#> [16] backports_1.5.0       labeling_0.4.3        utf8_1.2.6           \n#> [19] rmarkdown_2.29        ps_1.9.1              ragg_1.5.0           \n#> [22] purrr_1.1.0           xfun_0.53             cachem_1.1.0         \n#> [25] jsonlite_2.0.0        broom_1.0.9           parallel_4.5.1       \n#> [28] R6_2.6.1              stringi_1.8.7         RColorBrewer_1.1-3   \n#> [31] lubridate_1.9.4       estimability_1.5.1    knitr_1.50           \n#> [34] zoo_1.8-14            pacman_0.5.1          Matrix_1.7-4         \n#> [37] splines_4.5.1         timechange_0.3.0      tidyselect_1.2.1     \n#> [40] abind_1.4-8           yaml_2.3.10           codetools_0.2-20     \n#> [43] curl_7.0.0            processx_3.8.6        pkgbuild_1.4.8       \n#> [46] plyr_1.8.9            lattice_0.22-7        withr_3.0.2          \n#> [49] bridgesampling_1.1-2  coda_0.19-4.1         evaluate_1.0.5       \n#> [52] survival_3.8-3        RcppParallel_5.1.11-1 tensorA_0.36.2.1     \n#> [55] checkmate_2.3.3       stats4_4.5.1          distributional_0.5.0 \n#> [58] generics_0.1.4        rprojroot_2.1.1       rstantools_2.5.0     \n#> [61] scales_1.4.0          xtable_1.8-4          glue_1.8.0           \n#> [64] emmeans_1.11.2-8      tools_4.5.1           data.table_1.17.8    \n#> [67] mvtnorm_1.3-3         grid_4.5.1            QuickJSR_1.8.0       \n#> [70] colorspace_2.1-1      nlme_3.1-168          cli_3.6.5            \n#> [73] textshaping_1.0.3     svUnit_1.0.8          Brobdingnag_1.2-9    \n#> [76] V8_7.0.0              gtable_0.3.6          digest_0.6.37        \n#> [79] TH.data_1.1-4         htmlwidgets_1.6.4     farver_2.1.2         \n#> [82] memoise_2.0.1         htmltools_0.5.8.1     lifecycle_1.0.4      \n#> [85] MASS_7.3-65\n```\n:::\n\n:::\n\n\n## Bibliografia {.unnumbered .unlisted}\n",
    "supporting": [
      "05_stan_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}