{
  "hash": "c3dfa5df7551eb2c454e4d7bcdaee2fc",
  "result": {
    "engine": "knitr",
    "markdown": "# La regressione verso la media {#sec-linear-models-regression-toward_mean}\n\n::: {.epigraph}\n> “The mean filial regression towards mediocrity was directly proportional to the parental deviation from it.”\n>\n> -- **Francis Galton**, Regression Towards Mediocrity in Hereditary Stature, Journal of the Anthropological Institute 15, 1886, pp. 246–263.\n:::\n\n## Introduzione {.unnumbered .unlisted}\n\nIl concetto di *regressione verso la media* fu introdotto da Francis Galton alla fine dell’Ottocento, osservando la trasmissione ereditaria dell’altezza. Egli notò che i figli di padri eccezionalmente alti rimanevano in media sopra la statura generale, ma meno dei loro padri; e che lo stesso valeva, in direzione opposta, per i figli di padri molto bassi. Questo “ritorno parziale verso il centro” della distribuzione è il fenomeno che ancora oggi porta il nome di regressione verso la media.\n\nPerché avviene? Un valore estremo, come un’altezza particolarmente alta o bassa, è il risultato della combinazione di fattori genetici, ambientali e casuali. I figli ereditano solo una parte di questi fattori, e nuove influenze si aggiungono al quadro: il risultato è che le loro altezze tendono a essere meno estreme, più vicine alla media della popolazione. Non significa che il figlio di un padre altissimo diventi basso: rimane in media più alto degli altri, ma meno estremo.\n\nIl cuore statistico del fenomeno sta nella *correlazione imperfetta* tra le due variabili considerate (altezza del padre e del figlio). Se la correlazione fosse pari a 1, gli estremi si replicherebbero perfettamente. Ma quando $\\rho < 1$, i valori attesi dei figli risultano più vicini alla media rispetto a quelli dei padri. Galton stimò, nel suo esempio, una correlazione attorno a 0.5: segno che una parte importante, ma non totale, dell’estremità paterna si trasmette al figlio.\n\nStudiare la regressione verso la media ci permette di capire più a fondo la logica della regressione lineare bivariata. Si tratta infatti di un’applicazione concreta dell’idea di pendenza inferiore a 1 quando la correlazione non è perfetta. Questo fenomeno, apparentemente controintuitivo, è in realtà inevitabile nei dati psicologici e ha implicazioni profonde per l’interpretazione delle osservazioni e dei cambiamenti individuali.\n\n### Panoramica del capitolo {.unnumbered .unlisted}\n\n- Origine storica e intuizione del fenomeno (Galton).\n- Regressione e correlazione: forme standardizzate e non standardizzate.\n- Visualizzazione della RTM tramite retta di regressione.\n- RTM ≠ causalità: errori di misura e artefatti di selezione.\n\n::: {.callout-tip collapse=true}\n## Prerequisiti\n\n- Leggere il capitolo *Basic Regression* di [Statistical Inference via Data Science: A ModernDive into R and the Tidyverse (Second Edition)](https://moderndive.com/v2/).\n- Leggere il capitolo *Linear Statistical Models* [@schervish2014probability].\n:::\n\n::: {.callout-caution collapse=true title=\"Preparazione del Notebook\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(HistData)\n```\n:::\n\n:::\n\n### I dati di Galton\n\nEsaminiamo il fenomeno della regressione verso la media usando i dati di Galton. Nel pacchetto `HistData` di R sono disponibili i dati originali raccolti da Galton, che includono informazioni sull'altezza di padri, madri, figli maschi e femmine. Per semplificare l'analisi, possiamo creare un dataset che include solo l'altezza del padre e l'altezza di un figlio maschio scelto casualmente da ogni famiglia:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(1234)\n\ngalton_heights <- GaltonFamilies |>\n  filter(gender == \"male\") |>\n  group_by(family) |>\n  sample_n(1) |>\n  ungroup() |>\n  select(father, childHeight) |>\n  rename(son = childHeight)\n```\n:::\n\n\nQuesto dataset contiene due colonne: `father` (altezza del padre) e `son` (altezza del figlio maschio). Calcolando la media e la deviazione standard delle altezze dei padri e dei figli, otteniamo:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngalton_heights |> \n  summarize(\n    mean_father = mean(father), \n    sd_father   = sd(father),\n    mean_son    = mean(son), \n    sd_son      = sd(son)\n  )\n#> # A tibble: 1 × 4\n#>   mean_father sd_father mean_son sd_son\n#>         <dbl>     <dbl>    <dbl>  <dbl>\n#> 1        69.1      2.55     69.1   2.62\n```\n:::\n\n\nI risultati mostrano che, in media, i padri e i figli hanno altezze simili, anche se le distribuzioni non sono identiche. Un grafico di dispersione (scatterplot) evidenzia una chiara tendenza: padri più alti tendono ad avere figli più alti:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngalton_heights |>\n  ggplot(aes(father, son)) +\n  geom_point(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](02_regr_toward_mean_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\n### Il coefficiente di correlazione\n\nLa forza e la direzione dell'associazione lineare tra le due variabili sono misurate dal coefficiente di correlazione di Pearson, definito come:\n\n$$\n\\rho = \\frac{1}{n}\\sum_{i=1}^n \n\\left(\\frac{x_i - \\mu_x}{\\sigma_x}\\right)\n\\left(\\frac{y_i - \\mu_y}{\\sigma_y}\\right).\n$$\n\ndove $\\mu_X, \\mu_Y$ sono le medie e $\\sigma_X, \\sigma_Y$ le deviazioni standard delle rispettive popolazioni.   La sua stima campionaria, $r$, è calcolata in R come:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngalton_heights |> \n  summarize(r = cor(father, son)) |> \n  pull(r)\n#> [1] 0.443\n```\n:::\n\n\nUn coefficiente di 0.5 indica un'associazione lineare positiva di moderata intensità, implicando che l'altezza paterna spiega solo parzialmente la variabilità dell'altezza dei figli.\n\n### L'aspettativa condizionata e l'emergenza del fenomeno di regressione\n\nUn obiettivo inferenziale comune è la stima del valore atteso dell'altezza del figlio ($Y$), condizionata a un specifico valore dell'altezza del padre ($X = x_0$), formalmente $\\mathbb{E}(Y \\mid X = x_0)$. \n\nUn approccio intuitivo è stratificare i dati e calcolare la media campionaria del sottogruppo. Ad esempio, per $X = 72$ pollici:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngalton_heights |> \n  filter(round(father) == 72) |>\n  summarize(avg_son = mean(son))\n#> # A tibble: 1 × 1\n#>   avg_son\n#>     <dbl>\n#> 1    70.2\n```\n:::\n\n\nTale stima risulta sistematicamente più vicina alla media generale di $Y$ di quanto $x_0$ non lo sia alla media di $X$. Questo fenomeno è noto come regressione verso la media.\n\n## Visualizzare del fenomeno attraverso la stratificazione\n\nIl fenomeno è generalizzabile visualizzando la media condizionata $\\mathbb{E}(Y \\mid X = x)$ per diversi valori di $x$, ottenuti tramite stratificazione:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngalton_heights |>\n  mutate(father_strata = factor(round(father))) |>\n  group_by(father_strata) |>\n  summarize(avg_son = mean(son)) |>\n  ggplot(aes(x = father_strata, y = avg_son)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](02_regr_toward_mean_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=85%}\n:::\n:::\n\n\nLa nube di punti delle medie condizionate presenta una pendenza positiva ma inferiore a 45°, dimostrando visivamente la regressione verso la media.\n\n## Modello di regressione lineare e interpretazione dei parametri\n\nIl modello statistico che formalizza questa relazione è la regressione lineare semplice:\n\n$$\nY = \\beta_0 + \\beta_1 X + \\varepsilon ,\n$$\n\ndove $\\epsilon$ è un termine di errore stocastico con media zero.\n\nGli stimatori dei minimi quadrati ordinari (OLS) per i parametri $\\beta_0$ (intercetta) e $\\beta_1$ (pendenza) sono:\n\n$$\n\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}, \\quad \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\n$$\n\ndove $s_X$, $s_Y$ sono le deviazioni standard campionarie e $\\bar{X}$, $\\bar{Y}$ le medie campionarie.\n\nL'applicazione del modello ai dati di Galton fornisce:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lm(son ~ father, data = galton_heights)\ncoef(fit)\n#> (Intercept)      father \n#>      37.632       0.456\n```\n:::\n\n\nLa pendenza stimata $\\hat{\\beta}_1 = 0.454$ conferma che per ogni pollice in più del padre, l'altezza attesa del figlio aumenta di circa 0.454 pollici, un valore inferiore a 1 che è consistente con la regressione verso la media.\n\n#### Standardizzazione\n\nStandardizzando le variabili ($Z_X = (X - \\bar{X})/s_X$, $Z_Y = (Y - \\bar{Y})/s_Y$), il modello di regressione assume la forma\n\n$$\nZ_Y = \\rho\\, Z_X + \\varepsilon.\n$$\n\nVediamo come si arriva a questo risultato. Consideriamo il modello lineare semplice\n\n$$\nY = \\beta_0 + \\beta_1 X + \\varepsilon, \n\\qquad \\mathbb{E}[\\varepsilon] = 0, \n\\qquad \\mathrm{Cov}(X,\\varepsilon) = 0.\n$$\n\nDalle *equazioni normali* dei minimi quadrati otteniamo, a livello di popolazione:\n\n$$\n\\beta_1 = \\frac{\\mathrm{Cov}(X,Y)}{\\mathrm{Var}(X)}, \n\\qquad \n\\beta_0 = \\mu_Y - \\beta_1 \\mu_X.\n$$\n\nScrivendo la covarianza come $\\mathrm{Cov}(X,Y) = \\rho \\sigma_X \\sigma_Y$, segue che\n\n$$\n\\beta_1 = \\rho \\,\\frac{\\sigma_Y}{\\sigma_X}.\n$$\n\nSe ora standardizziamo $X$ e $Y$, entrambe le deviazioni standard valgono 1, quindi la pendenza diventa\n\n$$\n\\beta_1^* = \\rho.\n$$\n\nInoltre, poiché le variabili standardizzate hanno media zero, anche l’intercetta scompare.\n\nIn conclusione, la regressione di $Z_Y$ su $Z_X$ ha sempre intercetta pari a 0 e coefficiente angolare pari alla correlazione $\\rho$.\n\nInfatti, nei dati campionari:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit_standardized <- lm(scale(son) ~ scale(father), data = galton_heights)\ncoef(fit_standardized)\n#>   (Intercept) scale(father) \n#>     -7.60e-15      4.43e-01\n```\n:::\n\n\nLa pendenza è $0.4434$, numericamente uguale alla correlazione $r$ calcolata in precedenza (a meno di errori di arrotondamento). Poiché $|\\rho| < 1$, la previsione per un valore standardizzato $z_x$ sarà $\\hat{z}_y = \\rho z_x$, che è sempre, in valore assoluto, minore di $z_x$. Questo spiega matematicamente il perché un valore estremo di $X$ porta a una previsione per $Y$ che è meno estrema, ossia più vicina alla sua media standardizzata (zero).\n\nIn sintesi, la correlazione imperfetta ($\\rho < 1$) è la ragione principale per cui un valore estremo di $X$ (ad esempio, un padre molto alto) porta a un valore $\\hat{Y}$ che è sì superiore (o inferiore) alla media, ma *meno* estremo del padre. Questo “ritorno verso il centro” è ciò che chiamiamo *regressione verso la media*.\n\n## Riflessioni conclusive {.unnumbered .unlisted}\n\nLa regressione verso la media è un fenomeno statistico inevitabile ogni volta che due misurazioni sono correlate, ma non perfettamente. In termini standardizzati, se $\\rho < 1$, il valore atteso di una variabile condizionata a un valore estremo dell’altra risulta più vicino alla media. Ciò significa che le osservazioni molto alte o molto basse tenderanno, in media, a “rientrare” verso il centro nelle rilevazioni successive.\n\nQuesto effetto non è un artefatto, ma la naturale conseguenza della variabilità residua e della presenza di errori di misura. In psicologia, le implicazioni sono notevoli: senza tener conto della regressione verso la media, si rischia di interpretare come *cambiamento reale* ciò che è in parte dovuto a fluttuazioni statistiche. È il caso, ad esempio, di studi pre–post senza gruppo di controllo, in cui miglioramenti apparenti possono riflettere semplicemente la tendenza dei valori estremi ad avvicinarsi alla media.\n\nComprendere questo fenomeno significa anche comprendere meglio il funzionamento del modello di regressione lineare bivariata: la regressione verso la media è, in fondo, l’espressione grafica e concettuale del fatto che la pendenza della retta di regressione sia inferiore a 1 quando la correlazione non è perfetta.\n\nQuesto ci ricorda che la regressione lineare, pur essendo un modello fenomenologico e descrittivo, può offrire intuizioni preziose sul comportamento dei dati psicologici e sui limiti delle nostre inferenze. Nei prossimi capitoli ci sposteremo dal quadro frequentista a quello *bayesiano*, per vedere come sia possibile affrontare queste stesse questioni in termini di probabilità sui parametri, incorporando conoscenze pregresse e ottenendo una rappresentazione più trasparente dell’incertezza.\n\n::: {.callout-note collapse=true title=\"Informazioni sull'ambiente di sviluppo\"}\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.5.1 (2025-06-13)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.6.1\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.1\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] HistData_0.9-3        pillar_1.11.0         tinytable_0.13.0     \n#>  [4] patchwork_1.3.2       ggdist_3.3.3          tidybayes_3.0.7      \n#>  [7] bayesplot_1.14.0      ggplot2_3.5.2         reliabilitydiag_0.2.1\n#> [10] priorsense_1.1.1      posterior_1.6.1       loo_2.8.0            \n#> [13] rstan_2.32.7          StanHeaders_2.32.10   brms_2.22.0          \n#> [16] Rcpp_1.1.0            sessioninfo_1.2.3     conflicted_1.2.0     \n#> [19] janitor_2.2.1         matrixStats_1.5.0     modelr_0.1.11        \n#> [22] tibble_3.3.0          dplyr_1.1.4           tidyr_1.3.1          \n#> [25] rio_1.2.3             here_1.0.1           \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] svUnit_1.0.8          tidyselect_1.2.1      farver_2.1.2         \n#>  [4] fastmap_1.2.0         TH.data_1.1-4         tensorA_0.36.2.1     \n#>  [7] pacman_0.5.1          digest_0.6.37         timechange_0.3.0     \n#> [10] estimability_1.5.1    lifecycle_1.0.4       survival_3.8-3       \n#> [13] magrittr_2.0.3        compiler_4.5.1        rlang_1.1.6          \n#> [16] tools_4.5.1           knitr_1.50            labeling_0.4.3       \n#> [19] bridgesampling_1.1-2  htmlwidgets_1.6.4     curl_7.0.0           \n#> [22] pkgbuild_1.4.8        RColorBrewer_1.1-3    abind_1.4-8          \n#> [25] multcomp_1.4-28       withr_3.0.2           purrr_1.1.0          \n#> [28] grid_4.5.1            stats4_4.5.1          colorspace_2.1-1     \n#> [31] xtable_1.8-4          inline_0.3.21         emmeans_1.11.2-8     \n#> [34] scales_1.4.0          MASS_7.3-65           cli_3.6.5            \n#> [37] mvtnorm_1.3-3         rmarkdown_2.29        ragg_1.5.0           \n#> [40] generics_0.1.4        RcppParallel_5.1.11-1 cachem_1.1.0         \n#> [43] stringr_1.5.1         splines_4.5.1         parallel_4.5.1       \n#> [46] vctrs_0.6.5           V8_7.0.0              Matrix_1.7-4         \n#> [49] sandwich_3.1-1        jsonlite_2.0.0        arrayhelpers_1.1-0   \n#> [52] systemfonts_1.2.3     glue_1.8.0            codetools_0.2-20     \n#> [55] distributional_0.5.0  lubridate_1.9.4       stringi_1.8.7        \n#> [58] gtable_0.3.6          QuickJSR_1.8.0        htmltools_0.5.8.1    \n#> [61] Brobdingnag_1.2-9     R6_2.6.1              textshaping_1.0.3    \n#> [64] rprojroot_2.1.1       evaluate_1.0.5        lattice_0.22-7       \n#> [67] backports_1.5.0       memoise_2.0.1         broom_1.0.9          \n#> [70] snakecase_0.11.1      rstantools_2.5.0      coda_0.19-4.1        \n#> [73] gridExtra_2.3         nlme_3.1-168          checkmate_2.3.3      \n#> [76] xfun_0.53             zoo_1.8-14            pkgconfig_2.0.3\n```\n:::\n\n:::\n\n\n## Bibliografia {.unnumbered .unlisted} \n\n",
    "supporting": [
      "02_regr_toward_mean_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}