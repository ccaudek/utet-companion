# Introduzione {.unnumbered .unlisted}

L’analisi dei dati rappresenta un insieme di pratiche fondamentali per estrarre significato, scoprire insight e guidare il processo decisionale sulla base delle evidenze. Ma come possiamo rendere l’analisi dei dati psicologici più affidabile e rigorosa? È sufficiente applicare algoritmi standard o seguire procedure predefinite? Oppure, ridurre l’analisi a un semplice insieme di "ricette" statistiche rischia di impoverire la nostra comprensione dei fenomeni psicologici [@McElreath_rethinking]?  

Queste domande ci invitano a riflettere sulla natura stessa della ricerca empirica in psicologia. Contrariamente a quanto suggerito dall’approccio frequentista del *null hypothesis testing*, l’analisi dei dati non è un processo meccanico e automatico. Considerarla tale contribuisce a uno dei problemi più urgenti della psicologia contemporanea: la crisi della replicabilità [@korbmacher2023replication].  

La replicabilità costituisce un criterio epistemologico fondamentale nella ricerca psicologica, in quanto garantisce la validità delle inferenze scientifiche e la generalizzabilità dei risultati. L’incapacità di replicare i risultati empirici mina la robustezza delle teorie psicologiche, compromettendone la validità costruttiva ed esterna. Tale instabilità metodologica ha implicazioni sostanziali anche a livello applicativo: interventi clinici basati su evidenze non replicabili possono condurre a conclusioni erronee, mentre politiche educative e strategie organizzative fondate su risultati fragili rischiano di produrre effetti nulli o controproducenti [@ioannidis2019have; @tackett2019psychology; @funder2014improving; @shrout2018psychology].  

Il paradigma frequentista può aver contribuito alla crisi della replicabilità attraverso la sua dipendenza da *p*-value soglia-dipendenti e la tendenza a favorire risultati statisticamente significativi ma potenzialmente spurii. Parallelamente, gli incentivi accademici—quali la pressione alla pubblicazione e la preferenza per risultati innovativi—hanno sistematicamente incentivato pratiche di ricerca discutibili, tra cui il *p-hacking* e la selezione selettiva di risultati. Per contrastare queste criticità, è necessario adottare framework analitici alternativi che garantiscano maggiore trasparenza e robustezza metodologica.  

L’inferenza bayesiana rappresenta un approccio promettente, poiché consente una quantificazione diretta della probabilità delle ipotesi e una gestione più flessibile dell’incertezza [@gelman1995bayesian]. Tuttavia, la sua adozione richiede più della mera sostituzione dei metodi frequentisti: implica l’integrazione di tecniche avanzate—come la modellazione gerarchica bayesiana e l’identificazione di relazioni causali—con una rigorosa caratterizzazione dei processi generativi dei dati e delle assunzioni teoriche sottostanti [@wagenmakers2018bayesian; @oberauer2019addressing; @yarkoni2022generalizability].  

In questo testo, analizzeremo sistematicamente le limitazioni degli approcci tradizionali, esamineremo i fattori strutturali alla base della crisi di replicabilità e valuteremo l’efficacia di metodologie alternative nel migliorare l’affidabilità della ricerca psicologica. L’obiettivo è fornire un framework metodologico integrato, che combini rigore statistico, trasparenza analitica e coerenza teorica, orientando gli studenti verso pratiche di ricerca empiricamente e concettualmente più solide.  


## Bibliografia {.unnumbered .unlisted}

