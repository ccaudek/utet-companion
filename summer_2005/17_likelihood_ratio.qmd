# Il rapporto di verosimiglianze {#sec-prob-likelihood-ratio}

::: callout-important
## In questo capitolo, imparerai a:

- comprendere il concetto di rapporto di verosimiglianza;
- interpretare l'associato valore-p.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Estimation* [@schervish2014probability].
- Leggere il capitolo *Bayes' rule* [@Johnson2022bayesrules].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::


## Introduzione 

Quando conduciamo un‚Äôanalisi statistica, spesso ci troviamo di fronte alla necessit√† di **confrontare due modelli** che cercano di spiegare gli stessi dati. Immaginiamo, ad esempio, di voler capire qual √® la **media** di una certa variabile (come il punteggio a un test, il tempo di reazione, ecc.). Potremmo avere **due ipotesi** alternative sull‚Äôeffettivo valore della media:

- secondo un primo modello, la media √® $\mu_1$ (ad esempio, l‚Äôipotesi "nulla", che rappresenta uno stato di riferimento o di assenza di effetto),
- secondo un secondo modello, la media √® $\mu_2$ (ad esempio, l‚Äôipotesi "alternativa", che rappresenta un cambiamento o un effetto).

Per decidere **quale modello √® pi√π compatibile con i dati osservati**, possiamo usare la **verosimiglianza** (*likelihood*).  La verosimiglianza misura **quanto bene un certo valore del parametro spiega i dati osservati**. Pi√π la verosimiglianza √® alta, pi√π i dati sono ‚Äúcoerenti‚Äù con quel valore.

### Il confronto: il rapporto di verosimiglianze

Per confrontare i due modelli, calcoliamo la verosimiglianza dei dati in ciascun caso, e ne facciamo il rapporto:

$$
\lambda = \frac{L(\mu_2 \mid \text{dati})}{L(\mu_1 \mid \text{dati})}
$$ {#eq-prob-lr-def}

dove:

- $L(\mu_2 \mid \text{dati})$ √® la verosimiglianza del **modello alternativo** (cio√®, quanto sono compatibili i dati con $\mu_2$),
- $L(\mu_1 \mid \text{dati})$ √® la verosimiglianza del **modello nullo** (cio√®, quanto sono compatibili i dati con $\mu_1$).

Questa quantit√† si chiama **rapporto di verosimiglianze** (*likelihood ratio*, LR), e rappresenta uno strumento per quantificare quanto i dati favoriscono un modello rispetto all‚Äôaltro.

### Come si interpreta $\lambda$?

- Se $\lambda > 1$, significa che i dati supportano **pi√π il modello alternativo**: i dati sono pi√π probabili sotto $\mu_2$ che sotto $\mu_1$.
- Se $\lambda < 1$, significa che i dati supportano **pi√π il modello nullo**: i dati sono pi√π probabili sotto $\mu_1$ che sotto $\mu_2$.
- Se $\lambda \approx 1$, allora i dati non permettono di distinguere chiaramente tra i due modelli.

> üí° *Il rapporto di verosimiglianze ci dice quale modello rende i dati osservati pi√π ‚Äúplausibili‚Äù.*


## Un Esempio Semplice

Immagina di aver lanciato una moneta **10 volte** e di aver ottenuto **7 teste**. Ti chiedi ora quale tra questi due modelli spiega meglio i dati:

- **Modello 1 (nullo)**: la moneta √® equa, quindi la probabilit√† di testa √® $\mu_1 = 0.5$;
- **Modello 2 (alternativo)**: la moneta √® truccata a favore delle teste, e la probabilit√† di testa √® $\mu_2 = 0.7$.

### Calcolo delle verosimiglianze

Useremo la **distribuzione binomiale**, che descrive il numero di successi (in questo caso, teste) in un numero fisso di prove (10 lanci), dato un certo valore di probabilit√†.

La verosimiglianza √® semplicemente la **probabilit√† di ottenere 7 teste su 10 lanci**, sotto ciascun modello:

- sotto il **modello nullo** ($\mu_1 = 0.5$):

  $$
  L(0.5 \mid \text{7 teste}) = \binom{10}{7} (0.5)^7 (1 - 0.5)^3 = 120 \cdot (0.5)^{10} \approx 0.117
  $$

- sotto il **modello alternativo** ($\mu_2 = 0.7$):

  $$
  L(0.7 \mid \text{7 teste}) = \binom{10}{7} (0.7)^7 (0.3)^3 \approx 120 \cdot 0.0824 \cdot 0.027 = 0.267
  $$

### Calcolo del rapporto di verosimiglianze

Ora possiamo calcolare il **rapporto**:

$$
\lambda = \frac{L(0.7 \mid \text{7 teste})}{L(0.5 \mid \text{7 teste})} \approx \frac{0.267}{0.117} \approx 2.28
$$

üëâ Questo significa che **i dati sono circa 2.3 volte pi√π compatibili con l‚Äôipotesi che la moneta sia truccata** (con $\mu = 0.7$) rispetto a quella che sia equa ($\mu = 0.5$).

### Visualizzare le funzioni di verosimiglianza

Possiamo visualizzare graficamente come cambia la verosimiglianza al variare della probabilit√† di testa ($\theta$), mantenendo fisso il numero di lanci e il numero di teste osservate.

#### Codice R

```{r}
# Parametri osservati
n <- 10        # numero totale di lanci
x <- 7         # numero di teste osservate

# Sequenza di probabilit√† (theta)
theta <- seq(0, 1, length.out = 100)

# Verosimiglianza per ogni theta
likelihood <- dbinom(x, size = n, prob = theta)

# Crea dataframe per ggplot
df <- data.frame(theta = theta, likelihood = likelihood)

# Verosimiglianza nei due modelli
L_0.5 <- dbinom(x, size = n, prob = 0.5)
L_0.7 <- dbinom(x, size = n, prob = 0.7)
LR <- L_0.7 / L_0.5

# Crea grafico
ggplot(df, aes(x = theta, y = likelihood)) +
  geom_line(color = okabe_ito_palette[2], linewidth = 1.2) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "red") +
  geom_vline(xintercept = 0.7, linetype = "dashed", color = "darkgreen") +
  geom_point(aes(x = 0.5, y = L_0.5), color = "red", size = 3) +
  geom_point(aes(x = 0.7, y = L_0.7), color = "darkgreen", size = 3) +
  labs(
    title = "Funzione di verosimiglianza per 7 teste su 10 lanci",
    x = expression(theta),
    y = "Verosimiglianza"
  ) +
  annotate(
    "text", x = 0.5, y = L_0.5 + 0.01, 
    label = "mu == 0.5", 
    parse = TRUE, hjust = -0.2, color = "red"
  ) +
  annotate(
    "text", x = 0.7, y = L_0.7 + 0.01, 
    label = "mu == 0.7", parse = TRUE, hjust = -0.2, color = "darkgreen")
```
```{r}
# Stampa dei risultati numerici
cat("L(mu = 0.5) =", round(L_0.5, 3), "\n")
cat("L(mu = 0.7) =", round(L_0.7, 3), "\n")
cat("Likelihood Ratio =", round(LR, 2), "\n")
```

Il grafico mostra come cambia la verosimiglianza al variare di $\theta$, e indica visivamente i valori assunti nei due modelli specifici. Si vede chiaramente che $\theta = 0.7$ √® pi√π compatibile con l‚Äôosservazione di 7 teste.

**In sintesi**, il **rapporto di verosimiglianze** √® uno strumento per confrontare due ipotesi. Non richiede che una delle due sia vera, ma solo di confrontare **quanto bene ciascuna spiega i dati osservati**. In questo esempio, i dati favoriscono l‚Äôipotesi che la moneta sia truccata, ma non in modo schiacciante. Il valore di $\lambda = 2.28$ indica un‚Äôevidenza moderata a favore del modello alternativo.

## Rapporti di Verosimiglianza Aggiustati e Criterio di Akaike

Spesso il rapporto di verosimiglianza "grezzo" ($\lambda$) deve essere aggiustato per tenere conto della differenza nel numero di parametri tra i modelli confrontati. Infatti, quando confrontiamo due modelli, quello con pi√π parametri tende quasi sempre a descrivere meglio i dati osservati, ma ci√≤ pu√≤ essere dovuto semplicemente alla sua maggiore complessit√†. Questo fenomeno √® noto come **sovradattamento (overfitting)**.

Per correggere questa tendenza, si usa un **rapporto di verosimiglianza aggiustato** (Adjusted Likelihood Ratio, indicato con $\lambda_{\text{adj}}$. Questo tipo di aggiustamento penalizza i modelli pi√π complessi, rendendo il confronto tra modelli pi√π equo e affidabile.

### Relazione con il Criterio di Akaike (AIC)

Una modalit√† comune per effettuare questa correzione √® tramite il **Criterio di Akaike (AIC)**. L'AIC √® definito come:

$$
\text{AIC} = 2k - 2\log(\lambda),
$$ {#eq-aic-def}

in cui:

- $k$ √® il numero dei parametri del modello.
- $\lambda$ √® il rapporto di verosimiglianza grezzo.

Da questa equazione possiamo ricavare una formula per calcolare il rapporto di verosimiglianza aggiustato utilizzando l'AIC:

$$
\lambda_{\text{adj}} = \lambda \times e^{(k_1 - k_2)},
$$

dove:

- $k_1$ √® il numero di parametri del modello pi√π semplice,
- $k_2$ √® il numero di parametri del modello pi√π complesso,
- $e^{(k_1 - k_2)}$ √® il fattore correttivo che penalizza il modello pi√π complesso.

In breve, pi√π parametri ha un modello, maggiore sar√† la penalizzazione applicata.

### Rapporto tra Likelihood Ratio e AIC

Il rapporto di verosimiglianza aggiustato tramite l'AIC consente di confrontare in modo equo modelli con un numero differente di parametri. Senza questa correzione, rischieremmo di scegliere sempre modelli pi√π complessi, indipendentemente dalla loro reale capacit√† esplicativa, con il rischio di sovrastimare la qualit√† della loro spiegazione.

Utilizzare il rapporto di verosimiglianza aggiustato, quindi, permette di scegliere il modello migliore considerando sia la capacit√† di adattarsi ai dati, sia la semplicit√† del modello stesso.

## Illustrazione

Immaginiamo un semplice esperimento psicologico sulla **memoria visiva**. Vogliamo capire se **mostrare immagini emotivamente intense** aiuta le persone a ricordare meglio, rispetto a immagini **neutre**.

Abbiamo due gruppi di partecipanti:

- il **gruppo neutro** vede 30 immagini neutre e ne ricorda correttamente **14**;
- il **gruppo emozionale** vede 30 immagini emotivamente intense e ne ricorda **22**.

### Obiettivo

Vogliamo confrontare due modelli alternativi:

- **modello nullo (H‚ÇÄ)**: la probabilit√† di ricordare un'immagine √® **uguale** nei due gruppi;
- **modello alternativo (H‚ÇÅ)**: la probabilit√† di ricordare **√® diversa** nei due gruppi.

### Dati osservati

```{r}
successi_neutro <- 14
successi_emozione <- 22
prove <- 30
```

### 1. Calcolo della Verosimiglianza

**Ipotesi nulla: probabilit√† comune.**

Se la probabilit√† √® la stessa in entrambi i gruppi, possiamo stimarla combinando i successi totali:

```{r}
p_null <- (successi_neutro + successi_emozione) / (2 * prove)
```

**Log-verosimiglianza sotto H‚ÇÄ.**

Sotto l‚Äôipotesi nulla, i dati di entrambi i gruppi devono essere spiegati da **una sola probabilit√†**:

```{r}
ll_null <- dbinom(successi_neutro, prove, p_null, log = TRUE) + 
           dbinom(successi_emozione, prove, p_null, log = TRUE)
```

**Ipotesi alternativa: probabilit√† diversa per ogni gruppo.**

Stimiamo separatamente la probabilit√† di ricordare in ciascun gruppo:

```{r}
p_neutro <- successi_neutro / prove
p_emozione <- successi_emozione / prove
```

**Log-verosimiglianza sotto H‚ÇÅ.**

Ogni gruppo ha la propria verosimiglianza:

```{r}
ll_alt <- dbinom(successi_neutro, prove, p_neutro, log = TRUE) + 
          dbinom(successi_emozione, prove, p_emozione, log = TRUE)
```

### 2. Confronto tra Modelli

**Rapporto di verosimiglianza (non penalizzato).**

Calcoliamo il **rapporto tra le due verosimiglianze**:

```{r}
lr <- exp(ll_alt - ll_null)
```

Questo ci dice **quanto meglio il modello alternativo spiega i dati rispetto al modello nullo**.

### 3. Penalizzazione per la Complessit√†

I modelli pi√π complessi tendono a spiegare meglio i dati, ma rischiano di **adattarsi troppo**. Per questo, usiamo un criterio che penalizza la complessit√†: l‚Äô**AIC** (Akaike Information Criterion).

Numero di parametri:

```{r}
k_null <- 1  # un'unica probabilit√† per entrambi i gruppi
k_alt <- 2   # probabilit√† distinte per ciascun gruppo
```

Calcolo dell‚ÄôAIC per ciascun modello:

```{r}
AIC_null <- 2 * k_null - 2 * ll_null
AIC_alt <- 2 * k_alt - 2 * ll_alt
```

**Rapporto di verosimiglianza aggiustato.**

Usiamo l‚ÄôAIC per calcolare una versione **penalizzata** del rapporto di verosimiglianze:

```{r}
lr_adj <- exp((AIC_null - AIC_alt) / 2)
```

Risultati:

```{r}
cat("Rapporto di verosimiglianza grezzo:", round(lr, 2), "\n")
cat("Rapporto di verosimiglianza aggiustato (Œª_adj):", round(lr_adj, 2), "\n")
```

**Interpretazione:**

- se **Œª_adj > 1**, i dati sono pi√π compatibili con il **modello alternativo** (due probabilit√† distinte);
- se **Œª_adj ‚âà 1**, non c‚Äô√® abbastanza evidenza per preferire un modello all‚Äôaltro.

### 4. Test del Rapporto di Verosimiglianza

Possiamo testare formalmente **se la differenza tra i modelli √® rilevante** o potrebbe essere dovuta al caso.

La statistica test √®:

$$
-2 \cdot (\log L_{H_0} - \log L_{H_1})
$$

Questa statistica segue (approssimativamente) una **distribuzione chi-quadrato** con un numero di gradi di libert√† pari alla **differenza nel numero di parametri** tra i modelli:

```{r}
LR_test <- -2 * (ll_null - ll_alt)
df <- k_alt - k_null
p_value <- 1 - pchisq(LR_test, df)

cat("Statistica test (-2 log LR):", round(LR_test, 2), "\n")
cat("Gradi di libert√†:", df, "\n")
cat("Valore p del test:", round(p_value, 4), "\n")
```

**In sintesi**, 

- se **p < 0.05**, possiamo concludere che il **modello alternativo √® da preferire**: i dati sono difficilmente compatibili con l‚Äôipotesi di probabilit√† uguali nei due gruppi;
- se **p > 0.05**, non abbiamo evidenza sufficiente per preferire il modello alternativo.

Nel nostro esempio:

- la statistica test √® ‚âà 4.48;
- il **valore-p** √® ‚âà 0.0337.

üëâ Poich√© il valore-p √® inferiore a 0.05, possiamo concludere che **il gruppo emozionale ha una probabilit√† di ricordare credibilmente diversa da quella del gruppo neutro**.

## Riflessioni Conclusive

In conclusione, usando il rapporto di verosimiglianza aggiustato e l'AIC possiamo confrontare modelli statistici in maniera equilibrata, tenendo conto sia dell'adattamento ai dati che della semplicit√† del modello.

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

