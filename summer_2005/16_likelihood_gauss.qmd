# La verosimiglianza gaussiana {#sec-prob-likelihood-gauss}

::: callout-important
## In questo capitolo, imparerai a:

- comprendere il concetto di verosimiglianza nel contesto del modello gaussiano.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Estimation* [@schervish2014probability].
- Leggere il capitolo *Bayes' rule* [@Johnson2022bayesrules].
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione {.unnumbered .unlisted}

::: {.lead}
La distribuzione gaussiana (o distribuzione normale) è una delle distribuzioni più utilizzate in statistica perché descrive molti fenomeni naturali e psicologici. In questo capitolo esploreremo come si calcola la verosimiglianza, ovvero la plausibilità dei parametri, nel caso della distribuzione normale.
:::

## Modello gaussiano e verosimiglianza

### Caso di una singola osservazione

Immaginiamo di misurare il Quoziente Intellettivo (QI) di una persona e ottenere un valore specifico, ad esempio 114. Assumiamo che il QI segua una distribuzione normale con media $\mu$ sconosciuta e deviazione standard $\sigma$ nota (ad esempio $\sigma = 15$).

La *funzione di densità di probabilità* per una distribuzione normale è:

$$
f(y \mid \mu, \sigma) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left(-\frac{(y-\mu)^2}{2\sigma^2}\right) ,
$$

dove:

- $y$ è il valore osservato,
- $\mu$ è la media (il parametro che vogliamo stimare),
- $\sigma$ è la deviazione standard (conosciuta).

La *verosimiglianza* misura quanto diversi valori di $\mu$ sono plausibili, dato il valore osservato (114).

**Esempio pratico in R:**

```{r}
# Dati iniziali
y_obs <- 114
sigma <- 15
mu_values <- seq(70, 160, length.out = 1000)

# Calcolo della verosimiglianza
likelihood <- dnorm(y_obs, mean = mu_values, sd = sigma)

# Grafico della verosimiglianza
ggplot(data.frame(mu = mu_values, likelihood = likelihood), aes(x = mu, y = likelihood)) +
  geom_line(color = okabe_ito_palette[2], linewidth = 1.2) +
  labs(
    title = "Verosimiglianza per un singolo valore di QI (114)",
    x = "Media (μ)",
    y = "Verosimiglianza"
  )
```

**Qual è il valore migliore per $\mu$?**

Il valore migliore di $\mu$ sarà quello che rende massima la verosimiglianza. In questo semplice caso, è esattamente il valore osservato (114):

```{r}
mu_ottimale <- mu_values[which.max(likelihood)]
cat("Il valore ottimale di μ è:", mu_ottimale)
```

### Log-verosimiglianza

Spesso, per semplicità di calcolo, si usa la *log-verosimiglianza*, che trasforma i prodotti in somme, rendendo i calcoli più semplici e stabili:

$$
\log L(\mu \mid y, \sigma) = -\frac{1}{2}\log(2\pi) - \log(\sigma) - \frac{(y-\mu)^2}{2\sigma^2}.
$$

**Calcolo pratico con R:**

```{r}
# Funzione di log-verosimiglianza negativa usando dnorm()
negative_log_likelihood <- function(mu, y, sigma) {
  # Ritorniamo il valore negativo della log-verosimiglianza
  -dnorm(y, mean = mu, sd = sigma, log = TRUE)
}

result <- optim(
  par = 100, # Valore iniziale
  fn = negative_log_likelihood,
  y = y_obs,
  sigma = sigma,
  method = "L-BFGS-B",
  lower = 70,
  upper = 160
)

mu_max_loglik <- result$par
cat("Il valore ottimale di μ dalla log-verosimiglianza è:", mu_max_loglik)
```

In questo caso, otteniamo nuovamente $\mu = 114$.


## Un campione di osservazioni indipendenti

Supponiamo di aver raccolto i punteggi alla scala *BDI-II* per 30 persone. Ciascun punteggio è un’osservazione indipendente da una distribuzione normale con *media incognita $\mu$* e *deviazione standard nota $\sigma = 6.5$*.

```{r}
# Dati osservati (punteggi BDI-II)
y <- c(
  26, 35, 30, 25, 44, 30, 33, 43, 22, 43, 24, 19, 39, 31, 25, 
  28, 35, 30, 26, 31, 41, 36, 26, 35, 33, 28, 27, 34, 27, 22
)

sigma <- 6.5
```

### Calcolo della log-verosimiglianza 

Definiamo una funzione che calcola la *log-verosimiglianza totale*:

```{r}
log_likelihood <- function(mu, y, sigma) {
  sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))
}
```

Esploriamo ora un intervallo di valori plausibili per $\mu$ e calcoliamo la log-verosimiglianza per ciascun valore:

```{r}
# Intervallo di valori possibili per μ
mu_range <- seq(mean(y) - 2 * sigma, mean(y) + 2 * sigma, length.out = 1000)

# Inizializza vettore dei risultati
log_lik_values <- numeric(length(mu_range))

# Ciclo esplicito per chiarezza didattica
for (i in seq_along(mu_range)) {
  mu_val <- mu_range[i]
  log_lik_values[i] <- log_likelihood(mu_val, y, sigma)
}
```

### Visualizzazione della log-verosimiglianza

```{r}
ggplot(
  data.frame(mu = mu_range, log_likelihood = log_lik_values),
  aes(x = mu, y = log_likelihood)
) +
  geom_line(color = okabe_ito_palette[2], linewidth = 1.2) +
  geom_vline(xintercept = mean(y), linetype = "dashed", color = "red") +
  labs(
    title = "Log-verosimiglianza per punteggi BDI-II",
    x = expression(mu),
    y = "Log-verosimiglianza"
  )
```

La linea tratteggiata rossa indica la *media campionaria*, che — come ci aspettiamo — è anche il valore che *massimizza la log-verosimiglianza*.

### Ottimizzazione numerica

Se volessimo calcolare il valore ottimale di $\mu$ in modo automatico:

```{r}
# Funzione negativa da minimizzare
negative_log_likelihood <- function(mu, y, sigma) {
  -sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))
}

# Ottimizzazione con limiti
result <- optim(
  par = mean(y),                   # Valore iniziale
  fn = negative_log_likelihood,   # Funzione da minimizzare
  y = y,
  sigma = sigma,
  method = "L-BFGS-B",
  lower = min(mu_range),
  upper = max(mu_range)
)

mu_optimal <- result$par
cat("Il valore ottimale di μ è:", mu_optimal)
```

- Abbiamo utilizzato `dnorm(..., log = TRUE)` per calcolare in modo semplice e numericamente stabile la log-verosimiglianza.
- Il valore di $\mu$ che *massimizza la log-verosimiglianza* corrisponde alla *media campionaria*.
- Questo è un caso in cui *la stima di massima verosimiglianza ha una forma chiusa*, ma l’approccio numerico resta utile e generalizzabile.


## Riflessioni conclusive

- Nel caso di una distribuzione normale con $\sigma$ nota, la miglior stima di $\mu$ (stima di massima verosimiglianza) è sempre la media delle osservazioni.
- Visualizzare la funzione di verosimiglianza aiuta a capire la plausibilità relativa dei diversi valori di $\mu$.
- La log-verosimiglianza semplifica e rende più stabili i calcoli numerici.


## Esercizi {.unnumbered .unlisted}

::: {.callout-important title="Problemi" collapse="true"}

Supponi di aver misurato il livello di ansia (ad esempio usando una scala standardizzata) in un campione di 15 persone con i seguenti punteggi:

```r
ansia <- c(23, 27, 30, 29, 25, 28, 26, 24, 31, 29, 27, 26, 28, 30, 25)
```

Assumendo che la deviazione standard sia nota e pari a 3.5, svolgi le seguenti attività in R:

1. Calcola la funzione di verosimiglianza gaussiana per diversi valori di $\mu$ nell'intervallo da 20 a 35.
2. Trova numericamente il valore di $\mu$ che rende massima la verosimiglianza (stima di massima verosimiglianza, MLE).
3. Disegna un grafico della funzione di verosimiglianza per visualizzare il risultato.

:::

## Informazioni sull'ambiente di sviluppo {.unnumbered .unlisted}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered .unlisted}

